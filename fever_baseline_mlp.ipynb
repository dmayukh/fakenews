{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEVER dataset processing\n",
    "\n",
    "<h5>Process the claims in the fever dataset</h5>\n",
    "\n",
    "In this notebook, we will prepare the training dataset and buid a baseline model that would set us up for the NLI tasks\n",
    "\n",
    "We use the following repos for reference code:\n",
    "\n",
    "- [fever-baselines](https://github.com/klimzaporojets/fever-baselines.git)\n",
    "- [fever-allennlp-reader](https://github.com/j6mes/fever-allennlp-reader)\n",
    "- [fever-allennlp](https://github.com/j6mes/fever-allennlp)\n",
    "\n",
    "Note, AllenNLP here is used only for the NLI training, using models such as Decomposable Attention, Elmo + ESIM, ESIM etc. We will not use any of it here.\n",
    "In this notebook, we will first focus on extracting the data from the pre-processed Wiki corpus provided by [fever.ai](https://fever.ai/dataset/fever.html).\n",
    "\n",
    "The data is available in a [docker image](https://hub.docker.com/r/feverai/common), 21GB in size. The container is created and the volume /local/ from it is mounted and made available to our [container](https://github.com/dmayukh/fakenews/Dockerfile) \n",
    "\n",
    "\n",
    "We will install a few dependencies such as:\n",
    "- numpy>=1.15\n",
    "- regex\n",
    "- allennlp==2.5.0\n",
    "- fever-scorer==2.0.39\n",
    "- fever-drqa==1.0.13\n",
    "\n",
    "The following packages are installed by the above dependencies\n",
    "- torchvision-0.9.1\n",
    "- google_cloud_storage-1.38.0\n",
    "- overrides==3.1.0\n",
    "- transformers-4.6.1\n",
    "- spacy-3.0.6\n",
    "- sentencepiece-0.1.96\n",
    "- torch-1.8.1\n",
    "- wandb-0.10.33\n",
    "- lmdb-1.2.1\n",
    "- jsonnet-0.17.0\n",
    "\n",
    "We do not really need allennlp or fever-scorer as of yet, we would only need DrQA. We would prefer to use the DrQA from the official github, but for now we will go with what was prepackaged by the [j6mes](https://pypi.org/project/fever-drqa/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Pre-parsed FEVER Datasets</h4>\n",
    "Create the database from the DB file that contains the preprocessed Wiki pages. This DB was made available to us by FEVER.\n",
    "\n",
    "FeverDocDB is a simple wrapper that opens a SQLlite3 connection to the database and provides methods to execute simple select queries to fetch ids for documents and to fetch lines given a document.\n",
    "\n",
    "We will not require this in the first pass of our work here, since we are only interested in findings the documents closest to a claim text.\n",
    "\n",
    "The function to fetch lines per document is what uses the connection to the database. In order to find the closest documents for a given claim, we use the ranker that uses a <b>pre-created TFIDF index</b> which can locate the document ids given a claim text.\n",
    "\n",
    "The pre-created index is available in '/local/fever-common/data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz'\n",
    "\n",
    "\n",
    "Sample data from training file:\n",
    "\n",
    "> {\"id\": 75397, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\", \"evidence\": [[[92206, 104971, \"Nikolaj_Coster-Waldau\", 7], [92206, 104971, \"Fox_Broadcasting_Company\", 0]]]}\n",
    "\n",
    "A closer look at the evidence:\n",
    "\n",
    "> [[92206, 104971, \"Nikolaj_Coster-Waldau\", 7]\n",
    "\n",
    "92206 and 104971 are the annotation ids, while the \"Nikolaj_Coster-Waldau\" is the evidence page and the line number is 7.\n",
    "\n",
    "\n",
    "#### Formatting the input text\n",
    "\n",
    "The training of the model is done on the evidence provided by the human annotators, therefore we use the 'evidence' to run our training.\n",
    "\n",
    "After formatting, the training examples are written as below that is then used to train the MLP\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
    "  'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
    "  'label': 0,\n",
    "  'label_text': 'SUPPORTS'}\n",
    "\n",
    "The baseline model is a simple MLP that uses the count vectorizer to vectorize the claim text and the evidence page texts. It also uses an additional feature which is the cosine similarity between the vectorized claim text and the vectorized combined texts from all the evidences.\n",
    "\n",
    "The vectorizers are saved to the filesystem that can be used later for transorming the incoming sentences.\n",
    "\n",
    "The trained model is used to run eval on the dev dataset of the same format.\n",
    "\n",
    "\n",
    "<h5>Retrieval of the evidence</h5>\n",
    "\n",
    "We also attempt to extract the evidence from the corresponding pages\n",
    "\n",
    "First, using the tfidf doc ranker, we extract the top 5 pages that are similar to the claim text\n",
    "\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .', 'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)], 'label': 0, 'label_text': 'SUPPORTS', 'predicted_pages': [('Coster', 498.82682448841246), ('Nikolaj', 348.42021460316823), ('The_Other_Woman_-LRB-2014_film-RRB-', 316.8405030379064), ('Nikolaj_Coster-Waldau', 316.8405030379064), ('Nukaaka_Coster-Waldau', 292.47605893902585)]}\n",
    "\n",
    "For each of the pages, we extract the lines from the page text and use 'online tfidf ranker' to fetch the closest matching lines from the text.\n",
    "\n",
    "The training examples are then formatted as below which is then used to run EVAL on the MLP model\n",
    "\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
    " 'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
    " 'label': 0,\n",
    " 'label_text': 'SUPPORTS',\n",
    " 'predicted_pages': [('Coster', 498.82682448841246),\n",
    "  ('Nikolaj', 348.42021460316823),\n",
    "  ('The_Other_Woman_-LRB-2014_film-RRB-', 316.8405030379064),\n",
    "  ('Nikolaj_Coster-Waldau', 316.8405030379064),\n",
    "  ('Nukaaka_Coster-Waldau', 292.47605893902585)],\n",
    " 'predicted_sentences': [('Nikolaj', 7),\n",
    "  ('The_Other_Woman_-LRB-2014_film-RRB-', 1),\n",
    "  ('Nukaaka_Coster-Waldau', 1),\n",
    "  ('Coster', 63),\n",
    "  ('Nikolaj_Coster-Waldau', 0)]}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = 'data/data'\n",
    "working_dir = 'working/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 13114, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"J. R. R. Tolkien created Gimli.\", \"evidence\": [[[28359, 34669, \"Gimli_-LRB-Middle-earth-RRB-\", 0]], [[28359, 34670, \"Gimli_-LRB-Middle-earth-RRB-\", 1]]]}\n",
      "{\"id\": 152180, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Susan Sarandon is an award winner.\", \"evidence\": [[[176133, 189101, \"Susan_Sarandon\", 1]], [[176133, 189102, \"Susan_Sarandon\", 2]], [[176133, 189103, \"Susan_Sarandon\", 8]]]}\n"
     ]
    }
   ],
   "source": [
    "!tail -2 data/data/fever-data/train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
      "{\"id\": 163803, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Ukrainian Soviet Socialist Republic was a founding participant of the UN.\", \"evidence\": [[[296950, 288668, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[298602, 290067, \"Ukrainian_Soviet_Socialist_Republic\", 7], [298602, 290067, \"United_Nations\", 0]], [[300696, 291816, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344347, 327887, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344994, 328433, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344997, 328435, \"Ukrainian_Soviet_Socialist_Republic\", 7]]]}\n"
     ]
    }
   ],
   "source": [
    "!head -2 data/data/fever-data/paper_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training dataset\n",
    "\n",
    "The training examples have three (3) classes:\n",
    "- SUPPORTS\n",
    "- REFUTES\n",
    "- NOT ENOUGH INFO\n",
    "\n",
    "For the 'NOT ENOUGH INFO' class, the evidences are set to None. This would cause problems with training since we would still like to generate features for the samples which have been put in this class.\n",
    "\n",
    "Next, we will loop over the records in the training dataset to create the training records. Specifically, we would be generating evidences for the samples in the 'NOT ENOUGH INFO' class so that the None values now have some page information.\n",
    "\n",
    "Our strategy for dealing with missing evidences for the 'NOT ENOUGH INFO' class is to find the pages that are closest to the claims based on the tfidf similarity. The tfidf similarity of the documents in the fever DB is already precomputed and make available to us via the index file:\n",
    "\n",
    "> '/local/fever-common/data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the directory where we will save our prepared datasets\n",
    "\n",
    "The raw training data is available at \n",
    "\n",
    "> /local/fever-common/data/fever-data/train.jsonl\n",
    "\n",
    "The raw dev data from the FEVER paper is available at \n",
    "\n",
    "> /local/fever-common/data/fever-data/paper_dev.jsonl\n",
    "\n",
    "We wil generate the training dataset by sampling for NEI examples based on closest document match against our claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p working/data/training/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.dataset.DatasetGenerator import DatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145449 working/data/training/train.ns.pages.p5.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l working/data/training/train.ns.pages.p5.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the training dataset\n",
    "\n",
    "This takes a while, if we have already run this step in the past, we would simply jump to <b>Building the feature sets</b> step and use the file from the working_dir + 'train.ns.pages.p5.jsonl' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/145449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to working/data/training/baseline//train.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [25:35<00:00, 94.70it/s] \n"
     ]
    }
   ],
   "source": [
    "ds_generator = DatasetGenerator(dataset_root='data/data/',out_dir='working/data/training/baseline/', database_path='data/data/fever/fever.db')\n",
    "ds_generator.generate_nei_evidences('train', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/9999 [00:00<09:23, 17.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to working/data/training/baseline//paper_dev.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [02:23<00:00, 69.78it/s] \n"
     ]
    }
   ],
   "source": [
    "ds_generator = DatasetGenerator(dataset_root='data/data/',out_dir='working/data/training/baseline/', database_path='data/data/fever/fever.db')\n",
    "ds_generator.generate_nei_evidences('paper_dev', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 working/data/training/baseline/paper_dev.ns.pages.p5.jsonl\n",
      "  145449 working/data/training/baseline/train.ns.pages.p5.jsonl\n",
      "  155448 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l  working/data/training/baseline/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the feature sets\n",
    "\n",
    "Using the training data and dev data we generated, we will create the vectorizers and save them to local files\n",
    "\n",
    "The training and dev data is available at \n",
    "\n",
    "> working/data/training/baseline/train.ns.pages.p5.jsonl \n",
    "\n",
    "> working/data/training/baseline/paper_dev.ns.pages.p5.jsonl\n",
    "\n",
    "The key information we need from the training samples are the claim text and the texts from the evidence pages\n",
    "\n",
    "For each training example, generate:\n",
    "- a tokenized claim, \n",
    "- the label id, \n",
    "- the label text, \n",
    "- list of wiki pages that were provided as evidence.\n",
    "\n",
    "This is done using a custom formatter `training_line_formatter` we would write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.dataset.DatasetReader import DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:01<00:00, 80259.74it/s] \n",
      "100%|██████████| 145449/145449 [00:01<00:00, 142429.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(3,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/training/baseline/train.ns.pages.p5.jsonl'\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=None, database_path='data/data/fever/fever.db')\n",
    "raw, data = dsreader.read()\n",
    "ds_train = dsreader.get_dataset()\n",
    "print(ds_train.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the label encoder from training, we will need them for the dev dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "label_checkpoint_file = 'working/data/training/baseline/label_encoder_train.pkl'\n",
    "with open(label_checkpoint_file, 'wb') as f:\n",
    "    pickle.dump(dsreader.labelencoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 182615.14it/s]\n",
      "100%|██████████| 9999/9999 [00:00<00:00, 204362.41it/s]\n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/training/baseline/paper_dev.ns.pages.p5.jsonl'\n",
    "label_checkpoint_file = 'working/data/training/baseline/label_encoder_train.pkl'\n",
    "#note, use type = 'train' since formatting would be like the train examples\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=label_checkpoint_file, database_path='data/data/fever/fever.db', type='train')\n",
    "raw_dev, data_dev = dsreader.read()\n",
    "ds_dev = dsreader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the vectorizers\n",
    "\n",
    "We will build a <b>term frequency vectorizer</b> and a TDIDF vectorizer and save them to a file.\n",
    "\n",
    "The vocabulary will be limited to 5000. For each of the claim and the body text, we would produce the vectors which would be of dimension 5000.\n",
    "\n",
    "We will also add the cosine similarity between the claim vector and the body text vector and use it as an additional feature.\n",
    "\n",
    "The dimension of our feature would be then 5000 + 5000 + 1 = 10001\n",
    "\n",
    "We will be using the contents of both the training and dev set to build the vectorizers. \n",
    "\n",
    "We will need to read the dataset into memory from the td.dataset readers, since CountVectorizers cannot operate in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "max_len = 4  # Sequence length to pad the outputs to.\n",
    "bow_vectorizer = TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len)\n",
    "freq_vectorizer = TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='count')\n",
    "tfidf_vectorizer = TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='tf-idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_train.map(lambda x, y: x[0] + ' ' + x[1])\n",
    "bow_vectorizer.adapt(ds.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_vectorizer.adapt(ds.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.adapt(ds.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x7fcebb5394d0>) with an unsupported type (<class 'tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b0422103807e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'working/data/training/baseline/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m tf.io.write_file(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'bow_vectorizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mwrite_file\u001b[0;34m(filename, contents, name)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m       return write_file_eager_fallback(\n\u001b[0;32m-> 2266\u001b[0;31m           filename, contents, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   2267\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mwrite_file_eager_fallback\u001b[0;34m(filename, contents, name, ctx)\u001b[0m\n\u001b[1;32m   2293\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_file_eager_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m   \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m   \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (<tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x7fcebb5394d0>) with an unsupported type (<class 'tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "### Save the vectorizers\n",
    "# import os\n",
    "# path = 'working/data/training/baseline/'\n",
    "# tf.io.write_file(\n",
    "#     path + 'bow_vectorizer.pkl', bow_vectorizer, name=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'working/data/training/baseline/'\n",
    "# with open(os.path.join(path + 'freq_vectorizer.pkl'), \"wb+\") as f:\n",
    "#     pickle.dump(freq_vectorizer, f)\n",
    "# path = 'working/data/training/baseline/'\n",
    "# with open(os.path.join(path + 'tfidf_vectorizer.pkl'), \"wb+\") as f:\n",
    "#     pickle.dump(tfidf_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'the',\n",
       " 'and',\n",
       " 'in',\n",
       " 'of',\n",
       " 'a',\n",
       " 'is',\n",
       " 'rrb',\n",
       " 'lrb',\n",
       " 'end',\n",
       " 'start',\n",
       " 'by',\n",
       " 'was',\n",
       " 'for',\n",
       " 'as',\n",
       " 'to',\n",
       " 'film',\n",
       " 'on',\n",
       " 'an',\n",
       " 's',\n",
       " 'american',\n",
       " 'with',\n",
       " 'he',\n",
       " 'his',\n",
       " 'born',\n",
       " 'from',\n",
       " 'has',\n",
       " 'series',\n",
       " 'her',\n",
       " 'award',\n",
       " 'best',\n",
       " 'which',\n",
       " 'it',\n",
       " 'at',\n",
       " 'she',\n",
       " 'television',\n",
       " 'known',\n",
       " 'first',\n",
       " 'lsb',\n",
       " 'rsb',\n",
       " 'also',\n",
       " 'actor',\n",
       " 'one',\n",
       " 'directed',\n",
       " 'that',\n",
       " 'album',\n",
       " 'united',\n",
       " 'released',\n",
       " 'world',\n",
       " 'or',\n",
       " 'actress',\n",
       " 'who',\n",
       " 'films',\n",
       " 'states',\n",
       " 'won',\n",
       " 'drama',\n",
       " 'its',\n",
       " 'awards',\n",
       " 'most',\n",
       " 'written',\n",
       " 'role',\n",
       " 'two',\n",
       " 'are',\n",
       " 'after',\n",
       " 'new',\n",
       " 'including',\n",
       " 'academy',\n",
       " 'city',\n",
       " 'band',\n",
       " 'comedy',\n",
       " 'producer',\n",
       " '2016',\n",
       " 'stars',\n",
       " 'singer',\n",
       " 'received',\n",
       " '2012',\n",
       " 'music',\n",
       " 'based',\n",
       " 'may',\n",
       " 'john',\n",
       " 'name',\n",
       " 'been',\n",
       " 'debut',\n",
       " '2015',\n",
       " 'second',\n",
       " 'produced',\n",
       " 'starring',\n",
       " '2014',\n",
       " 'million',\n",
       " 'roles',\n",
       " 'their',\n",
       " '2013',\n",
       " 'rock',\n",
       " '2011',\n",
       " 'three',\n",
       " 'had',\n",
       " 'english',\n",
       " 'british',\n",
       " 'studio',\n",
       " 'all',\n",
       " 'director',\n",
       " 'other',\n",
       " 'such',\n",
       " 'time',\n",
       " '2010',\n",
       " 'career',\n",
       " 'be',\n",
       " 'starred',\n",
       " 'golden',\n",
       " 'show',\n",
       " 'character',\n",
       " '2008',\n",
       " 'only',\n",
       " 'have',\n",
       " 'over',\n",
       " 'april',\n",
       " '2009',\n",
       " 'not',\n",
       " 'year',\n",
       " '2006',\n",
       " 'appeared',\n",
       " 'us',\n",
       " 'became',\n",
       " 'country',\n",
       " 'war',\n",
       " 'made',\n",
       " 'during',\n",
       " 'song',\n",
       " 'season',\n",
       " 'march',\n",
       " 'july',\n",
       " 'professional',\n",
       " 'september',\n",
       " 'four',\n",
       " 'were',\n",
       " 'november',\n",
       " 'novel',\n",
       " 'years',\n",
       " 'more',\n",
       " 'october',\n",
       " 'david',\n",
       " '2007',\n",
       " 'played',\n",
       " 'june',\n",
       " '2005',\n",
       " 'august',\n",
       " 'well',\n",
       " 'several',\n",
       " 'state',\n",
       " 'globe',\n",
       " 'same',\n",
       " 'began',\n",
       " 'south',\n",
       " 'january',\n",
       " 'james',\n",
       " 'december',\n",
       " 'since',\n",
       " 'february',\n",
       " 'into',\n",
       " 'record',\n",
       " 'nominated',\n",
       " 'national',\n",
       " 'records',\n",
       " 'group',\n",
       " 'number',\n",
       " 'supporting',\n",
       " '2004',\n",
       " 'international',\n",
       " 'football',\n",
       " '2003',\n",
       " 'game',\n",
       " '2',\n",
       " 'michael',\n",
       " 'before',\n",
       " 'lead',\n",
       " 'work',\n",
       " 'star',\n",
       " 'member',\n",
       " 'york',\n",
       " 'major',\n",
       " 'part',\n",
       " 'third',\n",
       " 'george',\n",
       " 'later',\n",
       " 'created',\n",
       " 'science',\n",
       " 'team',\n",
       " 'north',\n",
       " 'player',\n",
       " 'artist',\n",
       " 'but',\n",
       " 'story',\n",
       " 'nominations',\n",
       " 'romantic',\n",
       " '1999',\n",
       " 'single',\n",
       " '2001',\n",
       " 'many',\n",
       " '2000',\n",
       " 'fiction',\n",
       " 'premiered',\n",
       " 'president',\n",
       " 'performance',\n",
       " 'movie',\n",
       " 'thriller',\n",
       " 'company',\n",
       " 'club',\n",
       " 'life',\n",
       " 'america',\n",
       " 'largest',\n",
       " 'five',\n",
       " 'west',\n",
       " '2017',\n",
       " 'writer',\n",
       " 'songwriter',\n",
       " 'than',\n",
       " 'being',\n",
       " 'both',\n",
       " '1998',\n",
       " 'top',\n",
       " 'king',\n",
       " 'indian',\n",
       " 'worldwide',\n",
       " 'man',\n",
       " 'him',\n",
       " 'include',\n",
       " '2002',\n",
       " 'former',\n",
       " '10',\n",
       " 'league',\n",
       " 'early',\n",
       " 'action',\n",
       " 'history',\n",
       " 'albums',\n",
       " 'people',\n",
       " 'between',\n",
       " 'robert',\n",
       " 'love',\n",
       " '1',\n",
       " 'california',\n",
       " 'author',\n",
       " 'this',\n",
       " 'called',\n",
       " 'musical',\n",
       " 'french',\n",
       " 'through',\n",
       " '1997',\n",
       " 'i',\n",
       " 'emmy',\n",
       " 'nomination',\n",
       " 'kingdom',\n",
       " 'stage',\n",
       " 'cast',\n",
       " 'fictional',\n",
       " 'when',\n",
       " 'singles',\n",
       " 'school',\n",
       " 'fantasy',\n",
       " 'about',\n",
       " 'while',\n",
       " 'they',\n",
       " 'book',\n",
       " 'black',\n",
       " 'play',\n",
       " 'home',\n",
       " 'tv',\n",
       " 'original',\n",
       " 'disney',\n",
       " 'acting',\n",
       " 'crime',\n",
       " 'where',\n",
       " 'no',\n",
       " 'horror',\n",
       " '3',\n",
       " 'success',\n",
       " 'release',\n",
       " 'de',\n",
       " 'video',\n",
       " 'fame',\n",
       " '4',\n",
       " 'published',\n",
       " 'worked',\n",
       " 'following',\n",
       " '20',\n",
       " 'young',\n",
       " 'successful',\n",
       " 'family',\n",
       " 'until',\n",
       " '16',\n",
       " 'paul',\n",
       " 'under',\n",
       " 'age',\n",
       " 'area',\n",
       " 'republic',\n",
       " '1996',\n",
       " 'named',\n",
       " 'actors',\n",
       " 'england',\n",
       " 'earned',\n",
       " 'marvel',\n",
       " 'island',\n",
       " 'times',\n",
       " 'musician',\n",
       " 'comics',\n",
       " 'girl',\n",
       " 'university',\n",
       " 'picture',\n",
       " '100',\n",
       " '18',\n",
       " 'title',\n",
       " '1994',\n",
       " 'founded',\n",
       " 'model',\n",
       " 'high',\n",
       " '1995',\n",
       " 'up',\n",
       " 'pictures',\n",
       " 'richard',\n",
       " 'out',\n",
       " 'leading',\n",
       " 'london',\n",
       " 'screenwriter',\n",
       " 'century',\n",
       " 'martin',\n",
       " '9',\n",
       " 'last',\n",
       " 'located',\n",
       " 'some',\n",
       " 'ii',\n",
       " '15',\n",
       " 'j',\n",
       " 'my',\n",
       " 'often',\n",
       " '12',\n",
       " 'production',\n",
       " 'formed',\n",
       " 'screenplay',\n",
       " '1993',\n",
       " 'tom',\n",
       " 'there',\n",
       " '8',\n",
       " '22',\n",
       " 'games',\n",
       " 'comedydrama',\n",
       " 'death',\n",
       " '13',\n",
       " 'western',\n",
       " 'you',\n",
       " 'los',\n",
       " 'great',\n",
       " 'fourth',\n",
       " 'canadian',\n",
       " 'includes',\n",
       " 'tennis',\n",
       " 'critics',\n",
       " 'playing',\n",
       " 'championship',\n",
       " 'developed',\n",
       " 'lee',\n",
       " 'billboard',\n",
       " '17',\n",
       " 'house',\n",
       " 'william',\n",
       " 'among',\n",
       " 'association',\n",
       " 'popular',\n",
       " 'population',\n",
       " 'day',\n",
       " 'entertainment',\n",
       " 'screen',\n",
       " 'political',\n",
       " 'r',\n",
       " '5',\n",
       " 'night',\n",
       " 'angeles',\n",
       " '24',\n",
       " 'comic',\n",
       " 'set',\n",
       " 'six',\n",
       " '21',\n",
       " 'capital',\n",
       " '1992',\n",
       " '6',\n",
       " '1991',\n",
       " 'east',\n",
       " '25',\n",
       " 'wrote',\n",
       " '7',\n",
       " 'central',\n",
       " 'big',\n",
       " 'critical',\n",
       " '1986',\n",
       " 'winning',\n",
       " '11',\n",
       " 'empire',\n",
       " 'feature',\n",
       " 'europe',\n",
       " '1989',\n",
       " 'comedian',\n",
       " 'officially',\n",
       " 'party',\n",
       " 'final',\n",
       " 'can',\n",
       " 'christopher',\n",
       " 'hall',\n",
       " '23',\n",
       " 'seven',\n",
       " 'live',\n",
       " 'features',\n",
       " 'place',\n",
       " 'san',\n",
       " 'greatest',\n",
       " 'chris',\n",
       " 'making',\n",
       " 'served',\n",
       " 'grand',\n",
       " 'led',\n",
       " 'grammy',\n",
       " 'fox',\n",
       " 'scott',\n",
       " 'commercial',\n",
       " 'animated',\n",
       " '14',\n",
       " 'motion',\n",
       " 'jr',\n",
       " 'person',\n",
       " 'canada',\n",
       " 'women',\n",
       " 'queen',\n",
       " 'european',\n",
       " '30',\n",
       " '26',\n",
       " 'fifth',\n",
       " 'seasons',\n",
       " 'india',\n",
       " 'network',\n",
       " 'featured',\n",
       " 'australian',\n",
       " 'voice',\n",
       " 'plays',\n",
       " 'independent',\n",
       " 'festival',\n",
       " '1980',\n",
       " 'solo',\n",
       " 'champion',\n",
       " 'jones',\n",
       " '1969',\n",
       " '1988',\n",
       " 'titles',\n",
       " 'sold',\n",
       " 'will',\n",
       " 'portrayed',\n",
       " '1990',\n",
       " 'dark',\n",
       " 'along',\n",
       " 'studios',\n",
       " 'charles',\n",
       " 'peter',\n",
       " 'then',\n",
       " 'elizabeth',\n",
       " 'died',\n",
       " 'children',\n",
       " 'franchise',\n",
       " 'cup',\n",
       " 'books',\n",
       " 'rapper',\n",
       " 'used',\n",
       " 'sitcom',\n",
       " 'men',\n",
       " 'jackson',\n",
       " 'me',\n",
       " '19',\n",
       " 'office',\n",
       " 'language',\n",
       " 'hollywood',\n",
       " 'adaptation',\n",
       " 'late',\n",
       " 'sequel',\n",
       " 'williams',\n",
       " '1985',\n",
       " 'included',\n",
       " 'held',\n",
       " 'win',\n",
       " 'bafta',\n",
       " 'referred',\n",
       " 'mark',\n",
       " 'hot',\n",
       " 'social',\n",
       " 'populous',\n",
       " 'theatre',\n",
       " 'region',\n",
       " 'recording',\n",
       " 'la',\n",
       " 'gained',\n",
       " 'guild',\n",
       " 'open',\n",
       " 'arts',\n",
       " 'politician',\n",
       " '1975',\n",
       " '1971',\n",
       " 'novels',\n",
       " 'frank',\n",
       " '27',\n",
       " 'any',\n",
       " 'commonly',\n",
       " 'ranked',\n",
       " 'songs',\n",
       " 'countries',\n",
       " 'uk',\n",
       " 'went',\n",
       " 'superhero',\n",
       " 'acclaim',\n",
       " 'wars',\n",
       " 'rose',\n",
       " 'list',\n",
       " 'female',\n",
       " 'park',\n",
       " 'married',\n",
       " '1984',\n",
       " 'appearing',\n",
       " '1981',\n",
       " 'adapted',\n",
       " 'various',\n",
       " 'aired',\n",
       " 'system',\n",
       " 'works',\n",
       " 'highest',\n",
       " 'france',\n",
       " 'epic',\n",
       " '28',\n",
       " 'tour',\n",
       " 'considered',\n",
       " 'brother',\n",
       " 'jane',\n",
       " '1976',\n",
       " 'steven',\n",
       " 'did',\n",
       " 'characters',\n",
       " 'nba',\n",
       " 'red',\n",
       " 'media',\n",
       " 'numerous',\n",
       " 'joseph',\n",
       " 'washington',\n",
       " 'bill',\n",
       " 'came',\n",
       " 'signed',\n",
       " 'italian',\n",
       " '1987',\n",
       " 'each',\n",
       " 'main',\n",
       " 'child',\n",
       " 'eight',\n",
       " 'outstanding',\n",
       " 'originally',\n",
       " 'having',\n",
       " 'county',\n",
       " 'members',\n",
       " 'against',\n",
       " '1982',\n",
       " 'edward',\n",
       " 'historical',\n",
       " 'hit',\n",
       " '29',\n",
       " 'asia',\n",
       " 'thomas',\n",
       " 'productions',\n",
       " 'dc',\n",
       " 'africa',\n",
       " 'reviews',\n",
       " 'episodes',\n",
       " 'woman',\n",
       " 'industry',\n",
       " 'brothers',\n",
       " 'wife',\n",
       " 'around',\n",
       " 'walt',\n",
       " 'established',\n",
       " 'short',\n",
       " 'jack',\n",
       " 'union',\n",
       " 'taylor',\n",
       " 'german',\n",
       " 'pop',\n",
       " 'better',\n",
       " '1972',\n",
       " 'old',\n",
       " 'modern',\n",
       " 'germany',\n",
       " 'them',\n",
       " 'guitarist',\n",
       " 'tony',\n",
       " 'own',\n",
       " 'southern',\n",
       " '1977',\n",
       " 'basketball',\n",
       " 'th',\n",
       " 'wrestling',\n",
       " 'wwe',\n",
       " 'henry',\n",
       " '1979',\n",
       " 'sports',\n",
       " 'government',\n",
       " 'simply',\n",
       " 'center',\n",
       " 'art',\n",
       " 'followed',\n",
       " 'currently',\n",
       " 'recorded',\n",
       " 'broadcast',\n",
       " 'alongside',\n",
       " 'fire',\n",
       " 'steve',\n",
       " 'ten',\n",
       " 'professionally',\n",
       " 'ice',\n",
       " 'total',\n",
       " '1990s',\n",
       " 'episode',\n",
       " 'least',\n",
       " 'c',\n",
       " '1983',\n",
       " 'development',\n",
       " 'back',\n",
       " 'white',\n",
       " 'biographical',\n",
       " 'spanish',\n",
       " 'thrones',\n",
       " 'artists',\n",
       " 'ocean',\n",
       " 'footballer',\n",
       " 'performances',\n",
       " 'primarily',\n",
       " '1973',\n",
       " 'period',\n",
       " 'm',\n",
       " 'long',\n",
       " 'stone',\n",
       " 'took',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'performed',\n",
       " '1963',\n",
       " '1967',\n",
       " 'within',\n",
       " '200',\n",
       " 'green',\n",
       " 'doctor',\n",
       " 'another',\n",
       " 'hbo',\n",
       " '3d',\n",
       " 'batman',\n",
       " 'large',\n",
       " 'becoming',\n",
       " 'do',\n",
       " 'street',\n",
       " 'singersongwriter',\n",
       " 'now',\n",
       " 'boston',\n",
       " 'continued',\n",
       " 'public',\n",
       " '1978',\n",
       " 'andrew',\n",
       " 'father',\n",
       " 'copies',\n",
       " 'activist',\n",
       " 'law',\n",
       " 'kevin',\n",
       " 'dr',\n",
       " 'blue',\n",
       " '1970',\n",
       " 'sixth',\n",
       " 'days',\n",
       " 'radio',\n",
       " 'trilogy',\n",
       " 'lost',\n",
       " 'girls',\n",
       " 'teen',\n",
       " 'nbc',\n",
       " 'australia',\n",
       " '1962',\n",
       " 'general',\n",
       " 'human',\n",
       " 'channel',\n",
       " 'japan',\n",
       " 'label',\n",
       " 'college',\n",
       " 'mike',\n",
       " 'bestselling',\n",
       " 'northern',\n",
       " 'host',\n",
       " 'gold',\n",
       " 'river',\n",
       " 'become',\n",
       " '1960',\n",
       " 'widely',\n",
       " 'tim',\n",
       " '31',\n",
       " 'slam',\n",
       " '1968',\n",
       " 'economic',\n",
       " 'bbc',\n",
       " 'like',\n",
       " 'wilson',\n",
       " 'smith',\n",
       " 'abc',\n",
       " 'power',\n",
       " 'road',\n",
       " 'would',\n",
       " 'columbia',\n",
       " 'acclaimed',\n",
       " 'royal',\n",
       " 'china',\n",
       " 'mary',\n",
       " 'francisco',\n",
       " 'ever',\n",
       " 'special',\n",
       " 'allen',\n",
       " 'chart',\n",
       " 'howard',\n",
       " 'never',\n",
       " 'nine',\n",
       " 'democratic',\n",
       " 'summer',\n",
       " 'good',\n",
       " 'portrayal',\n",
       " 'planet',\n",
       " 'broadway',\n",
       " 'whose',\n",
       " 'primetime',\n",
       " 'events',\n",
       " 'prince',\n",
       " 'd',\n",
       " 'v',\n",
       " 'featuring',\n",
       " 'recognition',\n",
       " 'kennedy',\n",
       " 'joe',\n",
       " 'boys',\n",
       " 'sometimes',\n",
       " 'although',\n",
       " 'korean',\n",
       " 'players',\n",
       " 'harry',\n",
       " 'jim',\n",
       " 'adventure',\n",
       " 'patrick',\n",
       " 'choice',\n",
       " 'throughout',\n",
       " 'positive',\n",
       " 'whom',\n",
       " 'championships',\n",
       " 'global',\n",
       " 'space',\n",
       " 'business',\n",
       " 'civil',\n",
       " '1974',\n",
       " 'chicago',\n",
       " 'working',\n",
       " 'movies',\n",
       " 'due',\n",
       " 'oscar',\n",
       " 'leader',\n",
       " 'highestgrossing',\n",
       " 'eastern',\n",
       " 'nation',\n",
       " 'appearances',\n",
       " 'winner',\n",
       " 'box',\n",
       " 'middle',\n",
       " 't',\n",
       " 'found',\n",
       " 'male',\n",
       " 'presidential',\n",
       " 'brown',\n",
       " 'reached',\n",
       " 'nt',\n",
       " 'african',\n",
       " 'rights',\n",
       " 'greek',\n",
       " 'douglas',\n",
       " 'what',\n",
       " 'jon',\n",
       " 'beginning',\n",
       " '1964',\n",
       " 'water',\n",
       " 'organization',\n",
       " 'iii',\n",
       " 'ireland',\n",
       " 'different',\n",
       " 'town',\n",
       " 'super',\n",
       " 'next',\n",
       " 'magazine',\n",
       " 'formerly',\n",
       " 'paris',\n",
       " 'opera',\n",
       " 'given',\n",
       " 'billion',\n",
       " 'land',\n",
       " 'annual',\n",
       " 'left',\n",
       " 'lady',\n",
       " 'dead',\n",
       " 'little',\n",
       " 'inc',\n",
       " 'boy',\n",
       " 'writing',\n",
       " 'shows',\n",
       " 'era',\n",
       " 'started',\n",
       " 'consecutive',\n",
       " 'raised',\n",
       " 'form',\n",
       " 'multiple',\n",
       " 'executive',\n",
       " 'acted',\n",
       " '1980s',\n",
       " 'f',\n",
       " 'miniseries',\n",
       " 'cowritten',\n",
       " 'air',\n",
       " 'brian',\n",
       " 'texas',\n",
       " 'animation',\n",
       " 'islands',\n",
       " 'dancer',\n",
       " 'involved',\n",
       " 'b',\n",
       " 'ben',\n",
       " 'ford',\n",
       " 'irish',\n",
       " 'matt',\n",
       " 'just',\n",
       " 'carter',\n",
       " 'romance',\n",
       " 'composed',\n",
       " 'drummer',\n",
       " '1966',\n",
       " 'adam',\n",
       " 'jennifer',\n",
       " 'real',\n",
       " 'stephen',\n",
       " 'fashion',\n",
       " 'ryan',\n",
       " 'achieved',\n",
       " 'russell',\n",
       " 'st',\n",
       " 'order',\n",
       " 'mother',\n",
       " 'spain',\n",
       " 'awarded',\n",
       " 'federal',\n",
       " 'bob',\n",
       " 'reality',\n",
       " 'baseball',\n",
       " 'others',\n",
       " 'japanese',\n",
       " 'way',\n",
       " 'anne',\n",
       " '1970s',\n",
       " 'debuted',\n",
       " 'francis',\n",
       " 'highly',\n",
       " 'founder',\n",
       " 'bay',\n",
       " 'nations',\n",
       " 'these',\n",
       " 'seventh',\n",
       " 'knight',\n",
       " 'date',\n",
       " 'friends',\n",
       " 'critically',\n",
       " 'olympic',\n",
       " 'shakespeare',\n",
       " 'governor',\n",
       " 'down',\n",
       " '20th',\n",
       " 'act',\n",
       " 'notable',\n",
       " 'vocalist',\n",
       " 'distributed',\n",
       " 'grossed',\n",
       " 'anthony',\n",
       " 'alan',\n",
       " 'however',\n",
       " 'culture',\n",
       " 'battle',\n",
       " '1953',\n",
       " 'filmfare',\n",
       " 'previously',\n",
       " 'novelist',\n",
       " 'version',\n",
       " 'roman',\n",
       " 'tournament',\n",
       " 'line',\n",
       " 'hosted',\n",
       " 'directorial',\n",
       " 'johnson',\n",
       " '1954',\n",
       " 'mystery',\n",
       " 'mr',\n",
       " 'military',\n",
       " 'eighth',\n",
       " 'use',\n",
       " '1960s',\n",
       " 'service',\n",
       " 'event',\n",
       " 'earth',\n",
       " 'sport',\n",
       " 'psychological',\n",
       " 'mixed',\n",
       " 'miller',\n",
       " 'daniel',\n",
       " 'coast',\n",
       " 'fx',\n",
       " 'sister',\n",
       " 'louis',\n",
       " 'dance',\n",
       " 'style',\n",
       " 'education',\n",
       " 'further',\n",
       " 'beauty',\n",
       " 'ensemble',\n",
       " 'upon',\n",
       " 'take',\n",
       " 'range',\n",
       " 'term',\n",
       " 'grant',\n",
       " 'davis',\n",
       " 'army',\n",
       " 'personality',\n",
       " 'run',\n",
       " 'daughter',\n",
       " '1955',\n",
       " 'atlantic',\n",
       " 'lord',\n",
       " 'small',\n",
       " 'mtv',\n",
       " 'addition',\n",
       " 'punk',\n",
       " 'initially',\n",
       " 'christian',\n",
       " 'much',\n",
       " 'subsequently',\n",
       " 'joined',\n",
       " 'half',\n",
       " 'potter',\n",
       " 'competition',\n",
       " 'lion',\n",
       " 'justice',\n",
       " 'sarah',\n",
       " 'heavyweight',\n",
       " 'force',\n",
       " 'filmmaker',\n",
       " 'according',\n",
       " 'saint',\n",
       " 'harris',\n",
       " 'finals',\n",
       " 'cable',\n",
       " 'future',\n",
       " 'cultural',\n",
       " 'roll',\n",
       " 'captain',\n",
       " 'mountain',\n",
       " 'guest',\n",
       " 'though',\n",
       " 'design',\n",
       " 'ep',\n",
       " 'religion',\n",
       " 'project',\n",
       " '1961',\n",
       " 'uefa',\n",
       " 'sales',\n",
       " 'economy',\n",
       " 'moore',\n",
       " 'marie',\n",
       " 'field',\n",
       " 'italy',\n",
       " 'driver',\n",
       " 'together',\n",
       " 'chinese',\n",
       " 'we',\n",
       " 'services',\n",
       " 'jeff',\n",
       " 'titled',\n",
       " 'wrestler',\n",
       " 'court',\n",
       " 'influential',\n",
       " 'khan',\n",
       " 'earning',\n",
       " 'evans',\n",
       " 'premier',\n",
       " 'kapoor',\n",
       " 'how',\n",
       " 'division',\n",
       " 'election',\n",
       " 'beatles',\n",
       " 'credited',\n",
       " 'jason',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((32,), (32,)), (32, 3)), types: ((tf.string, tf.string), tf.int32)>\n",
      "((TensorSpec(shape=(32,), dtype=tf.string, name=None), TensorSpec(shape=(32,), dtype=tf.string, name=None)), TensorSpec(shape=(32, 3), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 60\n",
    "BUFFER_SIZE = 32000\n",
    "\n",
    "hypothesis = ds_train.map(lambda x, y: x[0])\n",
    "evidence = ds_train.map(lambda x, y: x[1])\n",
    "labels = ds_train.map(lambda x, y: y)\n",
    "# print(data)\n",
    "# print(labels)\n",
    "features = tf.data.Dataset.zip((hypothesis,evidence))\n",
    "d = tf.data.Dataset.zip((features,labels))\n",
    "dataset_train = d.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_train)\n",
    "print(dataset_train.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((32,), (32,)), (32, 3)), types: ((tf.string, tf.string), tf.int32)>\n",
      "((TensorSpec(shape=(32,), dtype=tf.string, name=None), TensorSpec(shape=(32,), dtype=tf.string, name=None)), TensorSpec(shape=(32, 3), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 60\n",
    "BUFFER_SIZE = 32000\n",
    "\n",
    "hypothesis = ds_dev.map(lambda x, y: x[0])\n",
    "evidence = ds_dev.map(lambda x, y: x[1])\n",
    "labels = ds_dev.map(lambda x, y: y)\n",
    "# print(data)\n",
    "# print(labels)\n",
    "features = tf.data.Dataset.zip((hypothesis,evidence))\n",
    "d = tf.data.Dataset.zip((features,labels))\n",
    "dataset_dev = d.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_dev)\n",
    "print(dataset_dev.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[b'[START] the flash aired in the nineties . [END]'\n",
      "  b'The Flash is a 1990 American television series developed by the writing team of Danny Bilson and Paul De Meo that aired on CBS . The Flash is a 1990 American television series developed by the writing team of Danny Bilson and Paul De Meo that aired on CBS . The Flash is a 1990 American television series developed by the writing team of Danny Bilson and Paul De Meo that aired on CBS . The Flash is a 1990 American television series developed by the writing team of Danny Bilson and Paul De Meo that aired on CBS . The Flash is a 1990 American television series developed by the writing team of Danny Bilson and Paul De Meo that aired on CBS .']\n",
      " [b'[START] winter passing had mixed reviews . [END]'\n",
      "  b'The film premiered in 2005 to mixed reviews , and was not released in the United Kingdom until 2013 , when it was released under the new title Happy Endings .']\n",
      " [b'[START] m . s . reddy produced ramayanam . [END]'\n",
      "  b'Ramayanam is a 1996 mythological Telugu film directed by Gunasekhar and produced by M. S. Reddy .']\n",
      " [b'[START] steven knight writes and cooks . [END]'\n",
      "  b'Stephen Knight -LRB- poet -RRB- -LRB- born 1960 -RRB- , Welsh writer Steven Knight -LRB- born 1959 -RRB- , British writer and co-creator of Who Wants to Be a Millionaire ?']\n",
      " [b'[START] robert browning had mastery of epic poetry . [END]'\n",
      "  b'The collection Dramatis Personae and the book-length epic poem The Ring and the Book followed , and made him a leading British poet .']\n",
      " [b'[START] charles i of england was a race car driver . [END]'\n",
      "  b'Joachim Sauter -LRB- born 1959 -RRB- , German media artist and designer']\n",
      " [b'[START] christian bale has only been featured in comedy films . [END]'\n",
      "  b\"Bale first caught the public eye at the age of 13 , when he was cast in the starring role of Steven Spielberg 's Empire of the Sun -LRB- 1987 -RRB- . Empire of the Sun is a 1987 American epic coming-of-age war film based on J. G. Ballard 's semi-autobiographical novel of the same name . Bale went on to receive greater commercial recognition for his starring role as Batman in Christopher Nolan 's Batman Begins -LRB- 2005 -RRB- , The Dark Knight -LRB- 2008 -RRB- and The Dark Knight Rises -LRB- 2012 -RRB- . The Dark Knight Rises is a 2012 British-American superhero film directed by Christopher Nolan , who co-wrote the screenplay with his brother Jonathan Nolan , and the story with David S. Goyer . His portrayal of Dicky Eklund in the David O. Russell-directed biographical film The Fighter -LRB- 2010 -RRB- , earned him critical acclaim and a number of awards , including the Academy Award for Best Supporting Actor . The Fighter is a 2010 American biographical sports drama film directed by David O. Russell , and starring Mark Wahlberg , Christian Bale , Amy Adams and Melissa Leo .\"]\n",
      " [b'[START] cowboy was performed during the super bowl xxxviii halftime show . [END]'\n",
      "  b'The song was performed during the Super Bowl XXXVIII halftime show in 2004 .']], shape=(8, 2), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for d in dataset_train.take(1):\n",
    "    print(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "evidence (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hypothesis (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization_2 (TextVecto (None, 5000)         0           hypothesis[0][0]                 \n",
      "                                                                 evidence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization_1 (TextVecto (None, 5000)         0           hypothesis[0][0]                 \n",
      "                                                                 evidence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           text_vectorization_2[0][0]       \n",
      "                                                                 text_vectorization_2[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 10001)        0           text_vectorization_1[1][0]       \n",
      "                                                                 text_vectorization_1[0][0]       \n",
      "                                                                 dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          1000200     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 100)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            303         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,000,503\n",
      "Trainable params: 1,000,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "4545/4545 [==============================] - 170s 30ms/step - loss: 0.6549 - accuracy: 0.7327 - val_loss: 0.8227 - val_accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "4545/4545 [==============================] - 166s 30ms/step - loss: 0.5245 - accuracy: 0.7931 - val_loss: 0.8830 - val_accuracy: 0.6302\n",
      "Epoch 3/10\n",
      "4545/4545 [==============================] - 166s 30ms/step - loss: 0.4750 - accuracy: 0.8139 - val_loss: 0.9153 - val_accuracy: 0.6376\n",
      "Epoch 4/10\n",
      "4545/4545 [==============================] - 166s 30ms/step - loss: 0.4426 - accuracy: 0.8270 - val_loss: 0.9525 - val_accuracy: 0.6395\n",
      "Epoch 5/10\n",
      "4545/4545 [==============================] - 166s 30ms/step - loss: 0.4222 - accuracy: 0.8349 - val_loss: 1.0623 - val_accuracy: 0.6334\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "inp1 = keras.Input(shape=(None, ), dtype=tf.string, name = \"hypothesis\")\n",
    "inp2 = keras.Input(shape=(None, ), dtype=tf.string, name = \"evidence\")\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "claim_tfs = freq_vectorizer(inp1)\n",
    "body_tfs = freq_vectorizer(inp2)\n",
    "claim_tfidf = tfidf_vectorizer(inp1)\n",
    "body_tfidf = tfidf_vectorizer(inp2)\n",
    "\n",
    "cosine_layer = keras.layers.Dot((1,1), normalize=True)\n",
    "cosine_similarity = cosine_layer((claim_tfidf, body_tfidf))\n",
    "\n",
    "w = keras.layers.concatenate([body_tfs, claim_tfs, cosine_similarity], axis = 1)\n",
    "\n",
    "x1 = keras.layers.Dense(100, activation='relu')(w)\n",
    "x2 = keras.layers.Dropout(0.4)(x1)\n",
    "x3 = keras.layers.Dense(3, activation='softmax')(x2)\n",
    "model = keras.Model([inp1, inp2], x3)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=lr), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "checkpoint_filepath = 'working/data/training/baseline/checkpoint_mlp'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "\n",
    "# Train. Do not specify batch size because the dataset takes care of that.\n",
    "history = model.fit(dataset_train, epochs=10, callbacks=[stop_early], validation_data=dataset_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
