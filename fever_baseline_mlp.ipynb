{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEVER dataset processing\n",
    "\n",
    "<h5>Process the claims in the fever dataset</h5>\n",
    "\n",
    "In this notebook, we will prepare the training dataset and buid a baseline model that would set us up for the NLI tasks\n",
    "\n",
    "We use the following repos for reference code:\n",
    "\n",
    "- [fever-baselines](https://github.com/klimzaporojets/fever-baselines.git)\n",
    "- [fever-allennlp-reader](https://github.com/j6mes/fever-allennlp-reader)\n",
    "- [fever-allennlp](https://github.com/j6mes/fever-allennlp)\n",
    "\n",
    "Note, AllenNLP here is used only for the NLI training, using models such as Decomposable Attention, Elmo + ESIM, ESIM etc. We will not use any of it here.\n",
    "In this notebook, we will first focus on extracting the data from the pre-processed Wiki corpus provided by [fever.ai](https://fever.ai/dataset/fever.html).\n",
    "\n",
    "The data is available in a [docker image](https://hub.docker.com/r/feverai/common), 21GB in size. The container is created and the volume /local/ from it is mounted and made available to our [container](https://github.com/dmayukh/fakenews/Dockerfile) \n",
    "\n",
    "\n",
    "We will install a few dependencies such as:\n",
    "- numpy>=1.15\n",
    "- regex\n",
    "- allennlp==2.5.0\n",
    "- fever-scorer==2.0.39\n",
    "- fever-drqa==1.0.13\n",
    "\n",
    "The following packages are installed by the above dependencies\n",
    "- torchvision-0.9.1\n",
    "- google_cloud_storage-1.38.0\n",
    "- overrides==3.1.0\n",
    "- transformers-4.6.1\n",
    "- spacy-3.0.6\n",
    "- sentencepiece-0.1.96\n",
    "- torch-1.8.1\n",
    "- wandb-0.10.33\n",
    "- lmdb-1.2.1\n",
    "- jsonnet-0.17.0\n",
    "\n",
    "We do not really need allennlp or fever-scorer as of yet, we would only need DrQA. We would prefer to use the DrQA from the official github, but for now we will go with what was prepackaged by the [j6mes](https://pypi.org/project/fever-drqa/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Pre-parsed FEVER Datasets</h4>\n",
    "Create the database from the DB file that contains the preprocessed Wiki pages. This DB was made available to us by FEVER.\n",
    "\n",
    "FeverDocDB is a simple wrapper that opens a SQLlite3 connection to the database and provides methods to execute simple select queries to fetch ids for documents and to fetch lines given a document.\n",
    "\n",
    "We will not require this in the first pass of our work here, since we are only interested in findings the documents closest to a claim text.\n",
    "\n",
    "The function to fetch lines per document is what uses the connection to the database. In order to find the closest documents for a given claim, we use the ranker that uses a <b>pre-created TFIDF index</b> which can locate the document ids given a claim text.\n",
    "\n",
    "The pre-created index is available in '/local/fever-common/data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz'\n",
    "\n",
    "\n",
    "Sample data from training file:\n",
    "\n",
    "> {\"id\": 75397, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\", \"evidence\": [[[92206, 104971, \"Nikolaj_Coster-Waldau\", 7], [92206, 104971, \"Fox_Broadcasting_Company\", 0]]]}\n",
    "\n",
    "A closer look at the evidence:\n",
    "\n",
    "> [[92206, 104971, \"Nikolaj_Coster-Waldau\", 7]\n",
    "\n",
    "92206 and 104971 are the annotation ids, while the \"Nikolaj_Coster-Waldau\" is the evidence page and the line number is 7.\n",
    "\n",
    "\n",
    "#### Formatting the input text\n",
    "\n",
    "The training of the model is done on the evidence provided by the human annotators, therefore we use the 'evidence' to run our training.\n",
    "\n",
    "After formatting, the training examples are written as below that is then used to train the MLP\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
    "  'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
    "  'label': 0,\n",
    "  'label_text': 'SUPPORTS'}\n",
    "\n",
    "The baseline model is a simple MLP that uses the count vectorizer to vectorize the claim text and the evidence page texts. It also uses an additional feature which is the cosine similarity between the vectorized claim text and the vectorized combined texts from all the evidences.\n",
    "\n",
    "The vectorizers are saved to the filesystem that can be used later for transorming the incoming sentences.\n",
    "\n",
    "The trained model is used to run eval on the dev dataset of the same format.\n",
    "\n",
    "\n",
    "<h5>Retrieval of the evidence</h5>\n",
    "\n",
    "We also attempt to extract the evidence from the corresponding pages\n",
    "\n",
    "First, using the tfidf doc ranker, we extract the top 5 pages that are similar to the claim text\n",
    "\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .', 'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)], 'label': 0, 'label_text': 'SUPPORTS', 'predicted_pages': [('Coster', 498.82682448841246), ('Nikolaj', 348.42021460316823), ('The_Other_Woman_-LRB-2014_film-RRB-', 316.8405030379064), ('Nikolaj_Coster-Waldau', 316.8405030379064), ('Nukaaka_Coster-Waldau', 292.47605893902585)]}\n",
    "\n",
    "For each of the pages, we extract the lines from the page text and use 'online tfidf ranker' to fetch the closest matching lines from the text.\n",
    "\n",
    "The training examples are then formatted as below which is then used to run EVAL on the MLP model\n",
    "\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
    " 'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
    " 'label': 0,\n",
    " 'label_text': 'SUPPORTS',\n",
    " 'predicted_pages': [('Coster', 498.82682448841246),\n",
    "  ('Nikolaj', 348.42021460316823),\n",
    "  ('The_Other_Woman_-LRB-2014_film-RRB-', 316.8405030379064),\n",
    "  ('Nikolaj_Coster-Waldau', 316.8405030379064),\n",
    "  ('Nukaaka_Coster-Waldau', 292.47605893902585)],\n",
    " 'predicted_sentences': [('Nikolaj', 7),\n",
    "  ('The_Other_Woman_-LRB-2014_film-RRB-', 1),\n",
    "  ('Nukaaka_Coster-Waldau', 1),\n",
    "  ('Coster', 63),\n",
    "  ('Nikolaj_Coster-Waldau', 0)]}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = 'data/data'\n",
    "working_dir = 'working/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 13114, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"J. R. R. Tolkien created Gimli.\", \"evidence\": [[[28359, 34669, \"Gimli_-LRB-Middle-earth-RRB-\", 0]], [[28359, 34670, \"Gimli_-LRB-Middle-earth-RRB-\", 1]]]}\n",
      "{\"id\": 152180, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Susan Sarandon is an award winner.\", \"evidence\": [[[176133, 189101, \"Susan_Sarandon\", 1]], [[176133, 189102, \"Susan_Sarandon\", 2]], [[176133, 189103, \"Susan_Sarandon\", 8]]]}\n"
     ]
    }
   ],
   "source": [
    "!tail -2 data/data/fever-data/train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
      "{\"id\": 163803, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Ukrainian Soviet Socialist Republic was a founding participant of the UN.\", \"evidence\": [[[296950, 288668, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[298602, 290067, \"Ukrainian_Soviet_Socialist_Republic\", 7], [298602, 290067, \"United_Nations\", 0]], [[300696, 291816, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344347, 327887, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344994, 328433, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344997, 328435, \"Ukrainian_Soviet_Socialist_Republic\", 7]]]}\n"
     ]
    }
   ],
   "source": [
    "!head -2 data/data/fever-data/paper_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training dataset\n",
    "\n",
    "The training examples have three (3) classes:\n",
    "- SUPPORTS\n",
    "- REFUTES\n",
    "- NOT ENOUGH INFO\n",
    "\n",
    "For the 'NOT ENOUGH INFO' class, the evidences are set to None. This would cause problems with training since we would still like to generate features for the samples which have been put in this class.\n",
    "\n",
    "Next, we will loop over the records in the training dataset to create the training records. Specifically, we would be generating evidences for the samples in the 'NOT ENOUGH INFO' class so that the None values now have some page information.\n",
    "\n",
    "Our strategy for dealing with missing evidences for the 'NOT ENOUGH INFO' class is to find the pages that are closest to the claims based on the tfidf similarity. The tfidf similarity of the documents in the fever DB is already precomputed and make available to us via the index file:\n",
    "\n",
    "> '/local/fever-common/data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the directory where we will save our prepared datasets\n",
    "\n",
    "The raw training data is available at \n",
    "\n",
    "> /local/fever-common/data/fever-data/train.jsonl\n",
    "\n",
    "The raw dev data from the FEVER paper is available at \n",
    "\n",
    "> /local/fever-common/data/fever-data/paper_dev.jsonl\n",
    "\n",
    "We wil generate the training dataset by sampling for NEI examples based on closest document match against our claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p working/data/training/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.dataset.DatasetGenerator import DatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145449 working/data/training/train.ns.pages.p5.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l working/data/training/train.ns.pages.p5.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the training dataset\n",
    "\n",
    "This takes a while, if we have already run this step in the past, we would simply jump to <b>Building the feature sets</b> step and use the file from the working_dir + 'train.ns.pages.p5.jsonl' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/145449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to working/data/training/baseline//train.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [25:35<00:00, 94.70it/s] \n"
     ]
    }
   ],
   "source": [
    "ds_generator = DatasetGenerator(dataset_root='data/data/',out_dir='working/data/training/baseline/', database_path='data/data/fever/fever.db')\n",
    "ds_generator.generate_nei_evidences('train', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/9999 [00:00<09:23, 17.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to working/data/training/baseline//paper_dev.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [02:23<00:00, 69.78it/s] \n"
     ]
    }
   ],
   "source": [
    "ds_generator = DatasetGenerator(dataset_root='data/data/',out_dir='working/data/training/baseline/', database_path='data/data/fever/fever.db')\n",
    "ds_generator.generate_nei_evidences('paper_dev', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 working/data/training/baseline/paper_dev.ns.pages.p5.jsonl\n",
      "  145449 working/data/training/baseline/train.ns.pages.p5.jsonl\n",
      "  155448 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l  working/data/training/baseline/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the feature sets\n",
    "\n",
    "Using the training data and dev data we generated, we will create the vectorizers and save them to local files\n",
    "\n",
    "The training and dev data is available at \n",
    "\n",
    "> working/data/training/baseline/train.ns.pages.p5.jsonl \n",
    "\n",
    "> working/data/training/baseline/paper_dev.ns.pages.p5.jsonl\n",
    "\n",
    "The key information we need from the training samples are the claim text and the texts from the evidence pages\n",
    "\n",
    "For each training example, generate:\n",
    "- a tokenized claim, \n",
    "- the label id, \n",
    "- the label text, \n",
    "- list of wiki pages that were provided as evidence.\n",
    "\n",
    "This is done using a custom formatter `training_line_formatter` we would write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.dataset.DatasetReader import DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:01<00:00, 80259.74it/s] \n",
      "100%|██████████| 145449/145449 [00:01<00:00, 142429.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(3,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/training/baseline/train.ns.pages.p5.jsonl'\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=None, database_path='data/data/fever/fever.db')\n",
    "raw, data = dsreader.read()\n",
    "ds_train = dsreader.get_dataset()\n",
    "print(ds_train.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the label encoder from training, we will need them for the dev dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "label_checkpoint_file = 'working/data/training/baseline/label_encoder_train.pkl'\n",
    "with open(label_checkpoint_file, 'wb') as f:\n",
    "    pickle.dump(dsreader.labelencoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 182615.14it/s]\n",
      "100%|██████████| 9999/9999 [00:00<00:00, 204362.41it/s]\n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/training/baseline/paper_dev.ns.pages.p5.jsonl'\n",
    "label_checkpoint_file = 'working/data/training/baseline/label_encoder_train.pkl'\n",
    "#note, use type = 'train' since formatting would be like the train examples\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=label_checkpoint_file, database_path='data/data/fever/fever.db', type='train')\n",
    "raw_dev, data_dev = dsreader.read()\n",
    "ds_dev = dsreader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the vectorizers\n",
    "\n",
    "We will build a <b>term frequency vectorizer</b> and a TDIDF vectorizer and save them to a file.\n",
    "\n",
    "The vocabulary will be limited to 5000. For each of the claim and the body text, we would produce the vectors which would be of dimension 5000.\n",
    "\n",
    "We will also add the cosine similarity between the claim vector and the body text vector and use it as an additional feature.\n",
    "\n",
    "The dimension of our feature would be then 5000 + 5000 + 1 = 10001\n",
    "\n",
    "We will be using the contents of both the training and dev set to build the vectorizers. \n",
    "\n",
    "We will need to read the dataset into memory from the td.dataset readers, since CountVectorizers cannot operate in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "max_len = 4  # Sequence length to pad the outputs to.\n",
    "bow_vectorizer = TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len)\n",
    "freq_vectorizer = TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='count')\n",
    "tfidf_vectorizer = TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='tf-idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_train.map(lambda x, y: x[0] + ' ' + x[1])\n",
    "bow_vectorizer.adapt(ds.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_vectorizer.adapt(ds.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.adapt(ds.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((32,), (32,)), (32, 3)), types: ((tf.string, tf.string), tf.int32)>\n",
      "((TensorSpec(shape=(32,), dtype=tf.string, name=None), TensorSpec(shape=(32,), dtype=tf.string, name=None)), TensorSpec(shape=(32, 3), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 60\n",
    "BUFFER_SIZE = 32000\n",
    "\n",
    "hypothesis = ds_train.map(lambda x, y: x[0])\n",
    "evidence = ds_train.map(lambda x, y: x[1])\n",
    "labels = ds_train.map(lambda x, y: y)\n",
    "# print(data)\n",
    "# print(labels)\n",
    "features = tf.data.Dataset.zip((hypothesis,evidence))\n",
    "d = tf.data.Dataset.zip((features,labels))\n",
    "dataset_train = d.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_train)\n",
    "print(dataset_train.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((32,), (32,)), (32, 3)), types: ((tf.string, tf.string), tf.int32)>\n",
      "((TensorSpec(shape=(32,), dtype=tf.string, name=None), TensorSpec(shape=(32,), dtype=tf.string, name=None)), TensorSpec(shape=(32, 3), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 60\n",
    "BUFFER_SIZE = 32000\n",
    "\n",
    "hypothesis = ds_dev.map(lambda x, y: x[0])\n",
    "evidence = ds_dev.map(lambda x, y: x[1])\n",
    "labels = ds_dev.map(lambda x, y: y)\n",
    "# print(data)\n",
    "# print(labels)\n",
    "features = tf.data.Dataset.zip((hypothesis,evidence))\n",
    "d = tf.data.Dataset.zip((features,labels))\n",
    "dataset_dev = d.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_dev)\n",
    "print(dataset_dev.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "evidence (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hypothesis (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization_2 (TextVecto (None, 5000)         0           hypothesis[0][0]                 \n",
      "                                                                 evidence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization_1 (TextVecto (None, 5000)         0           hypothesis[0][0]                 \n",
      "                                                                 evidence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           text_vectorization_2[0][0]       \n",
      "                                                                 text_vectorization_2[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 10001)        0           text_vectorization_1[1][0]       \n",
      "                                                                 text_vectorization_1[0][0]       \n",
      "                                                                 dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          1000200     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 100)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            303         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,000,503\n",
      "Trainable params: 1,000,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "4545/4545 [==============================] - 170s 30ms/step - loss: 0.6549 - accuracy: 0.7327 - val_loss: 0.8227 - val_accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "4545/4545 [==============================] - 166s 30ms/step - loss: 0.5245 - accuracy: 0.7931 - val_loss: 0.8830 - val_accuracy: 0.6302\n",
      "Epoch 3/10\n",
      "4545/4545 [==============================] - 166s 30ms/step - loss: 0.4750 - accuracy: 0.8139 - val_loss: 0.9153 - val_accuracy: 0.6376\n",
      "Epoch 4/10\n",
      "4545/4545 [==============================] - 166s 30ms/step - loss: 0.4426 - accuracy: 0.8270 - val_loss: 0.9525 - val_accuracy: 0.6395\n",
      "Epoch 5/10\n",
      "4545/4545 [==============================] - 166s 30ms/step - loss: 0.4222 - accuracy: 0.8349 - val_loss: 1.0623 - val_accuracy: 0.6334\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "inp1 = keras.Input(shape=(None, ), dtype=tf.string, name = \"hypothesis\")\n",
    "inp2 = keras.Input(shape=(None, ), dtype=tf.string, name = \"evidence\")\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "claim_tfs = freq_vectorizer(inp1)\n",
    "body_tfs = freq_vectorizer(inp2)\n",
    "claim_tfidf = tfidf_vectorizer(inp1)\n",
    "body_tfidf = tfidf_vectorizer(inp2)\n",
    "\n",
    "cosine_layer = keras.layers.Dot((1,1), normalize=True)\n",
    "cosine_similarity = cosine_layer((claim_tfidf, body_tfidf))\n",
    "\n",
    "w = keras.layers.concatenate([body_tfs, claim_tfs, cosine_similarity], axis = 1)\n",
    "\n",
    "x1 = keras.layers.Dense(100, activation='relu')(w)\n",
    "x2 = keras.layers.Dropout(0.4)(x1)\n",
    "x3 = keras.layers.Dense(3, activation='softmax')(x2)\n",
    "model = keras.Model([inp1, inp2], x3)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=lr), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "checkpoint_filepath = 'working/data/training/baseline/checkpoint_mlp'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "\n",
    "# Train. Do not specify batch size because the dataset takes care of that.\n",
    "history = model.fit(dataset_train, epochs=10, callbacks=[stop_early], validation_data=dataset_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
