{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data\n",
    "The test data is in 'paper_test.jsonl'\n",
    "\n",
    "The data is copied from the directory in the image to 'data/data/'\n",
    "\n",
    "The annotations are available in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'data/data/fever-data/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls data/data/fever-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999 data/data/fever-data/paper_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/data/fever-data/paper_test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
      "{\"id\": 163803, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Ukrainian Soviet Socialist Republic was a founding participant of the UN.\", \"evidence\": [[[296950, 288668, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[298602, 290067, \"Ukrainian_Soviet_Socialist_Republic\", 7], [298602, 290067, \"United_Nations\", 0]], [[300696, 291816, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344347, 327887, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344994, 328433, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344997, 328435, \"Ukrainian_Soviet_Socialist_Republic\", 7]]]}\n"
     ]
    }
   ],
   "source": [
    "!head -2 data/data/fever-data/paper_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drqa import retriever\n",
    "tdidf_npz_file = 'data/data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz'\n",
    "ranker = retriever.get_class('tfidf')(tfidf_path=tdidf_npz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<drqa.retriever.tfidf_doc_ranker.TfidfDocRanker at 0x7f63d9908390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the data, sample randomly for not enough info class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_dataset(split, k=5):\n",
    "    fever_root = 'data/'\n",
    "    working_dir = 'working/data/'\n",
    "    print(\"Saving prepared dataset to {}\".format(\"training/{0}.ns.pages.p{1}.jsonl\".format(split,k)))\n",
    "    with open(fever_root + \"data/fever-data/{0}.jsonl\".format(split),\"r\") as f_in:\n",
    "        with open(working_dir + \"training/{0}.ns.pages.p{1}.jsonl\".format(split,k),\"w+\") as f_out:\n",
    "            for line in tqdm(f_in.readlines()):\n",
    "                line = json.loads(line)\n",
    "                if line[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "                        doc_names, doc_scores = ranker.closest_docs(line['claim'], k)\n",
    "                        pp = list(doc_names)\n",
    "\n",
    "                        for idx,evidence_group in enumerate(line['evidence']):\n",
    "                            for evidence in evidence_group:\n",
    "                                if idx<len(pp):\n",
    "                                    evidence[2] = pp[idx]\n",
    "                                    evidence[3] = -1\n",
    "                \n",
    "                f_out.write(json.dumps(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prepared dataset to training/paper_test.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [02:36<00:00, 63.69it/s] \n"
     ]
    }
   ],
   "source": [
    "!rm -rf training/test.ns.pages.p5.jsonl\n",
    "prepare_dataset('paper_test', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_dev.ns.pages.p5.jsonl   train.ns.pages.p5.jsonl\n",
      "paper_test.ns.pages.p5.jsonl  train.pages.p5.jsonl\n"
     ]
    }
   ],
   "source": [
    "ls working/data/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999 working/data/training/paper_test.ns.pages.p5.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l working/data/training/paper_test.ns.pages.p5.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "class LabelSchema:\n",
    "    def __init__(self,labels):\n",
    "        self.labels = {self.preprocess(val):idx for idx,val in enumerate(labels)}\n",
    "        self.idx = {idx:self.preprocess(val) for idx,val in enumerate(labels)}\n",
    "\n",
    "    def get_id(self,label):\n",
    "        if self.preprocess(label) in self.labels:\n",
    "            return self.labels[self.preprocess(label)]\n",
    "        return None\n",
    "\n",
    "    def preprocess(self,item):\n",
    "        return item.lower()\n",
    "\n",
    "class FEVERLabelSchema(LabelSchema):\n",
    "    def __init__(self):\n",
    "        super().__init__([\"supports\", \"refutes\", \"not enough info\"])\n",
    "\n",
    "def nltk_tokenizer(text):\n",
    "    return \" \".join(word_tokenize(text))\n",
    "\n",
    "class training_line_formatter():\n",
    "    def __init__(self):\n",
    "        self.tokenize = nltk_tokenizer\n",
    "        \n",
    "    def format(self, lines):\n",
    "        formatted = []\n",
    "        for line in tqdm(lines):\n",
    "            fl = self.format_line(line)\n",
    "            if fl is not None:\n",
    "                if isinstance(fl,list):\n",
    "                    formatted.extend(fl)\n",
    "                else:\n",
    "                    formatted.append(fl)\n",
    "        return formatted\n",
    "\n",
    "    def format_line(self, line):\n",
    "        label_schema = FEVERLabelSchema()\n",
    "        # get the label, i.e. SUPPORTS etc.\n",
    "        annotation = line[\"label\"]\n",
    "        if annotation is None:\n",
    "            annotation = line[\"verifiable\"]\n",
    "        pages = []\n",
    "        # did we get the closest sentences to the claim text? is this the sentence or the line number from the doc text?\n",
    "        if 'predicted_sentences' in line:\n",
    "            pages.extend([(ev[0], ev[1]) for ev in line[\"predicted_sentences\"]])\n",
    "        elif 'predicted_pages' in line:\n",
    "            pages.extend([(ev[0], -1) for ev in line[\"predicted_pages\"]])\n",
    "        else:\n",
    "            # these are the human annotated evidence available in the original training file\n",
    "            for evidence_group in line[\"evidence\"]:\n",
    "                pages.extend([(ev[2], ev[3]) for ev in evidence_group])\n",
    "\n",
    "        return {\"claim\": self.tokenize(line[\"claim\"]), \"evidence\": pages, \"label\": label_schema.get_id(annotation),\n",
    "                \"label_text\": annotation}\n",
    "    \n",
    "class Reader:\n",
    "    def __init__(self,encoding=\"utf-8\"):\n",
    "        self.enc = encoding\n",
    "\n",
    "    def read(self,file):\n",
    "        with open(file,\"r\",encoding = self.enc) as f:\n",
    "            return self.process(f)\n",
    "\n",
    "    def process(self,f):\n",
    "        pass\n",
    "\n",
    "class JSONLineReader(Reader):\n",
    "    def process(self,fp):\n",
    "        data = []\n",
    "        for line in tqdm(fp.readlines()):\n",
    "            data.append(json.loads(line.strip()))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 180319.31it/s]\n",
      "100%|██████████| 9999/9999 [00:01<00:00, 8289.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'claim': 'Grease had bad reviews .',\n",
       "  'evidence': [('Grease_gun_-LRB-tool-RRB-', -1)],\n",
       "  'label': 2,\n",
       "  'label_text': 'NOT ENOUGH INFO'},\n",
       " {'claim': 'Ukrainian Soviet Socialist Republic was a founding participant of the UN .',\n",
       "  'evidence': [('Ukrainian_Soviet_Socialist_Republic', 7),\n",
       "   ('Ukrainian_Soviet_Socialist_Republic', 7),\n",
       "   ('United_Nations', 0),\n",
       "   ('Ukrainian_Soviet_Socialist_Republic', 7),\n",
       "   ('Ukrainian_Soviet_Socialist_Republic', 7),\n",
       "   ('Ukrainian_Soviet_Socialist_Republic', 7),\n",
       "   ('Ukrainian_Soviet_Socialist_Republic', 7)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'},\n",
       " {'claim': '2 Hearts is a musical composition by Minogue .',\n",
       "  'evidence': [('2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0),\n",
       "   ('2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0),\n",
       "   ('2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0),\n",
       "   ('2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "jlr = JSONLineReader()\n",
    "split = 'paper_test'\n",
    "working_dir = 'working/data/'\n",
    "k = 5\n",
    "test_data_file = working_dir + \"training/{0}.ns.pages.p{1}.jsonl\".format(split, k)\n",
    "test_data = jlr.read(test_data_file)\n",
    "\n",
    "formatter = training_line_formatter()\n",
    "formatted_test_data = formatter.format(test_data)\n",
    "\n",
    "test_data_formatted = []\n",
    "test_data_formatted.extend(filter(lambda record: record is not None, formatted_test_data))\n",
    "test_data_formatted[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PIPELINE setting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_dataset_test(split, k=5):\n",
    "    fever_root = 'data/'\n",
    "    working_dir = 'working/data/'\n",
    "    print(\"Saving prepared dataset to {}\".format(\"training/{0}.ns.pages.p{1}.jsonl\".format(split,k)))\n",
    "    with open(fever_root + \"data/fever-data/{0}.jsonl\".format(split),\"r\") as f_in:\n",
    "        with open(working_dir + \"training/{0}_pipeline.ns.pages.p{1}.jsonl\".format(split,k),\"w+\") as f_out:\n",
    "            for line in tqdm(f_in.readlines()):\n",
    "                line = json.loads(line)\n",
    "                \n",
    "                doc_names, doc_scores = ranker.closest_docs(line['claim'], k)\n",
    "                pp = list(doc_names)\n",
    "\n",
    "                for idx,evidence_group in enumerate(line['evidence']):\n",
    "                    for evidence in evidence_group:\n",
    "                        if idx<len(pp):\n",
    "                            evidence[2] = pp[idx]\n",
    "                            if line[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "                                evidence[3] = -1\n",
    "                            else:\n",
    "                                evidence[3] = -2\n",
    "                        else:\n",
    "                            evidence[2] = pp[-1] #repeat the last one\n",
    "                            evidence[3] = -2\n",
    "                if len(pp) > idx:\n",
    "                    for i in range(len(pp)-1-idx):\n",
    "                        ev = [[-1, None, pp[i], -2]]\n",
    "                        evidence_group.extend(ev)\n",
    "                #setting evidence of all samples to -1 so that during dataset preparation, we sample lines from the document as per nearest match DrQA\n",
    "                f_out.write(json.dumps(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc: working/data/training/paper_test_pipeline.ns.pages.p5.jsonl: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -rf working/data/training/paper_test_pipeline.ns.pages.p5.jsonl\n",
    "!wc -l working/data/training/paper_test_pipeline.ns.pages.p5.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prepared dataset to training/paper_test.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [07:16<00:00, 22.90it/s]\n"
     ]
    }
   ],
   "source": [
    "!rm -rf training/paper_test_pipeline.ns.pages.p5.jsonl\n",
    "prepare_dataset_test('paper_test', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, \"Grease_gun_-LRB-tool-RRB-\", -1], [-1, null, \"Grease_gun_-LRB-tool-RRB-\", -2], [-1, null, \"Nasal_sebum\", -2], [-1, null, \"Grease\", -2], [-1, null, \"Thermal_interface_material\", -2]]]}\n",
      "{\"id\": 163803, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Ukrainian Soviet Socialist Republic was a founding participant of the UN.\", \"evidence\": [[[296950, 288668, \"Emblem_of_the_Ukrainian_Soviet_Socialist_Republic\", -2]], [[298602, 290067, \"Flag_of_the_Moldavian_Autonomous_Soviet_Socialist_Republic\", -2], [298602, 290067, \"Flag_of_the_Moldavian_Autonomous_Soviet_Socialist_Republic\", -2]], [[300696, 291816, \"Ukrainian_Republic\", -2]], [[344347, 327887, \"List_of_Presidents_of_Ukraine\", -2]], [[344994, 328433, \"United_Nations_General_Assembly_Resolution_377\", -2]], [[344997, 328435, \"United_Nations_General_Assembly_Resolution_377\", -2]]]}\n",
      "{\"id\": 70041, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"2 Hearts is a musical composition by Minogue.\", \"evidence\": [[[225394, 230056, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", -2]], [[317953, 306972, \"Kylie_Minogue_singles_discography\", -2]], [[319638, 308345, \"Kylie_Minogue\", -2]], [[319643, 308348, \"X_-LRB-Kylie_Minogue_album-RRB-\", -2], [-1, null, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", -2]]]}\n",
      "{\"id\": 202314, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"The New Jersey Turnpike has zero shoulders.\", \"evidence\": [[[238335, 240393, \"Interstate_95_in_New_Jersey\", -2], [-1, null, \"Interstate_95_in_New_Jersey\", -2], [-1, null, \"List_of_turnpikes_in_New_Jersey\", -2], [-1, null, \"New_Jersey_Turnpike\", -2], [-1, null, \"Newark_Bay_Bridge\", -2]]]}\n",
      "{\"id\": 57085, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Legendary Entertainment is the owner of Wanda Cinemas.\", \"evidence\": [[[178035, null, \"Wanda_Cinemas\", -1], [182093, null, \"Wanda_Cinemas\", -1], [314120, null, \"Wanda_Cinemas\", -1], [314126, null, \"Wanda_Cinemas\", -1], [314131, null, \"Wanda_Cinemas\", -1], [-1, null, \"Wanda_Cinemas\", -2], [-1, null, \"Wanda_Group\", -2], [-1, null, \"Legendary_Entertainment\", -2], [-1, null, \"List_of_shows_produced_by_Legendary_Television\", -2]]]}\n",
      "{\"id\": 6032, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"Aruba is the only ABC Island.\", \"evidence\": [[[22769, 28071, \"Frans_Figaroa\", -2]], [[22769, 28072, \"Constitution_of_Aruba\", -2], [-1, null, \"Frans_Figaroa\", -2], [-1, null, \"Constitution_of_Aruba\", -2], [-1, null, \"Outline_of_Aruba\", -2]]]}\n",
      "{\"id\": 176630, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Great white sharks do not prefer dolphins as prey.\", \"evidence\": [[[204612, null, \"Great_white_shark\", -1], [-1, null, \"Great_white_shark\", -2], [-1, null, \"Andre_Hartman\", -2], [-1, null, \"Gansbaai\", -2], [-1, null, \"White_Shark_Cafe\\u0301\", -2]]]}\n",
      "{\"id\": 130048, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"Burbank, California has always been completely void of industry.\", \"evidence\": [[[152264, 167060, \"Burbank_City_Hall\", -2], [-1, null, \"Burbank_City_Hall\", -2], [-1, null, \"Burbank_-LRB-surname-RRB-\", -2], [-1, null, \"Maximum_II\", -2], [-1, null, \"Fort_Mangochi\", -2]]]}\n",
      "{\"id\": 100046, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"The Guthrie Theater's second building began operating in 1963.\", \"evidence\": [[[117690, null, \"Joe_Dowling\", -1], [-1, null, \"Joe_Dowling\", -2], [-1, null, \"Guthrie_Theater_production_history\", -2], [-1, null, \"Guthrie_Theater\", -2], [-1, null, \"Gold_Medal_Park\", -2]]]}\n",
      "{\"id\": 204575, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"Commodore is ranked above a rear admiral.\", \"evidence\": [[[241594, 243126, \"List_of_Supply_Officers_in_the_Royal_Navy_who_have_reached_flag_rank\", -2]], [[241594, 243127, \"Commodore_admiral\", -2], [241594, 243127, \"Commodore_admiral\", -2], [-1, null, \"List_of_Supply_Officers_in_the_Royal_Navy_who_have_reached_flag_rank\", -2], [-1, null, \"Commodore_admiral\", -2], [-1, null, \"Rear_admiral\", -2]]]}\n"
     ]
    }
   ],
   "source": [
    "!head working/data/training/paper_test_pipeline.ns.pages.p5.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
      "{\"id\": 163803, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Ukrainian Soviet Socialist Republic was a founding participant of the UN.\", \"evidence\": [[[296950, 288668, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[298602, 290067, \"Ukrainian_Soviet_Socialist_Republic\", 7], [298602, 290067, \"United_Nations\", 0]], [[300696, 291816, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344347, 327887, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344994, 328433, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344997, 328435, \"Ukrainian_Soviet_Socialist_Republic\", 7]]]}\n",
      "{\"id\": 70041, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"2 Hearts is a musical composition by Minogue.\", \"evidence\": [[[225394, 230056, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", 0]], [[317953, 306972, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", 0]], [[319638, 308345, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", 0]], [[319643, 308348, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", 0]]]}\n",
      "{\"id\": 202314, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"The New Jersey Turnpike has zero shoulders.\", \"evidence\": [[[238335, 240393, \"New_Jersey_Turnpike\", 15]]]}\n",
      "{\"id\": 57085, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Legendary Entertainment is the owner of Wanda Cinemas.\", \"evidence\": [[[178035, null, null, null], [182093, null, null, null], [314120, null, null, null], [314126, null, null, null], [314131, null, null, null]]]}\n",
      "{\"id\": 6032, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"Aruba is the only ABC Island.\", \"evidence\": [[[22769, 28071, \"ABC_islands_-LRB-Lesser_Antilles-RRB-\", 0]], [[22769, 28072, \"ABC_islands_-LRB-Lesser_Antilles-RRB-\", 1]]]}\n",
      "{\"id\": 176630, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Great white sharks do not prefer dolphins as prey.\", \"evidence\": [[[204612, null, null, null]]]}\n",
      "{\"id\": 130048, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"Burbank, California has always been completely void of industry.\", \"evidence\": [[[152264, 167060, \"Burbank,_California\", 7]]]}\n",
      "{\"id\": 100046, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"The Guthrie Theater's second building began operating in 1963.\", \"evidence\": [[[117690, null, null, null]]]}\n",
      "{\"id\": 204575, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"Commodore is ranked above a rear admiral.\", \"evidence\": [[[241594, 243126, \"Commodore_-LRB-rank-RRB-\", 0]], [[241594, 243127, \"Commodore_-LRB-rank-RRB-\", 9], [241594, 243127, \"Rear_admiral\", 0]]]}\n"
     ]
    }
   ],
   "source": [
    "!head data/data/fever-data/paper_test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/9999 [00:00<09:15, 17.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prepared dataset to training/paper_test.ns.pages.p5.jsonl\n",
      "line...\n",
      "{'id': 113501, 'verifiable': 'NOT VERIFIABLE', 'label': 'NOT ENOUGH INFO', 'claim': 'Grease had bad reviews.', 'evidence': [[[133128, None, None, None]]]}\n",
      "....\n",
      "docs....\n",
      "['Grease_gun_-LRB-tool-RRB-', 'Nasal_sebum', 'Grease', 'Thermal_interface_material', 'Grease_trap']\n",
      "LENGTH = 1\n",
      "for idx and evidence ....... 0\n",
      "0 [[133128, None, None, None]]\n",
      ">>>>>\n",
      "0 [[133128, None, 'Grease_gun_-LRB-tool-RRB-', -1]]\n",
      "IDX =  0 5\n",
      "************* FINAL LINE *********\n",
      "{'id': 113501, 'verifiable': 'NOT VERIFIABLE', 'label': 'NOT ENOUGH INFO', 'claim': 'Grease had bad reviews.', 'evidence': [[[133128, None, 'Grease_gun_-LRB-tool-RRB-', -1], [-1, None, 'Grease_gun_-LRB-tool-RRB-', -1], [-1, None, 'Nasal_sebum', -1], [-1, None, 'Grease', -1], [-1, None, 'Thermal_interface_material', -1]]]}\n",
      "line...\n",
      "{'id': 163803, 'verifiable': 'VERIFIABLE', 'label': 'SUPPORTS', 'claim': 'Ukrainian Soviet Socialist Republic was a founding participant of the UN.', 'evidence': [[[296950, 288668, 'Ukrainian_Soviet_Socialist_Republic', 7]], [[298602, 290067, 'Ukrainian_Soviet_Socialist_Republic', 7], [298602, 290067, 'United_Nations', 0]], [[300696, 291816, 'Ukrainian_Soviet_Socialist_Republic', 7]], [[344347, 327887, 'Ukrainian_Soviet_Socialist_Republic', 7]], [[344994, 328433, 'Ukrainian_Soviet_Socialist_Republic', 7]], [[344997, 328435, 'Ukrainian_Soviet_Socialist_Republic', 7]]]}\n",
      "....\n",
      "docs....\n",
      "['Emblem_of_the_Ukrainian_Soviet_Socialist_Republic', 'Flag_of_the_Moldavian_Autonomous_Soviet_Socialist_Republic', 'Ukrainian_Republic', 'List_of_Presidents_of_Ukraine', 'United_Nations_General_Assembly_Resolution_377']\n",
      "LENGTH = 6\n",
      "for idx and evidence ....... 0\n",
      "0 [[296950, 288668, 'Ukrainian_Soviet_Socialist_Republic', 7]]\n",
      ">>>>>\n",
      "0 [[296950, 288668, 'Emblem_of_the_Ukrainian_Soviet_Socialist_Republic', -1]]\n",
      "for idx and evidence ....... 1\n",
      "1 [[298602, 290067, 'Ukrainian_Soviet_Socialist_Republic', 7], [298602, 290067, 'United_Nations', 0]]\n",
      ">>>>>\n",
      "1 [[298602, 290067, 'Flag_of_the_Moldavian_Autonomous_Soviet_Socialist_Republic', -1], [298602, 290067, 'Flag_of_the_Moldavian_Autonomous_Soviet_Socialist_Republic', -1]]\n",
      "for idx and evidence ....... 2\n",
      "2 [[300696, 291816, 'Ukrainian_Soviet_Socialist_Republic', 7]]\n",
      ">>>>>\n",
      "2 [[300696, 291816, 'Ukrainian_Republic', -1]]\n",
      "for idx and evidence ....... 3\n",
      "3 [[344347, 327887, 'Ukrainian_Soviet_Socialist_Republic', 7]]\n",
      ">>>>>\n",
      "3 [[344347, 327887, 'List_of_Presidents_of_Ukraine', -1]]\n",
      "for idx and evidence ....... 4\n",
      "4 [[344994, 328433, 'Ukrainian_Soviet_Socialist_Republic', 7]]\n",
      ">>>>>\n",
      "4 [[344994, 328433, 'United_Nations_General_Assembly_Resolution_377', -1]]\n",
      "for idx and evidence ....... 5\n",
      "5 [[344997, 328435, 'Ukrainian_Soviet_Socialist_Republic', 7]]\n",
      ">>>>>\n",
      "5 [[344997, 328435, 'United_Nations_General_Assembly_Resolution_377', -1]]\n",
      "IDX =  5 5\n",
      "************* FINAL LINE *********\n",
      "{'id': 163803, 'verifiable': 'VERIFIABLE', 'label': 'SUPPORTS', 'claim': 'Ukrainian Soviet Socialist Republic was a founding participant of the UN.', 'evidence': [[[296950, 288668, 'Emblem_of_the_Ukrainian_Soviet_Socialist_Republic', -1]], [[298602, 290067, 'Flag_of_the_Moldavian_Autonomous_Soviet_Socialist_Republic', -1], [298602, 290067, 'Flag_of_the_Moldavian_Autonomous_Soviet_Socialist_Republic', -1]], [[300696, 291816, 'Ukrainian_Republic', -1]], [[344347, 327887, 'List_of_Presidents_of_Ukraine', -1]], [[344994, 328433, 'United_Nations_General_Assembly_Resolution_377', -1]], [[344997, 328435, 'United_Nations_General_Assembly_Resolution_377', -1]]]}\n",
      "line...\n",
      "{'id': 70041, 'verifiable': 'VERIFIABLE', 'label': 'SUPPORTS', 'claim': '2 Hearts is a musical composition by Minogue.', 'evidence': [[[225394, 230056, '2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0]], [[317953, 306972, '2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0]], [[319638, 308345, '2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0]], [[319643, 308348, '2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0]]]}\n",
      "....\n",
      "docs....\n",
      "['2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 'Kylie_Minogue_singles_discography', 'Kylie_Minogue', 'X_-LRB-Kylie_Minogue_album-RRB-', 'Giving_You_Up']\n",
      "LENGTH = 4\n",
      "for idx and evidence ....... 0\n",
      "0 [[225394, 230056, '2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0]]\n",
      ">>>>>\n",
      "0 [[225394, 230056, '2_Hearts_-LRB-Kylie_Minogue_song-RRB-', -1]]\n",
      "for idx and evidence ....... 1\n",
      "1 [[317953, 306972, '2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0]]\n",
      ">>>>>\n",
      "1 [[317953, 306972, 'Kylie_Minogue_singles_discography', -1]]\n",
      "for idx and evidence ....... 2\n",
      "2 [[319638, 308345, '2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0]]\n",
      ">>>>>\n",
      "2 [[319638, 308345, 'Kylie_Minogue', -1]]\n",
      "for idx and evidence ....... 3\n",
      "3 [[319643, 308348, '2_Hearts_-LRB-Kylie_Minogue_song-RRB-', 0]]\n",
      ">>>>>\n",
      "3 [[319643, 308348, 'X_-LRB-Kylie_Minogue_album-RRB-', -1]]\n",
      "IDX =  3 5\n",
      "************* FINAL LINE *********\n",
      "{'id': 70041, 'verifiable': 'VERIFIABLE', 'label': 'SUPPORTS', 'claim': '2 Hearts is a musical composition by Minogue.', 'evidence': [[[225394, 230056, '2_Hearts_-LRB-Kylie_Minogue_song-RRB-', -1]], [[317953, 306972, 'Kylie_Minogue_singles_discography', -1]], [[319638, 308345, 'Kylie_Minogue', -1]], [[319643, 308348, 'X_-LRB-Kylie_Minogue_album-RRB-', -1], [-1, None, '2_Hearts_-LRB-Kylie_Minogue_song-RRB-', -1]]]}\n",
      "line...\n",
      "{'id': 202314, 'verifiable': 'VERIFIABLE', 'label': 'REFUTES', 'claim': 'The New Jersey Turnpike has zero shoulders.', 'evidence': [[[238335, 240393, 'New_Jersey_Turnpike', 15]]]}\n",
      "....\n",
      "docs....\n",
      "['Interstate_95_in_New_Jersey', 'List_of_turnpikes_in_New_Jersey', 'New_Jersey_Turnpike', 'Newark_Bay_Bridge', 'New_Jersey_Route_495']\n",
      "LENGTH = 1\n",
      "for idx and evidence ....... 0\n",
      "0 [[238335, 240393, 'New_Jersey_Turnpike', 15]]\n",
      ">>>>>\n",
      "0 [[238335, 240393, 'Interstate_95_in_New_Jersey', -1]]\n",
      "IDX =  0 5\n",
      "************* FINAL LINE *********\n",
      "{'id': 202314, 'verifiable': 'VERIFIABLE', 'label': 'REFUTES', 'claim': 'The New Jersey Turnpike has zero shoulders.', 'evidence': [[[238335, 240393, 'Interstate_95_in_New_Jersey', -1], [-1, None, 'Interstate_95_in_New_Jersey', -1], [-1, None, 'List_of_turnpikes_in_New_Jersey', -1], [-1, None, 'New_Jersey_Turnpike', -1], [-1, None, 'Newark_Bay_Bridge', -1]]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fever_root = 'data/'\n",
    "working_dir = 'working/data/'\n",
    "cnt = 0\n",
    "print(\"Saving prepared dataset to {}\".format(\"training/{0}.ns.pages.p{1}.jsonl\".format(split,k)))\n",
    "with open(fever_root + \"data/fever-data/{0}.jsonl\".format(split),\"r\") as f_in:\n",
    "    with open(working_dir + \"training/{0}_pipeline.ns.pages.p{1}.jsonl\".format(split,k),\"w+\") as f_out:\n",
    "        for line in tqdm(f_in.readlines()):\n",
    "            line = json.loads(line)\n",
    "            print(\"line...\")\n",
    "            print(line)\n",
    "            print(\"....\")\n",
    "            doc_names, doc_scores = ranker.closest_docs(line['claim'], k)\n",
    "            pp = list(doc_names)\n",
    "            print(\"docs....\")\n",
    "            print(pp)\n",
    "            print(\"LENGTH =\", len(line['evidence']))\n",
    "            for idx,evidence_group in enumerate(line['evidence']):\n",
    "                print(\"for idx and evidence ....... {}\".format(idx))\n",
    "                print(idx,evidence_group)\n",
    "                for evidence in evidence_group:\n",
    "                    if idx<len(pp):\n",
    "                        evidence[2] = pp[idx]\n",
    "                        evidence[3] = -1\n",
    "                    else:\n",
    "                        evidence[2] = pp[-1]\n",
    "                        evidence[3] = -1\n",
    "                print(\">>>>>\")\n",
    "                print(idx,evidence_group)\n",
    "#             if (idx < len(line['evidence'])):\n",
    "#                 ev = [-1, -1, pp[idx]]\n",
    "            print(\"IDX = \", idx, len(pp))\n",
    "            if len(pp) > idx:\n",
    "                for i in range(len(pp)-1-idx):\n",
    "                    ev = [[-1, None, pp[i], -1]]\n",
    "                    evidence_group.extend(ev)\n",
    "            print(\"************* FINAL LINE *********\")\n",
    "            print(line)\n",
    "            cnt += 1\n",
    "            if cnt > 3:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_dev.jsonl   shared_task_dev.jsonl   train.jsonl\n",
      "paper_test.jsonl  shared_task_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "ls /local/fever-common/data/fever-data/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
      "{\"id\": 163803, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Ukrainian Soviet Socialist Republic was a founding participant of the UN.\", \"evidence\": [[[296950, 288668, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[298602, 290067, \"Ukrainian_Soviet_Socialist_Republic\", 7], [298602, 290067, \"United_Nations\", 0]], [[300696, 291816, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344347, 327887, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344994, 328433, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344997, 328435, \"Ukrainian_Soviet_Socialist_Republic\", 7]]]}\n"
     ]
    }
   ],
   "source": [
    "!head -2 /local/fever-common/data/fever-data/paper_test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the ranker, that tfidf ranker that can rank a documenr given a claim\n",
    "from drqa import retriever\n",
    "tdidf_npz_file = '/local/fever-common/data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz'\n",
    "ranker = retriever.get_class('tfidf')(tfidf_path=tdidf_npz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drqa.retriever import DocDB, utils\n",
    "class FeverDocDB(DocDB):\n",
    "\n",
    "    def __init__(self,path=None):\n",
    "        super().__init__(path)\n",
    "\n",
    "    def get_doc_lines(self, doc_id):\n",
    "        \"\"\"Fetch the raw text of the doc for 'doc_id'.\"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(\n",
    "            \"SELECT lines FROM documents WHERE id = ?\",\n",
    "            (utils.normalize(doc_id),)\n",
    "        )\n",
    "        result = cursor.fetchone()\n",
    "        cursor.close()\n",
    "        return result if result is None else result[0]\n",
    "\n",
    "    def get_non_empty_doc_ids(self):\n",
    "        \"\"\"Fetch all ids of docs stored in the db.\"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(\"SELECT id FROM documents WHERE length(trim(text)) > 0\")\n",
    "        results = [r[0] for r in cursor.fetchall()]\n",
    "        cursor.close()\n",
    "        return results\n",
    "database_path = '/local/fever-common/data/fever/fever.db'\n",
    "#database_path = 'data/data/fever/fever.db'\n",
    "database = FeverDocDB(database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "class SimpleRandom():\n",
    "    instance = None\n",
    "\n",
    "    def __init__(self,seed):\n",
    "        self.seed = seed\n",
    "        self.random = random.Random(seed)\n",
    "\n",
    "    def next_rand(self,a,b):\n",
    "        return self.random.randint(a,b)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_instance():\n",
    "        if SimpleRandom.instance is None:\n",
    "            SimpleRandom.instance = SimpleRandom(SimpleRandom.get_seed())\n",
    "        return SimpleRandom.instance\n",
    "\n",
    "    @staticmethod\n",
    "    def get_seed():\n",
    "        return int(os.getenv(\"RANDOM_SEED\", 12459))\n",
    "\n",
    "    @staticmethod\n",
    "    def set_seeds():\n",
    "\n",
    "        torch.manual_seed(SimpleRandom.get_seed())\n",
    "        if gpu():\n",
    "            torch.cuda.manual_seed_all(SimpleRandom.get_seed())\n",
    "        np.random.seed(SimpleRandom.get_seed())\n",
    "        random.seed(SimpleRandom.get_seed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_dataset_test(split, k=5):\n",
    "    cnt = 0\n",
    "    fever_root = '/local/fever-common/'\n",
    "    working_dir = 'working/data/'\n",
    "    print(\"Saving prepared dataset to {}\".format(\"training/{0}.ns.pages.p{1}.jsonl\".format(split,k)))\n",
    "    with open(fever_root + \"data/fever-data/{0}.jsonl\".format(split),\"r\") as f_in:\n",
    "        with open(working_dir + \"training/{0}_pipeline.pp.pages.p{1}.jsonl\".format(split,k),\"w+\") as f_out:\n",
    "            for line in tqdm(f_in.readlines()):\n",
    "                line = json.loads(line)\n",
    "                doc_names, doc_scores = ranker.closest_docs(line['claim'], k)\n",
    "                pp = list(doc_names)\n",
    "                for idx,evidence_group in enumerate(line['evidence']):\n",
    "                    for evidence in evidence_group:\n",
    "                        if idx<len(pp):\n",
    "                            evidence[2] = pp[idx]\n",
    "                            if line[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "                                evidence[3] = -1\n",
    "                            else:\n",
    "                                evidence[3] = -2\n",
    "                        else:\n",
    "                            evidence[2] = pp[-1] #repeat the last one\n",
    "                            evidence[3] = -2\n",
    "                if len(pp) > idx:\n",
    "                    for i in range(len(pp)-1-idx):\n",
    "                        ev = [[-1, None, pp[i], -2]]\n",
    "                        evidence_group.extend(ev)\n",
    "                #setting evidence of all samples to -1 so that during dataset preparation, we sample lines from the document as per nearest match DrQA\n",
    "                f_out.write(json.dumps(line) + \"\\n\")\n",
    "                #line[\"predicted_pages\"] = pp\n",
    "                #nearest_lines = find_nearest_lines(line['claim'], pp)\n",
    "#                 print(line['evidence'])\n",
    "#                 nearest_lines = find_nearest_lines(line['claim'], line['evidence'][0])\n",
    "#                 print(nearest_lines)\n",
    "#                 cnt += 1\n",
    "#                 #f_out.write(json.dumps(line) + \"\\n\")\n",
    "#                 print(line)\n",
    "#                 if cnt > 3:\n",
    "#                     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rm -rf working/data/training/paper_test_pipeline.pp.pages.p5.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/9999 [00:00<07:52, 21.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prepared dataset to training/paper_test.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [08:33<00:00, 19.46it/s]\n"
     ]
    }
   ],
   "source": [
    "prepare_dataset_test('paper_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, \"Grease_gun_-LRB-tool-RRB-\", -1], [-1, null, \"Grease_gun_-LRB-tool-RRB-\", -2], [-1, null, \"Nasal_sebum\", -2], [-1, null, \"Grease\", -2], [-1, null, \"Thermal_interface_material\", -2]]]}\n",
      "{\"id\": 163803, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Ukrainian Soviet Socialist Republic was a founding participant of the UN.\", \"evidence\": [[[296950, 288668, \"Emblem_of_the_Ukrainian_Soviet_Socialist_Republic\", -2]], [[298602, 290067, \"Flag_of_the_Moldavian_Autonomous_Soviet_Socialist_Republic\", -2], [298602, 290067, \"Flag_of_the_Moldavian_Autonomous_Soviet_Socialist_Republic\", -2]], [[300696, 291816, \"Ukrainian_Republic\", -2]], [[344347, 327887, \"List_of_Presidents_of_Ukraine\", -2]], [[344994, 328433, \"United_Nations_General_Assembly_Resolution_377\", -2]], [[344997, 328435, \"United_Nations_General_Assembly_Resolution_377\", -2]]]}\n",
      "{\"id\": 70041, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"2 Hearts is a musical composition by Minogue.\", \"evidence\": [[[225394, 230056, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", -2]], [[317953, 306972, \"Kylie_Minogue_singles_discography\", -2]], [[319638, 308345, \"Kylie_Minogue\", -2]], [[319643, 308348, \"X_-LRB-Kylie_Minogue_album-RRB-\", -2], [-1, null, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", -2]]]}\n",
      "{\"id\": 202314, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"The New Jersey Turnpike has zero shoulders.\", \"evidence\": [[[238335, 240393, \"Interstate_95_in_New_Jersey\", -2], [-1, null, \"Interstate_95_in_New_Jersey\", -2], [-1, null, \"List_of_turnpikes_in_New_Jersey\", -2], [-1, null, \"New_Jersey_Turnpike\", -2], [-1, null, \"Newark_Bay_Bridge\", -2]]]}\n",
      "{\"id\": 57085, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Legendary Entertainment is the owner of Wanda Cinemas.\", \"evidence\": [[[178035, null, \"Wanda_Cinemas\", -1], [182093, null, \"Wanda_Cinemas\", -1], [314120, null, \"Wanda_Cinemas\", -1], [314126, null, \"Wanda_Cinemas\", -1], [314131, null, \"Wanda_Cinemas\", -1], [-1, null, \"Wanda_Cinemas\", -2], [-1, null, \"Wanda_Group\", -2], [-1, null, \"Legendary_Entertainment\", -2], [-1, null, \"List_of_shows_produced_by_Legendary_Television\", -2]]]}\n",
      "{\"id\": 6032, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"Aruba is the only ABC Island.\", \"evidence\": [[[22769, 28071, \"Frans_Figaroa\", -2]], [[22769, 28072, \"Constitution_of_Aruba\", -2], [-1, null, \"Frans_Figaroa\", -2], [-1, null, \"Constitution_of_Aruba\", -2], [-1, null, \"Outline_of_Aruba\", -2]]]}\n",
      "{\"id\": 176630, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Great white sharks do not prefer dolphins as prey.\", \"evidence\": [[[204612, null, \"Great_white_shark\", -1], [-1, null, \"Great_white_shark\", -2], [-1, null, \"Andre_Hartman\", -2], [-1, null, \"Gansbaai\", -2], [-1, null, \"White_Shark_Cafe\\u0301\", -2]]]}\n",
      "{\"id\": 130048, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"Burbank, California has always been completely void of industry.\", \"evidence\": [[[152264, 167060, \"Burbank_City_Hall\", -2], [-1, null, \"Burbank_City_Hall\", -2], [-1, null, \"Burbank_-LRB-surname-RRB-\", -2], [-1, null, \"Maximum_II\", -2], [-1, null, \"Fort_Mangochi\", -2]]]}\n",
      "{\"id\": 100046, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"The Guthrie Theater's second building began operating in 1963.\", \"evidence\": [[[117690, null, \"Joe_Dowling\", -1], [-1, null, \"Joe_Dowling\", -2], [-1, null, \"Guthrie_Theater_production_history\", -2], [-1, null, \"Guthrie_Theater\", -2], [-1, null, \"Gold_Medal_Park\", -2]]]}\n",
      "{\"id\": 204575, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"Commodore is ranked above a rear admiral.\", \"evidence\": [[[241594, 243126, \"List_of_Supply_Officers_in_the_Royal_Navy_who_have_reached_flag_rank\", -2]], [[241594, 243127, \"Commodore_admiral\", -2], [241594, 243127, \"Commodore_admiral\", -2], [-1, null, \"List_of_Supply_Officers_in_the_Royal_Navy_who_have_reached_flag_rank\", -2], [-1, null, \"Commodore_admiral\", -2], [-1, null, \"Rear_admiral\", -2]]]}\n"
     ]
    }
   ],
   "source": [
    "!head working/data/training/paper_test_pipeline.pp.pages.p5.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "def get_lines(a):\n",
    "    lns = np.array([])\n",
    "    if isinstance(a, str):\n",
    "        return [a]\n",
    "    for l in a:\n",
    "        lns = np.append(lns, l)\n",
    "    return lns.tolist()\n",
    "def prepare_dataset_test_ps(split, k=5):\n",
    "    cnt = 0\n",
    "    fever_root = '/local/fever-common/'\n",
    "    working_dir = 'working/data/'\n",
    "    print(\"Saving prepared dataset to {}\".format(\"training/{0}_pipeline.ps.pages.p{1}.jsonl\".format(split,k)))\n",
    "    with open(working_dir + \"training/{0}_pipeline.pp.pages.p{1}.jsonl\".format(split, k),\"r\") as f_in:\n",
    "        with open(working_dir + \"training/{0}_pipeline.ps.pages.p{1}.jsonl\".format(split,k),\"w+\") as f_out:\n",
    "            for line in tqdm(f_in.readlines()):\n",
    "                line = json.loads(line)\n",
    "                claim = line['claim']\n",
    "                for idx,evidence_group in enumerate(line['evidence']):\n",
    "                    claims = [claim for i in range(len(evidence_group))]\n",
    "                    with ThreadPool(4) as threads:\n",
    "                        results = threads.map(find_nearest, zip(claims, evidence_group))\n",
    "                    docs = [r[0] for r in results]\n",
    "                   \n",
    "                    line_matches = [r[1] for r in results]\n",
    "                \n",
    "                    line_matches = [get_lines(ln) for ln in line_matches]\n",
    "                    line_ids = [r[2] for r in results]\n",
    "     \n",
    "                    predicted_lines = [[a, b] for a, b in zip(line_matches, line_ids)]\n",
    "                    ## match the number of lines matches to evidence\n",
    "                    lines_needed = len(evidence_group)\n",
    "                    for i in range(len(predicted_lines), lines_needed):\n",
    "                        predicted_lines[i] = []\n",
    "                   \n",
    "                    for idx, evidence in enumerate(evidence_group):\n",
    "                        evidence.append(predicted_lines[idx])\n",
    "                f_out.write(json.dumps(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prepared dataset to training/paper_test_pipeline.ps.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [12:49:26<00:00,  4.62s/it]   \n"
     ]
    }
   ],
   "source": [
    "prepare_dataset_test_ps(\"paper_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, \"Grease_gun_-LRB-tool-RRB-\", -1], [-1, null, \"Grease_gun_-LRB-tool-RRB-\", -2], [-1, null, \"Nasal_sebum\", -2], [-1, null, \"Grease\", -2], [-1, null, \"Thermal_interface_material\", -2]]], \"predicted_lines\": [\"The aperture may be of a type that fits closely with a receiving aperture on any number of mechanical devices .\"]}\n",
      "{\"id\": 163803, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Ukrainian Soviet Socialist Republic was a founding participant of the UN.\", \"evidence\": [[[296950, 288668, \"Emblem_of_the_Ukrainian_Soviet_Socialist_Republic\", -2]], [[298602, 290067, \"Flag_of_the_Moldavian_Autonomous_Soviet_Socialist_Republic\", -2], [298602, 290067, \"Flag_of_the_Moldavian_Autonomous_Soviet_Socialist_Republic\", -2]], [[300696, 291816, \"Ukrainian_Republic\", -2]], [[344347, 327887, \"List_of_Presidents_of_Ukraine\", -2]], [[344994, 328433, \"United_Nations_General_Assembly_Resolution_377\", -2]], [[344997, 328435, \"United_Nations_General_Assembly_Resolution_377\", -2]]], \"predicted_lines\": [\"The Uniting for Peace resolution -- also known as the `` Acheson Plan '' -- was adopted 3 November 1950 , after fourteen days of Assembly discussions , by a vote of 52 to 5 -LRB- Czechoslovakia , Poland , the Ukrainian Soviet Socialist Republic , the Union of Soviet Socialist Republics and the Byelorussian Soviet Socialist Republic -RRB- , with 2 abstentions -LRB- India and Argentina -RRB- .\", \"To facilitate prompt action by the General Assembly in the case of a dead-locked Security Council , the resolution created the mechanism of the `` emergency special session '' -LRB- ESS -RRB- , which can be called upon the basis of either a procedural vote in the Security Council , or within twenty-four hours of a request by a majority of UN Members being received by the Secretary-General .\", \"While the `` emergency special session '' framework was established by resolution A/RES/377 A , the UN Charter always contained provisions for `` special sessions '' , which , according to the General Assembly 's current `` Rules of Procedure '' , can be called within fifteen days of a request being received by the Secretary-General .\"]}\n",
      "{\"id\": 70041, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"2 Hearts is a musical composition by Minogue.\", \"evidence\": [[[225394, 230056, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", -2]], [[317953, 306972, \"Kylie_Minogue_singles_discography\", -2]], [[319638, 308345, \"Kylie_Minogue\", -2]], [[319643, 308348, \"X_-LRB-Kylie_Minogue_album-RRB-\", -2], [-1, null, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", -2]]], \"predicted_lines\": [\"`` 2 Hearts '' was released as the lead single from X .\", \"The tour received favorable reviews from critics and was a commercial success , making it Minogue 's most expensive tour at the time .\", \"The album was nominated for a BRIT Award for International Album in 2008 , as well as a Grammy Award for Best Electronic/Dance Album in 2009 , Minogue 's fifth Grammy Award nomination .\", \"Due to Minogue 's lack of success in North America , she traveled there in 2009 for her tour For You , for Me to promote the album .\", \"X is the tenth studio album by Australian singer Kylie Minogue .\", \"`` 2 Hearts '' is a song recorded by Australian singer Kylie Minogue for her tenth studio album , X -LRB- 2007 -RRB- .\", \"The music video for `` 2 Hearts '' was directed by Dawn Shadforth and filmed at Shepperton Studios in London , England .\", \"Upon its release , `` 2 Hearts '' received mixed reviews from music critics .\", \"The song was praised for its departure of musical content and the song 's strength , however the song received criticism for the song 's production and felt it did n't live up to expectations .\", \"The song was Minogue 's first commercial single since she was diagnosed with breast cancer in May 2005 .\"]}\n",
      "{\"id\": 202314, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"The New Jersey Turnpike has zero shoulders.\", \"evidence\": [[[238335, 240393, \"Interstate_95_in_New_Jersey\", -2], [-1, null, \"Interstate_95_in_New_Jersey\", -2], [-1, null, \"List_of_turnpikes_in_New_Jersey\", -2], [-1, null, \"New_Jersey_Turnpike\", -2], [-1, null, \"Newark_Bay_Bridge\", -2]]], \"predicted_lines\": [\"In New Jersey , it runs along much of the main line of the New Jersey Turnpike -LRB- Exit 6 to Exit 14 -RRB- , as well as the Pearl Harbor Memorial Extension -LRB- formerly and still commonly known as the Pennsylvania Turnpike Connector ; from Exit 6 to the Delaware River -- Turnpike Toll Bridge -RRB- and the New Jersey Turnpike 's northern continuation -LRB- from Exit 14 -RRB- to the George Washington Bridge , also maintained by the New Jersey Turnpike Authority , for a total of 77.96 mi .\", \"From the New York direction , I-95 enters New Jersey from the George Washington Bridge , continues south on the New Jersey Turnpike to Exit 6 , and then along the Pearl Harbor Memorial Turnpike Extension , until the Delaware River -- Turnpike Toll Bridge , where it becomes I-276 on the Pennsylvania side .\", \"Located in the northeastern part of the state near New York City , the 11.03 mi Western Spur of the New Jersey Turnpike , considered to be Route 95W by the New Jersey Department of Transportation , is also part of I-95 .\", \"Here , I-95 would follow present-day I-287 to the New Jersey Turnpike in Edison .\", \"In order to fill the gap , the Pennsylvania Turnpike/Interstate 95 Interchange Project has started construction of an interchange between the Pennsylvania Turnpike and I-95 in Bristol Township , Pennsylvania , with I-95 being rerouted to use the Pennsylvania Turnpike to the Delaware River-Turnpike Toll Bridge .\", \"In New Jersey , it runs along much of the main line of the New Jersey Turnpike -LRB- Exit 6 to Exit 14 -RRB- , as well as the Pearl Harbor Memorial Extension -LRB- formerly and still commonly known as the Pennsylvania Turnpike Connector ; from Exit 6 to the Delaware River -- Turnpike Toll Bridge -RRB- and the New Jersey Turnpike 's northern continuation -LRB- from Exit 14 -RRB- to the George Washington Bridge , also maintained by the New Jersey Turnpike Authority , for a total of 77.96 mi .\", \"From the New York direction , I-95 enters New Jersey from the George Washington Bridge , continues south on the New Jersey Turnpike to Exit 6 , and then along the Pearl Harbor Memorial Turnpike Extension , until the Delaware River -- Turnpike Toll Bridge , where it becomes I-276 on the Pennsylvania side .\", \"Located in the northeastern part of the state near New York City , the 11.03 mi Western Spur of the New Jersey Turnpike , considered to be Route 95W by the New Jersey Department of Transportation , is also part of I-95 .\", \"Here , I-95 would follow present-day I-287 to the New Jersey Turnpike in Edison .\", \"In order to fill the gap , the Pennsylvania Turnpike/Interstate 95 Interchange Project has started construction of an interchange between the Pennsylvania Turnpike and I-95 in Bristol Township , Pennsylvania , with I-95 being rerouted to use the Pennsylvania Turnpike to the Delaware River-Turnpike Toll Bridge .\", \"| New Jersey Turnpike\", \"This is a list of turnpike roads , built and operated by private companies in exchange for the privilege of collecting a toll , in the U.S. state of New Jersey , mainly in the 19th century .\", \"| Jersey City and Acquackanonk Turnpike\", \"| Jersey City - Bayonne - Bergen Point\", \"| Jersey City - Newark\", \"The New Jersey Turnpike -LRB- NJTP -RRB- , colloquially known to New Jerseyans as `` the Turnpike '' , is a toll road in New Jersey , maintained by the New Jersey Turnpike Authority .\", \"The Turnpike has 12 ft lanes , 10 ft shoulders , 13 rest areas named after notable residents of New Jersey , and unusual exit signage that was considered the pinnacle of highway building in the 1950s .\", \"The Turnpike is a major thoroughfare providing access to various localities in New Jersey , as well as Delaware , Pennsylvania , and New York .\", \"It provides access from the New Jersey Turnpike 's main roadway to Hudson County , New Jersey and the Holland Tunnel .\", \"It was completed April 4 , 1956 , as part of the New Jersey Turnpike 's Newark Bay -LRB- Hudson County -RRB- Extension , with a ribbon-cutting ceremony led by Governor of New Jersey Robert B. Meyner .\", \"It carries traffic on a toll regulated section of Interstate 78 along the New Jersey Turnpike to interchanges 14 through 14A .\", \"It crosses Newark Bay and connects the cities of Newark -LRB- in Essex County -RRB- and Bayonne -LRB- in Hudson County -RRB- in New Jersey , United States .\", \"The turnpike route creates the border between Bayonne and Jersey City and then runs northward along Port Jersey , Liberty State Park , and Downtown Jersey City .\"]}\n"
     ]
    }
   ],
   "source": [
    "!head working/data/training/paper_test_pipeline.ps.pages.p5.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drqascripts.retriever.build_tfidf_lines import OnlineTfidfDocRanker\n",
    "import math\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import numpy as np\n",
    "class RankArgs:\n",
    "    def __init__(self):\n",
    "        self.ngram = 2\n",
    "        self.hash_size = int(math.pow(2,24))\n",
    "        self.tokenizer = \"simple\"\n",
    "        self.num_workers = None\n",
    "args = RankArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_line_test(data_map):\n",
    "    \n",
    "    docs, claims = data_map\n",
    "    claim = claims[-1]\n",
    "    print(docs)\n",
    "    \n",
    "    lines = database.get_doc_lines(doc)\n",
    "    \n",
    "    ### if this is from annotated evidences\n",
    "    if line > -1: #we will not hit this for the test dataset, we have cleared off all the page indices from the test dataset\n",
    "        return lines.split(\"\\n\")[line].split(\"\\t\")[1] #get all the lines from the document and match with the line ids that were annotated as evidence by the human annotators\n",
    "    elif line <= -2:\n",
    "        #TODO: nearest 5 sentences from the document\n",
    "        non_empty_lines = [line.split(\"\\t\")[1] for line in lines.split(\"\\n\") if len(line.split(\"\\t\"))>1 and len(line.split(\"\\t\")[1].strip())]\n",
    "        tfidf = OnlineTfidfDocRanker(args,[line for line in non_empty_lines],None)\n",
    "        line_ids,scores = tfidf.closest_docs(claim,5)\n",
    "        return non_empty_lines[SimpleRandom.get_instance().next_rand(0,len(non_empty_lines)-1)]\n",
    "    else: ### if this is from not enough info evidences, NearestP method, to sample \"a\" single sentence randomly from the nearest page match\n",
    "        non_empty_lines = [line.split(\"\\t\")[1] for line in lines.split(\"\\n\") if len(line.split(\"\\t\"))>1 and len(line.split(\"\\t\")[1].strip())]\n",
    "        return non_empty_lines[SimpleRandom.get_instance().next_rand(0,len(non_empty_lines)-1)]\n",
    "    \n",
    "\n",
    "def find_nearest(claim_doc):\n",
    "    claim, evidence = claim_doc\n",
    "    doc = evidence[2]\n",
    "    tag = evidence[3]\n",
    "    lines = database.get_doc_lines(doc)\n",
    "    non_empty_lines = [line.split(\"\\t\")[1] for line in lines.split(\"\\n\") if len(line.split(\"\\t\"))>1 and len(line.split(\"\\t\")[1].strip())]\n",
    "    if tag == -2:\n",
    "        tfidf = OnlineTfidfDocRanker(args,[line for line in non_empty_lines],None)\n",
    "        res = tfidf.closest_docs(claim, 5)\n",
    "        line_ids,scores = tfidf.closest_docs(claim, 5)\n",
    "        return doc, np.array(non_empty_lines)[line_ids], line_ids\n",
    "    else:\n",
    "        return doc, non_empty_lines[SimpleRandom.get_instance().next_rand(0,len(non_empty_lines)-1)], []\n",
    "    \n",
    "    \n",
    "    \n",
    "def tfidf_claim(data_map):\n",
    "    #print(data_map)\n",
    "#     docs = data['docs']\n",
    "#     claim = data['claim']\n",
    "    claims, doc = data_map\n",
    "    claim = claims[-1]\n",
    "    #print(docs)\n",
    "#     print(docs)\n",
    "    ranked_lines = []\n",
    "    for doc in docs:\n",
    "        lines = database.get_doc_lines(doc)\n",
    "        non_empty_lines = [line.split(\"\\t\")[1] for line in lines.split(\"\\n\") if len(line.split(\"\\t\"))>1 and len(line.split(\"\\t\")[1].strip())]\n",
    "        tfidf = OnlineTfidfDocRanker(args,[line for line in non_empty_lines],None)\n",
    "        line_ids,scores = tfidf.closest_docs(claim, 5)\n",
    "#         print(line_ids)\n",
    "#         print(non_empty_lines)\n",
    "        ranked_lines.extend(np.array(non_empty_lines)[line_ids])\n",
    "    return ranked_lines\n",
    "\n",
    "# def find_nearest_lines(data):\n",
    "#     claim = data['claim']\n",
    "#     claims = [claim for i in range(len(data['evidence']))]\n",
    "#     with ThreadPool(4) as threads:\n",
    "#         results = threads.map(find_nearest, zip(claims, data['evidence']))\n",
    "#     return results\n",
    "\n",
    "def find_nearest_lines(claim, pp):\n",
    "    #claim = data['claim']\n",
    "    claims = [claim for i in range(len(pp))]\n",
    "    with ThreadPool(4) as threads:\n",
    "        results = threads.map(find_nearest, zip(claims, pp))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[133128, None, 'Grease_gun_-LRB-tool-RRB-', -1], [-1, None, 'Grease_gun_-LRB-tool-RRB-', -2], [-1, None, 'Nasal_sebum', -2], [-1, None, 'Grease', -2], [-1, None, 'Thermal_interface_material', -2]]\n",
      "Line count = 5\n",
      "[('Grease_gun_-LRB-tool-RRB-', 'Hand-powered , where there is no trigger mechanism , and the grease is forced through the aperture by the back-pressure built up by pushing on the butt of the grease gun , which slides a piston through the body of the tool , pumping grease out of the aperture .', None), ('Grease_gun_-LRB-tool-RRB-', array([], dtype='<U261'), []), ('Nasal_sebum', array([], dtype='<U304'), []), ('Grease', array([], dtype='<U139'), []), ('Thermal_interface_material', array([], dtype='<U258'), [])]\n",
      "CPU times: user 7.52 s, sys: 2.65 s, total: 10.2 s\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pp = [[[133128, None, 'Grease_gun_-LRB-tool-RRB-', -1], [-1, None, 'Grease_gun_-LRB-tool-RRB-', -2], \n",
    "       [-1, None, 'Nasal_sebum', -2], [-1, None, 'Grease', -2], [-1, None, 'Thermal_interface_material', -2]]]\n",
    "print(pp[0])\n",
    "claim = \"Grease had bad reviews.\"\n",
    "results = find_nearest_lines(claim, pp[0])\n",
    "print(\"Line count = {}\".format(len(results)))\n",
    "print(results)\n",
    "# print(matches)\n",
    "# lines = np.array([])\n",
    "# for l in matches:\n",
    "#     lines = np.append(lines, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Air-powered -LRB- pneumatic -RRB- , where compressed air is directed to the gun by hoses , the air pressure serving to force the grease through the aperture .'],\n",
       "      dtype='<U304')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
