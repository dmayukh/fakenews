{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on dev dataset\n",
    "\n",
    "The test dataset will use predicted pages and predicted sentences. \n",
    "\n",
    "The predictions are generated via a seperate process in the our pipeline which must be executed before this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.readers.DatasetReader import DatasetReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the label encoder, we will generate them from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:02<00:00, 48520.31it/s]\n",
      "100%|██████████| 145449/145449 [00:01<00:00, 77898.07it/s] \n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/training/train.ns.pages.p5.jsonl'\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=None, database_path='/local/fever-common/data/fever/fever.db')\n",
    "raw, data = dsreader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = dsreader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the label encoder\n",
    "import pickle\n",
    "with open('working/data/training/label_encoder_train.pkl', 'wb') as f:\n",
    "    pickle.dump(dsreader.labelencoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dev data\n",
    "Use the saved label encodings from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 12199.55it/s]\n",
      "100%|██████████| 9999/9999 [00:00<00:00, 48049.33it/s]\n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/dev/paper_dev_pipeline.ps.pages.p5.jsonl'\n",
    "label_checkpoint_file = 'working/data/training/label_encoder_train.pkl'\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=label_checkpoint_file, database_path='/local/fever-common/data/fever/fever.db', type='test')\n",
    "raw_test, test_data = dsreader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(3,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "ds_test = dsreader.get_dataset()\n",
    "print(ds_test.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the BERT tokenizer\n",
    "\n",
    "The FEVER vocab file is build using tokens that were concatenations of the train and the dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "import tensorflow_text as text\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "vocab_file_out = 'working/data/fever_vocab.txt'\n",
    "pt_tokenizer = text.BertTokenizer(vocab_file_out, **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the tensor dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: (60,), types: tf.int64>\n",
      "<MapDataset shapes: (60,), types: tf.int64>\n",
      "<BatchDataset shapes: (((64, 60), (64, 60)), (64, 3)), types: ((tf.int64, tf.int64), tf.int32)>\n",
      "((TensorSpec(shape=(64, 60), dtype=tf.int64, name=None), TensorSpec(shape=(64, 60), dtype=tf.int64, name=None)), TensorSpec(shape=(64, 3), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 60\n",
    "BUFFER_SIZE = 32000\n",
    "def tokenize_and_pad(text, max_len):\n",
    "    segment = pt_tokenizer.tokenize(text).merge_dims(1, -1)\n",
    "    inp = segment.to_tensor(shape=[None, max_len])\n",
    "    return inp[0]\n",
    "\n",
    "h = ds_test.map(lambda x, y: tokenize_and_pad(x[0], MAX_SEQ_LEN))\n",
    "e = ds_test.map(lambda x, y: tokenize_and_pad(x[1], MAX_SEQ_LEN))\n",
    "l = ds_test.map(lambda x, y: y)\n",
    "print(h)\n",
    "print(e)\n",
    "f = tf.data.Dataset.zip((h,e))\n",
    "d = tf.data.Dataset.zip((f,l))\n",
    "# do not shuffle\n",
    "dataset_test = d.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_test)\n",
    "print(dataset_test.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the prefilled embedding matrix from glove 300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arr_0']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile = np.load(\"working/data/embedding_mappings_300d.npz\")\n",
    "npzfile.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = npzfile['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "hypothesis (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "evidence (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    2400300     hypothesis[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    2400300     evidence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 300)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 300)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, None, 600)    1442400     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 600)    1442400     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, None, None)   0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, None, None)   0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, None)   0           permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, None)   0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, 600)    0           lambda_1[0][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 600)    0           lambda[0][0]                     \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, None, 600)    0           bidirectional[0][0]              \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, None, 600)    0           bidirectional[0][0]              \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, None, 600)    0           bidirectional_1[0][0]            \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 600)    0           bidirectional_1[0][0]            \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 2400)   0           bidirectional[0][0]              \n",
      "                                                                 dot_1[0][0]                      \n",
      "                                                                 subtract[0][0]                   \n",
      "                                                                 multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 2400)   0           bidirectional_1[0][0]            \n",
      "                                                                 dot_2[0][0]                      \n",
      "                                                                 subtract_1[0][0]                 \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Compresser (TimeDistributed)    (None, None, 300)    720300      concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 300)    0           Compresser[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 300)    0           Compresser[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "finaldecoder (Bidirectional)    (None, None, 600)    1442400     dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 600)          0           finaldecoder[0][0]               \n",
      "                                                                 finaldecoder[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 600)          0           finaldecoder[0][0]               \n",
      "                                                                 finaldecoder[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2400)         0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "                                                                 global_average_pooling1d[1][0]   \n",
      "                                                                 global_max_pooling1d[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 2400)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense300_ (Dense)               (None, 100)          240100      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100)          0           dense300_[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "judge300_ (Dense)               (None, 3)            303         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,088,503\n",
      "Trainable params: 5,287,903\n",
      "Non-trainable params: 4,800,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "\n",
    "vocab_size= 8000\n",
    "dim = 300\n",
    "inp1 = keras.Input(shape=(None, ), name = \"hypothesis\")\n",
    "inp2 = keras.Input(shape=(None, ), name = \"evidence\")\n",
    "\n",
    "embedding_hyp_layer = Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False)\n",
    "embedding_evi_layer = Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False)\n",
    "\n",
    "\n",
    "x_hyp = embedding_hyp_layer(inp1)\n",
    "x_hyp = tf.keras.layers.Dropout(0.5)(x_hyp)\n",
    "\n",
    "x_evi = embedding_evi_layer(inp2)\n",
    "x_evi = tf.keras.layers.Dropout(0.5)(x_evi)\n",
    "\n",
    "\n",
    "lstm_layer1 = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim), return_sequences=True))(x_hyp)\n",
    "\n",
    "lstm_layer2 = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim), return_sequences=True))(x_evi)\n",
    "\n",
    "\n",
    "F_p, F_h = lstm_layer1, lstm_layer2\n",
    "Eph = keras.layers.Dot(axes=(2, 2))([F_h, F_p])  # [batch_size, Hsize, Psize]\n",
    "Eh = Lambda(lambda x: keras.activations.softmax(x))(Eph)  # [batch_size, Hsize, Psize]\n",
    "Ep = keras.layers.Permute((2, 1))(Eph)  # [batch_size, Psize, Hsize)\n",
    "Ep = Lambda(lambda x: keras.activations.softmax(x))(Ep)  # [batch_size, Psize, Hsize]\n",
    "\n",
    "# 4, Normalize score matrix, encoder premesis and get alignment\n",
    "PremAlign = keras.layers.Dot((2, 1))([Ep, lstm_layer2]) # [-1, Psize, dim]\n",
    "HypoAlign = keras.layers.Dot((2, 1))([Eh, lstm_layer1]) # [-1, Hsize, dim]\n",
    "mm_1 = keras.layers.Multiply()([lstm_layer1, PremAlign])\n",
    "mm_2 = keras.layers.Multiply()([lstm_layer2, HypoAlign])\n",
    "sb_1 = keras.layers.Subtract()([lstm_layer1, PremAlign])\n",
    "sb_2 = keras.layers.Subtract()([lstm_layer2, HypoAlign])\n",
    "    \n",
    "\n",
    "# concat [a_, a~, a_ * a~, a_ - a~], isto za b_, b~\n",
    "PremAlign = keras.layers.Concatenate()([lstm_layer1, PremAlign, sb_1, mm_1,])  # [batch_size, Psize, 2*unit]\n",
    "HypoAlign = keras.layers.Concatenate()([lstm_layer2, HypoAlign, sb_2, mm_2])  # [batch_size, Hsize, 2*unit]\n",
    "\n",
    "\n",
    "# ff layer w/RELU activation\n",
    "Compresser = tf.keras.layers.TimeDistributed(Dense(300,\n",
    "                                   kernel_regularizer=l2(0.0),\n",
    "                                   bias_regularizer=l2(0.0),\n",
    "                                   activation='relu'),\n",
    "                             name='Compresser')\n",
    "\n",
    "PremAlign = Compresser(PremAlign)\n",
    "HypoAlign = Compresser(HypoAlign)\n",
    "    \n",
    "\n",
    "Decoder = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim), return_sequences=True), name='finaldecoder')\n",
    "\n",
    "\n",
    "PremAlign = Dropout(0.5)(PremAlign)\n",
    "HypoAlign = Dropout(0.5)(HypoAlign)\n",
    "final_p = Decoder(PremAlign)\n",
    "final_h = Decoder(HypoAlign)\n",
    "\n",
    "\n",
    "AveragePooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "MaxPooling = tf.keras.layers.GlobalMaxPooling1D()\n",
    "\n",
    "# AveragePooling = Lambda(lambda x: K.mean(x, axis=1)) # outs [-1, dim]\n",
    "# MaxPooling = Lambda(lambda x: K.max(x, axis=1)) # outs [-1, dim]\n",
    "avg_p = AveragePooling(final_p)\n",
    "avg_h = AveragePooling(final_h)\n",
    "max_p = MaxPooling(final_p)\n",
    "max_h = MaxPooling(final_h)\n",
    "# concat of avg and max pooling for hypothesis and premise\n",
    "Final = keras.layers.Concatenate()([avg_p, max_p, avg_h, max_h])\n",
    "# dropout layer\n",
    "Final = Dropout(0.5)(Final)\n",
    "# ff layer w/tanh activation\n",
    "Final = Dense(100,\n",
    "              kernel_regularizer=l2(0.0),\n",
    "              bias_regularizer=l2(0.0),\n",
    "              name='dense300_',\n",
    "              activation='tanh')(Final)\n",
    "\n",
    "# last dropout factor\n",
    "factor = 1\n",
    "# if self.LastDropoutHalf:\n",
    "#     factor = 2\n",
    "Final = Dropout(0.5 / factor)(Final)\n",
    "\n",
    "# softmax classifier\n",
    "Final = Dense(3,\n",
    "              activation='softmax',\n",
    "              name='judge300_')(Final)\n",
    "model = tf.keras.Model(inputs=[inp1, inp2], outputs=Final)\n",
    "\n",
    "LearningRate = 4e-4\n",
    "GradientClipping = 10.0\n",
    "\n",
    "# Optimizer = keras.optimizers.Adam(lr = LearningRate,\n",
    "#             clipnorm = GradientClipping)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model accuracy on DEV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 120s 753ms/step - loss: 1.9024 - accuracy: 0.5639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.902360200881958, 0.5639022588729858]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath = 'tmp/attention_esim/checkpoint_fever_rte_esim'\n",
    "model.load_weights(checkpoint_filepath)\n",
    "model.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the FEVER score\n",
    "\n",
    "- Strictly correct: when all the evidences predicted are correct and the predicted label is correct\n",
    "- Correct: when only the predicted label is correct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred_proba, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'working/data/dev_y_preds.npz'\n",
    "np.savez(outfile, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_y = dataset_test.map(lambda f, l: l)\n",
    "y_test_onehot = []\n",
    "for d in ds_y.batch(1):\n",
    "    for d1 in d:\n",
    "        y_test_onehot.append(d1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([np.argmax(a, axis=1) for a in y_test_onehot]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'working/data/dev_y_tests.npz'\n",
    "np.savez(outfile, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'NOT ENOUGH INFO']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d['label_text'] for d in test_data[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.87      0.61      3325\n",
      "           1       0.81      0.30      0.44      3331\n",
      "           2       0.65      0.52      0.58      3328\n",
      "\n",
      "    accuracy                           0.56      9984\n",
      "   macro avg       0.65      0.56      0.54      9984\n",
      "weighted avg       0.65      0.56      0.54      9984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "#['NOT ENOUGH INFO', 'REFUTES', 'SUPPORTS'] == [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In dataset type B\n",
    "\n",
    "We have predicted pages and predicted sentences per page for each claim. We need the predictions for those pages and sentences to compute the FEVER score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[108548,\n",
       "   None,\n",
       "   'Colin_Kaepernick',\n",
       "   -1,\n",
       "   [[\"Kaepernick began his professional career as a backup to Alex Smith , but became the 49ers ' starter in the middle of the 2012 season after Smith suffered a concussion .\"],\n",
       "    []]],\n",
       "  [-1,\n",
       "   None,\n",
       "   'Colin_Kaepernick',\n",
       "   -2,\n",
       "   [[\"He remained the team 's starting quarterback for the rest of the season and went on to lead the 49ers to their first Super Bowl appearance since 1994 , losing to the Baltimore Ravens .\",\n",
       "     'Colin Rand Kaepernick -LRB- -LSB- ` kæpərnɪk -RSB- ; born November 3 , 1987 -RRB- is an American football quarterback who is currently a free agent .',\n",
       "     'In 2016 , Kaepernick gained national attention when he began protesting by not standing while the United States national anthem was being performed before the start of games , motivated by what he viewed as the oppression of non-white races in the U.S. His actions prompted a wide variety of responses , including additional athletes in the NFL and other U.S. sports leagues protesting the anthem in various ways .',\n",
       "     \"Kaepernick began his professional career as a backup to Alex Smith , but became the 49ers ' starter in the middle of the 2012 season after Smith suffered a concussion .\",\n",
       "     'In the following seasons , Kaepernick lost and won back his starting job , with the 49ers missing the playoffs for three years consecutively .'],\n",
       "    [4, 0, 7, 3, 6]]],\n",
       "  [-1,\n",
       "   None,\n",
       "   'Pistol_offense',\n",
       "   -2,\n",
       "   [['The Wolf Pack also became the first team in college football history with three 1,000-yard rushers in the same season : quarterback Colin Kaepernick and running backs Luke Lippincott and Vai Taua .',\n",
       "     'The pistol has also made the transition to the NFL , mainly being used by the Carolina Panthers with Cam Newton and Robert Griffin III of the Washington Redskins , as well as the aforementioned Colin Kaepernick with the San Francisco 49ers , who in the NFL Playoffs versus the Green Bay Packers set the all-time single game rushing record for a quarterback with 181 yards .',\n",
       "     'Using the Pistol Offense , during the 2009 season , Nevada led the nation in rushing at 345 yards a game and were second in total offense at 506 yards .',\n",
       "     'The pistol offense is an American football formation and strategy was copied from coach Michael Taylor of Mill Valley by Chris Ault in 2004 while he was head coach at the University of Nevada , Reno .',\n",
       "     \"While the pistol offense has been experimented with by dozens of college football teams such as LSU , Syracuse , Indiana , and Missouri , Ault 's Nevada Wolf Pack is most strongly associated with the formation .\"],\n",
       "    [14, 16, 13, 0, 12]]],\n",
       "  [-1,\n",
       "   None,\n",
       "   '2016_San_Francisco_49ers_season',\n",
       "   -2,\n",
       "   [[\"The 2016 San Francisco 49ers season was the franchise 's 67th season in the National Football League , the 71st overall , the third playing its home games at Levi 's Stadium , and the only season under head coach Chip Kelly .\",\n",
       "     'The season was notable when Colin Kaepernick sat during the National Anthem in a preseason game against San Diego .',\n",
       "     'Despite the distraction , Kaepernick was named starter for Week 6 .',\n",
       "     'The 49ers defense also set an NFL record by allowing a 100 yard rusher in seven straight games and easily finished last in the league in rush defense .'],\n",
       "    [0, 7, 8, 6]]],\n",
       "  [-1,\n",
       "   None,\n",
       "   '2014_San_Francisco_49ers_season',\n",
       "   -2,\n",
       "   [[\"The 2014 San Francisco 49ers season was the franchise 's 65th season in the National Football League , the 69th overall and the fourth under the head coach/general manager tandem of Jim Harbaugh and Trent Baalke .\",\n",
       "     'The 49ers finished 30th in passing yards per game , 25th in the league averaging just 19.1 points per game while Colin Kaepernick was also sacked 52 times during the season , a team record .',\n",
       "     \"Despite missing significant starters on the defensive side of the ball due to injuries for most of the season -LRB- including Pro-Bowlers NaVorro Bowman and Patrick Willis -RRB- , the 49ers still finished with the NFL 's fifth ranked defense in total yards while also leading the league in interceptions with 23 , led by cornerback Perrish Cox , who had five .\",\n",
       "     'The 49ers defense also finished fourth in the league in total takeaways with 29 .',\n",
       "     'From Weeks 7 -- 15 , the 49ers averaged just 13.8 points per game , last in the league .'],\n",
       "    [0, 9, 6, 7, 10]]]]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test[:1][0]['evidence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In dataset type A\n",
    "\n",
    "We also need the original annotated pages and the sentences from the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 12474.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#!head -2 working/data/training/paper_test.ns.pages.p5.jsonl\n",
    "#re-read the original data, for the annotated evidences\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "class Reader:\n",
    "    def __init__(self,encoding=\"utf-8\"):\n",
    "        self.enc = encoding\n",
    "    def read(self,file):\n",
    "        with open(file,\"r\",encoding = self.enc) as f:\n",
    "            return self.process(f)\n",
    "    def process(self,f):\n",
    "        pass\n",
    "\n",
    "class JSONLineReader(Reader):\n",
    "    def process(self,fp):\n",
    "        data = []\n",
    "        for line in tqdm(fp.readlines()):\n",
    "            data.append(json.loads(line.strip()))\n",
    "        return data\n",
    "    \n",
    "jlr = JSONLineReader()\n",
    "split = 'paper_dev'\n",
    "working_dir = 'working/data/'\n",
    "k = 5\n",
    "test_data_file = working_dir + \"training/{0}.ns.pages.p{1}.jsonl\".format(split, k)\n",
    "data_orig = jlr.read(test_data_file)\n",
    "orig_evidences = [d['evidence'] for d in data_orig[:len(y_test)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the final predictions, for the label, the predicted pages and the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1400it [00:00, 13901.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to training/paper_dev_predicted_pipeline.ps.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9984it [00:00, 18979.29it/s]\n"
     ]
    }
   ],
   "source": [
    "split = 'paper_dev_predicted'\n",
    "k = 5\n",
    "with open(working_dir + \"training/{0}_pipeline.ps.pages.p{1}.jsonl\".format(split,k),\"w+\") as f_out:\n",
    "    print(\"Saving to training/{0}_pipeline.ps.pages.p{1}.jsonl\".format(split,k))\n",
    "    for rec, orig, true_label, predicted_label in tqdm(zip(raw_test[:len(y_test)], orig_evidences, y_test, y_pred)):\n",
    "        evs = []\n",
    "        for evidence_group in rec['evidence']:\n",
    "            for evidence in evidence_group:\n",
    "                #print(evidence)\n",
    "                if evidence[0] > -1:\n",
    "                    ev = [evidence[0], evidence[1], evidence[2], evidence[4][1]]\n",
    "                    evs.append(ev)\n",
    "\n",
    "        out = {'true_label': str(true_label), 'predicted_label': str(predicted_label), 'orig': orig, 'pred': evs}\n",
    "\n",
    "        f_out.write(json.dumps(out) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the predictions from the file to use in fever scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9984/9984 [00:00<00:00, 43543.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "jlr = JSONLineReader()\n",
    "split = 'paper_dev_predicted'\n",
    "working_dir = 'working/data/'\n",
    "k = 5\n",
    "test_data_file = working_dir + \"training/{0}_pipeline.ps.pages.p{1}.jsonl\".format(split, k)\n",
    "predicted_results = jlr.read(test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fever_score(predicted_results):\n",
    "    strictly_correct = 0\n",
    "    correct = 0\n",
    "    cnt = 0\n",
    "    for d in tqdm(predicted_results):\n",
    "        true_label = d['true_label']\n",
    "        predicted_label = d['predicted_label']\n",
    "        true_evidence = d['orig']\n",
    "        predicted_evidence = d['pred']\n",
    "        te = {}\n",
    "        pe = {}\n",
    "        #is correct?\n",
    "        if (true_label == predicted_label):\n",
    "            correct += 1\n",
    "            # is strictly correct?\n",
    "            if (true_label != '0') and (true_label==predicted_label):\n",
    "                for eg in true_evidence:\n",
    "                    for e in eg:\n",
    "                        if e[2] in te:\n",
    "                            te[e[2]].append(e[3])\n",
    "                        else:\n",
    "                            te[e[2]]= [e[3]]    \n",
    "\n",
    "                for e in predicted_evidence:\n",
    "                    if e[2] in pe:\n",
    "                        pe[e[2]].append(e[3])\n",
    "                    else:\n",
    "                        pe[e[2]]= [e[3]]\n",
    "\n",
    "                # for each annotated evidence, see if we predicted the evidences\n",
    "                # did we correctly predict all pages?\n",
    "                all_pages = all([k1 in pe.keys() for k1 in te.keys()])\n",
    "                if all_pages:\n",
    "                    #for the pages we predicted, did we predict all the sentences?\n",
    "                    for k in te.keys():\n",
    "                        if k in pe: # the page is predicted\n",
    "                            true_sents = np.unique(te[k])\n",
    "                            pre_sents = np.unique(pe[k][0])\n",
    "                            #if all the true sentences were predicted\n",
    "                            match = all([actual_sent in pre_sents for actual_sent in true_sents])\n",
    "                            #if match and (len(true_sents) == len(pre_sents)):\n",
    "                            #we are predicting 5 lines per page, so the count may not match with the true evidence lines\n",
    "                            if match:\n",
    "                                strictly_correct += 1\n",
    "            elif (true_label == '0') and (true_label == predicted_label): # not enough info\n",
    "                    strictly_correct += 1\n",
    "    noevscore = np.round(correct/len(predicted_results)*100,2)\n",
    "    score = np.round(strictly_correct/len(predicted_results)*100,2)\n",
    "    print(\"noevscore={}, score={}\".format(noevscore, score))\n",
    "    return noevscore, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9984/9984 [00:00<00:00, 49454.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noevscore=56.39, score=35.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(56.39, 35.96)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fever_score(predicted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From original FEVER paper\n",
    "The classification\n",
    "accuracy is <b>32.57%</b>. Ignoring the requirement for\n",
    "correct evidence (NoScoreEv) the accuracy is\n",
    "<b>52.09%</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
