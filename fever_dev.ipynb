{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-tender",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-arbitration",
   "metadata": {},
   "source": [
    "#### Evaluate the model on dev dataset\n",
    "\n",
    "The test dataset will use predicted pages and predicted sentences. \n",
    "\n",
    "The predictions are generated via a seperate process in the our pipeline which must be executed before this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "automatic-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.dataset.DatasetReader import DatasetReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-tulsa",
   "metadata": {},
   "source": [
    "We need the label encoder, we will generate them from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "usual-breakdown",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:01<00:00, 83508.15it/s] \n",
      "100%|██████████| 145449/145449 [00:01<00:00, 141248.98it/s]\n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/training/train.ns.pages.p5.jsonl'\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=None, database_path='data/data/fever/fever.db')\n",
    "raw, data = dsreader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = dsreader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rolled-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the label encoder\n",
    "import pickle\n",
    "with open('working/data/training/label_encoder_train.pkl', 'wb') as f:\n",
    "    pickle.dump(dsreader.labelencoder, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-level",
   "metadata": {},
   "source": [
    "### Load dev data\n",
    "Use the saved label encodings from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "foreign-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 17616.52it/s]\n",
      "100%|██████████| 9999/9999 [00:00<00:00, 133225.90it/s]\n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/training/paper_dev_pipeline.ps.pages.p5.jsonl'\n",
    "label_checkpoint_file = 'working/data/training/label_encoder_train.pkl'\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=label_checkpoint_file, database_path='data/data/fever/fever.db', type='test')\n",
    "raw_test, test_data = dsreader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "patient-polyester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(3,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "ds_test = dsreader.get_dataset()\n",
    "print(ds_test.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-version",
   "metadata": {},
   "source": [
    "#### Load the BERT tokenizer\n",
    "\n",
    "The FEVER vocab file is build using tokens that were concatenations of the train and the dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "authentic-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "import tensorflow_text as text\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "vocab_file_out = 'working/data/fever_vocab.txt'\n",
    "pt_tokenizer = text.BertTokenizer(vocab_file_out, **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-syntax",
   "metadata": {},
   "source": [
    "#### Prepare the tensor dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hearing-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "innocent-mainland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: (60,), types: tf.int64>\n",
      "<MapDataset shapes: (60,), types: tf.int64>\n",
      "<BatchDataset shapes: (((64, 60), (64, 60)), (64, 3)), types: ((tf.int64, tf.int64), tf.int32)>\n",
      "((TensorSpec(shape=(64, 60), dtype=tf.int64, name=None), TensorSpec(shape=(64, 60), dtype=tf.int64, name=None)), TensorSpec(shape=(64, 3), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 60\n",
    "BUFFER_SIZE = 32000\n",
    "def tokenize_and_pad(text, max_len):\n",
    "    segment = pt_tokenizer.tokenize(text).merge_dims(1, -1)\n",
    "    inp = segment.to_tensor(shape=[None, max_len])\n",
    "    return inp[0]\n",
    "\n",
    "h = ds_test.map(lambda x, y: tokenize_and_pad(x[0], MAX_SEQ_LEN))\n",
    "e = ds_test.map(lambda x, y: tokenize_and_pad(x[1], MAX_SEQ_LEN))\n",
    "l = ds_test.map(lambda x, y: y)\n",
    "print(h)\n",
    "print(e)\n",
    "f = tf.data.Dataset.zip((h,e))\n",
    "d = tf.data.Dataset.zip((f,l))\n",
    "# do not shuffle\n",
    "dataset_test = d.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_test)\n",
    "print(dataset_test.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-phrase",
   "metadata": {},
   "source": [
    "#### Load the prefilled embedding matrix from glove 300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moderate-storm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arr_0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile = np.load(\"working/data/embedding_mappings_300d.npz\")\n",
    "npzfile.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "welcome-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = npzfile['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-wichita",
   "metadata": {},
   "source": [
    "from mda.src.model.esim import esim#### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sensitive-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.model.esim import esim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "relative-defensive",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "hypothesis (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "evidence (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    2400300     hypothesis[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    2400300     evidence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 300)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 300)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, None, 600)    1442400     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 600)    1442400     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, None, None)   0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, None, None)   0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, None)   0           permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, None)   0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, 600)    0           lambda_1[0][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 600)    0           lambda[0][0]                     \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, None, 600)    0           bidirectional[0][0]              \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, None, 600)    0           bidirectional[0][0]              \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, None, 600)    0           bidirectional_1[0][0]            \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 600)    0           bidirectional_1[0][0]            \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 2400)   0           bidirectional[0][0]              \n",
      "                                                                 dot_1[0][0]                      \n",
      "                                                                 subtract[0][0]                   \n",
      "                                                                 multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 2400)   0           bidirectional_1[0][0]            \n",
      "                                                                 dot_2[0][0]                      \n",
      "                                                                 subtract_1[0][0]                 \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Compresser (TimeDistributed)    (None, None, 300)    720300      concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 300)    0           Compresser[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 300)    0           Compresser[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "finaldecoder (Bidirectional)    (None, None, 600)    1442400     dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 600)          0           finaldecoder[0][0]               \n",
      "                                                                 finaldecoder[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 600)          0           finaldecoder[0][0]               \n",
      "                                                                 finaldecoder[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2400)         0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "                                                                 global_average_pooling1d[1][0]   \n",
      "                                                                 global_max_pooling1d[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 2400)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense300_ (Dense)               (None, 100)          240100      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100)          0           dense300_[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "judge300_ (Dense)               (None, 3)            303         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,088,503\n",
      "Trainable params: 5,287,903\n",
      "Non-trainable params: 4,800,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "esim_model = esim(embedding_matrix=embedding_matrix, vocab_size = 8000, embedding_dim=300, alignment_dense_dim=300, final_dense_dim=100)\n",
    "model = esim_model.build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-youth",
   "metadata": {},
   "source": [
    "#### Evaluate model accuracy on DEV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "decent-spokesman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 25s 143ms/step - loss: 1.9557 - accuracy: 0.5501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9557477235794067, 0.5500801205635071]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath = 'tmp/attention_esim/checkpoint_fever_rte_esim'\n",
    "model.load_weights(checkpoint_filepath)\n",
    "model.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-player",
   "metadata": {},
   "source": [
    "#### Calculate the FEVER score\n",
    "\n",
    "- Strictly correct: when all the evidences predicted are correct and the predicted label is correct\n",
    "- Correct: when only the predicted label is correct "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-certificate",
   "metadata": {},
   "source": [
    "#### Compute the precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lined-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "satisfied-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred_proba, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "associate-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'working/data/dev_y_preds.npz'\n",
    "np.savez(outfile, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sublime-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_y = dataset_test.map(lambda f, l: l)\n",
    "y_test_onehot = []\n",
    "for d in ds_y.batch(1):\n",
    "    for d1 in d:\n",
    "        y_test_onehot.append(d1.numpy())\n",
    "y_test = np.array([np.argmax(a, axis=1) for a in y_test_onehot]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "warming-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'working/data/dev_y_tests.npz'\n",
    "np.savez(outfile, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "median-diana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'NOT ENOUGH INFO']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d['label_text'] for d in test_data[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "duplicate-brass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.88      0.60      3325\n",
      "           1       0.78      0.30      0.43      3331\n",
      "           2       0.66      0.48      0.56      3328\n",
      "\n",
      "    accuracy                           0.55      9984\n",
      "   macro avg       0.63      0.55      0.53      9984\n",
      "weighted avg       0.63      0.55      0.53      9984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "#['NOT ENOUGH INFO', 'REFUTES', 'SUPPORTS'] == [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-interference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "silver-tuning",
   "metadata": {},
   "source": [
    "For <b>fever score</b>, we will need to compare the labels and the evidences.\n",
    "\n",
    "First we need to extract the true labels and evidences from the original training dataset.\n",
    "\n",
    "In the original dataset, we will need to sample data for the NEI class just like we did for our original training\n",
    "\n",
    "Note: the dataset root path should match what's in /loca/fever-common/ in the 'fever-common' container\n",
    "\n",
    "We should have already generated the 'ns' file (i.e. **paper_dev.ns.pages.p5.jsonl**) during our training. We do not need to re-generate this file, this step below is just for the sake of completeness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "secure-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.dataset.DatasetGenerator import DatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "alive-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_generator = DatasetGenerator(dataset_root='data/data/',out_dir='working/data/out/', database_path='data/data/fever/fever.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "guided-wagner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/9999 [00:00<08:35, 19.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to working/data/out//paper_dev.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [02:20<00:00, 71.38it/s] \n"
     ]
    }
   ],
   "source": [
    "ds_generator.generate_nei_evidences('paper_dev', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-guyana",
   "metadata": {},
   "source": [
    "We need the gold evidences and the predicted evidences along with the label predictions to compute the fever score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "interstate-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "from mda.src.utils.readers import JSONLineReader\n",
    "from mda.src.utils.eval import *\n",
    "working_dir = 'working/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-yesterday",
   "metadata": {},
   "source": [
    "Read the original / gold evidences from the dev file. The predicted evidences are already available in the **raw_test** dataset we read from the **paper_dev_pipeline.ps.pages.p5.jsonl** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "maritime-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 180632.30it/s]\n"
     ]
    }
   ],
   "source": [
    "jlr = JSONLineReader()\n",
    "split = 'paper_dev'\n",
    "k = 5\n",
    "test_data_file = working_dir + \"training/{0}.ns.pages.p{1}.jsonl\".format(split, k)\n",
    "data_orig = jlr.read(test_data_file)\n",
    "orig_evidences = [d['evidence'] for d in data_orig[:len(y_test)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-transition",
   "metadata": {},
   "source": [
    "Generate the final predictions, for the label, the predicted pages and the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "empirical-niagara",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9984it [00:00, 78166.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to training/paper_dev_predicted_pipeline.ps.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "split = 'paper_dev_predicted'\n",
    "k = 5\n",
    "with open(working_dir + \"training/{0}_pipeline.ps.pages.p{1}.jsonl\".format(split,k),\"w+\") as f_out:\n",
    "    print(\"Saving to training/{0}_pipeline.ps.pages.p{1}.jsonl\".format(split,k))\n",
    "    for rec, orig, true_label, predicted_label in tqdm(zip(raw_test[:len(y_test)], orig_evidences, y_test, y_pred)):\n",
    "        evs = []\n",
    "        for evidence_group in rec['evidence']:\n",
    "            for evidence in evidence_group:\n",
    "                if evidence[0] > -1:\n",
    "                    ev = [evidence[0], evidence[1], evidence[2], evidence[4][1]]\n",
    "                    evs.append(ev)\n",
    "\n",
    "        out = {'true_label': str(true_label), 'predicted_label': str(predicted_label), 'orig': orig, 'pred': evs}\n",
    "\n",
    "        f_out.write(json.dumps(out) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-cookbook",
   "metadata": {},
   "source": [
    "Load the predictions from the file to use in fever scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "considerable-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9984/9984 [00:00<00:00, 146605.42it/s]\n"
     ]
    }
   ],
   "source": [
    "def read_jsonl_data(filename):\n",
    "    jlr = JSONLineReader()\n",
    "    predicted_results = jlr.read(filename)\n",
    "    return predicted_results\n",
    "split = 'paper_dev_predicted'\n",
    "k = 5\n",
    "filename = working_dir + \"training/{0}_pipeline.ps.pages.p{1}.jsonl\".format(split, k)\n",
    "predicted_results = read_jsonl_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "essential-connecticut",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9984/9984 [00:00<00:00, 125284.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noevscore=55.01, score=36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55.01, 36.0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_fever_score(predicted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-watershed",
   "metadata": {},
   "source": [
    "#### From original FEVER paper\n",
    "The classification\n",
    "accuracy is <b>32.57%</b>. Ignoring the requirement for\n",
    "correct evidence (NoScoreEv) the accuracy is\n",
    "<b>52.09%</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-drink",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-mustang",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
