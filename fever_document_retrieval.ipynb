{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEVER dataset processing\n",
    "\n",
    "<h5>Process the claims in the fever dataset</h5>\n",
    "\n",
    "In this notebook, we will prepare the training dataset and buid a baseline model that would set us up for the NLI tasks\n",
    "\n",
    "We use the following repos for reference code:\n",
    "\n",
    "- [fever-baselines](https://github.com/klimzaporojets/fever-baselines.git)\n",
    "- [fever-allennlp-reader](https://github.com/j6mes/fever-allennlp-reader)\n",
    "- [fever-allennlp](https://github.com/j6mes/fever-allennlp)\n",
    "\n",
    "Note, AllenNLP here is used only for the NLI training, using models such as Decomposable Attention, Elmo + ESIM, ESIM etc. In this notebook, we will first focus on extracying the data from the pre-processed Wiki corpus provided by [fever.ai](https://fever.ai/dataset/fever.html).\n",
    "\n",
    "The data is available in a [docker image](https://hub.docker.com/r/feverai/common), 21GB in size. The container is created and the volume /local/ from it is mounted and made available to our [container](https://github.com/dmayukh/fakenews/Dockerfile) \n",
    "\n",
    "\n",
    "We will install a few dependencies such as:\n",
    "- numpy>=1.15\n",
    "- regex\n",
    "- allennlp==2.5.0\n",
    "- fever-scorer==2.0.39\n",
    "- fever-drqa==1.0.13\n",
    "\n",
    "The following packages are installed by the above dependencies\n",
    "- torchvision-0.9.1\n",
    "- google_cloud_storage-1.38.0\n",
    "- overrides==3.1.0\n",
    "- transformers-4.6.1\n",
    "- spacy-3.0.6\n",
    "- sentencepiece-0.1.96\n",
    "- torch-1.8.1\n",
    "- wandb-0.10.33\n",
    "- lmdb-1.2.1\n",
    "- jsonnet-0.17.0\n",
    "\n",
    "We do not really need allennlp or fever-scorer as of yet, we would only need DrQA. I would prefer to use the DrQA from the official github, but for now we will go with what was prepackaged by the [j6mes](https://pypi.org/project/fever-drqa/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the database from the DB file that contains the preprocessed Wiki pages. This DB was made available to us by FEVER.\n",
    "\n",
    "FeverDocDB is a simple wrapper that opens a SQLlite3 connection to the database and provides methods to execute simple select queries to fetch ids for documents and to fetch lines given a document.\n",
    "\n",
    "We will not require this in the first pass of our work here, since we are only interested in findings the documents closest to a claim text.\n",
    "\n",
    "The function to fetch lines per document is what uses the connection to the database. In order to find the closest documents for a given claim, use use the ranker that uses a pre-created TFIDF index which can locate the document ids given a claim text.\n",
    "\n",
    "The pre-created index is available in '/local/fever-common/data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz'\n",
    "\n",
    "\n",
    "Sample data from training file:\n",
    "\n",
    "> {\"id\": 75397, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\", \"evidence\": [[[92206, 104971, \"Nikolaj_Coster-Waldau\", 7], [92206, 104971, \"Fox_Broadcasting_Company\", 0]]]}\n",
    "\n",
    "A closer look at the evidence:\n",
    "\n",
    "> [[92206, 104971, \"Nikolaj_Coster-Waldau\", 7]\n",
    "\n",
    "92206 and 104971 are the annotation ids, while the \"Nikolaj_Coster-Waldau\" is the evidence page and the line number is 7.\n",
    "\n",
    "\n",
    "#### Formatting the input text\n",
    "\n",
    "The training of the model is done on the evidence provided by the human annotators, therefore we use the 'evidence' to run our training.\n",
    "\n",
    "After formatting, the training examples are written as below that is then used to train the MLP\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
    "  'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
    "  'label': 0,\n",
    "  'label_text': 'SUPPORTS'}\n",
    "\n",
    "The baseline model is a simple MLP that uses the count vectorizer to vectorize the claim text and the evidence page texts. It also uses an additional feature which is the cosine similarity between the vectorized claim text and the vectorized combined texts from all the evidences.\n",
    "\n",
    "The vectorizers are saved to the filesystem that can be used later for transorming the incoming sentences.\n",
    "\n",
    "TODO: not sure why the specific evidence lines are not used for the training.\n",
    "\n",
    "The trained model is used to run eval on the dev dataset of the same format.\n",
    "\n",
    "TODO: inference is not explicitly done in this code. We will have to do inference which is most likely going to be done as follows:\n",
    "\n",
    "Given a claim, use the ranker to fetch the 5 closest pages from the DB. Create the features by using the saved vectorizers.\n",
    "\n",
    "Predict the label of the example, i.e. which class it belongs to. 'SUPPORTS' or 'REFUTES'.\n",
    "\n",
    "\n",
    "<h5>Retrieval of the evidence</h5>\n",
    "\n",
    "We also attempt to extract the evidence from the corresponding pages\n",
    "\n",
    "First, using the tfidf doc ranker, we extract the top 5 pages that are similar to the claim text\n",
    "\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .', 'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)], 'label': 0, 'label_text': 'SUPPORTS', 'predicted_pages': [('Coster', 498.82682448841246), ('Nikolaj', 348.42021460316823), ('The_Other_Woman_-LRB-2014_film-RRB-', 316.8405030379064), ('Nikolaj_Coster-Waldau', 316.8405030379064), ('Nukaaka_Coster-Waldau', 292.47605893902585)]}\n",
    "\n",
    "For each of the pages, we extract the lines from the page text and use 'online tfidf ranker' to fetch the closest matching lines from the text.\n",
    "\n",
    "The training examples are then formatted as below which is then used to run EVAL on the MLP model\n",
    "\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
    " 'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
    " 'label': 0,\n",
    " 'label_text': 'SUPPORTS',\n",
    " 'predicted_pages': [('Coster', 498.82682448841246),\n",
    "  ('Nikolaj', 348.42021460316823),\n",
    "  ('The_Other_Woman_-LRB-2014_film-RRB-', 316.8405030379064),\n",
    "  ('Nikolaj_Coster-Waldau', 316.8405030379064),\n",
    "  ('Nukaaka_Coster-Waldau', 292.47605893902585)],\n",
    " 'predicted_sentences': [('Nikolaj', 7),\n",
    "  ('The_Other_Woman_-LRB-2014_film-RRB-', 1),\n",
    "  ('Nukaaka_Coster-Waldau', 1),\n",
    "  ('Coster', 63),\n",
    "  ('Nikolaj_Coster-Waldau', 0)]}\n",
    "  \n",
    "The scoring of the evidence predictor is not straight forward. Use the fever-scorer to score the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 13114, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"J. R. R. Tolkien created Gimli.\", \"evidence\": [[[28359, 34669, \"Gimli_-LRB-Middle-earth-RRB-\", 0]], [[28359, 34670, \"Gimli_-LRB-Middle-earth-RRB-\", 1]]]}\n",
      "{\"id\": 152180, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Susan Sarandon is an award winner.\", \"evidence\": [[[176133, 189101, \"Susan_Sarandon\", 1]], [[176133, 189102, \"Susan_Sarandon\", 2]], [[176133, 189103, \"Susan_Sarandon\", 8]]]}\n"
     ]
    }
   ],
   "source": [
    "!tail -2 /local/fever-common/data/fever-data/train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
      "{\"id\": 163803, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Ukrainian Soviet Socialist Republic was a founding participant of the UN.\", \"evidence\": [[[296950, 288668, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[298602, 290067, \"Ukrainian_Soviet_Socialist_Republic\", 7], [298602, 290067, \"United_Nations\", 0]], [[300696, 291816, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344347, 327887, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344994, 328433, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344997, 328435, \"Ukrainian_Soviet_Socialist_Republic\", 7]]]}\n"
     ]
    }
   ],
   "source": [
    "!head -2 /local/fever-common/data/fever-data/paper_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training dataset\n",
    "\n",
    "The training examples have three (3) classes:\n",
    "- SUPPORTS\n",
    "- REFUTES\n",
    "- NOT ENOUGH INFO\n",
    "\n",
    "For the 'NOT ENOUGH INFO' class, the evidences are set to None. This would cause problems with training since we would still like to generate features for the samples which have been put in this class.\n",
    "\n",
    "Next, we will loop over the records in the training dataset to create the training records. Specifically, we would be generating evidences for the samples in the 'NOT ENOUGH INFO' class so that the None values now have some page information.\n",
    "\n",
    "Our strategy for dealing with missing evidences for the 'NOT ENOUGH INFO' class is to find the pages that are closest to the claims based on the tfidf similarity. The tfidf similarity of the documents in the fever DB is already precomputed and make available to us via the index file:\n",
    "\n",
    "> '/local/fever-common/data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz'\n",
    "\n",
    "Let's load the index file and create the ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drqa import retriever\n",
    "tdidf_npz_file = '/local/fever-common/data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz'\n",
    "ranker = retriever.get_class('tfidf')(tfidf_path=tdidf_npz_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the directory where we will save our prepared datasets\n",
    "\n",
    "The raw training data is available at \n",
    "\n",
    "> /local/fever-common/data/fever-data/train.jsonl\n",
    "\n",
    "The raw dev data from the FEVER paper is available at \n",
    "\n",
    "> /local/fever-common/data/fever-data/paper_dev.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p working/data/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_dataset(split, k=5):\n",
    "    fever_root = '/local/fever-common/'\n",
    "    working_dir = 'working/data/'\n",
    "    print(\"Saving prepared dataset to {}\".format(\"training/{0}.ns.pages.p{1}.jsonl\".format(split,k)))\n",
    "    with open(fever_root + \"data/fever-data/{0}.jsonl\".format(split),\"r\") as f_in:\n",
    "        with open(working_dir + \"training/{0}.ns.pages.p{1}.jsonl\".format(split,k),\"w+\") as f_out:\n",
    "            for line in tqdm(f_in.readlines()):\n",
    "                line = json.loads(line)\n",
    "                if line[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "                        doc_names, doc_scores = ranker.closest_docs(line['claim'], k)\n",
    "                        pp = list(doc_names)\n",
    "\n",
    "                        for idx,evidence_group in enumerate(line['evidence']):\n",
    "                            for evidence in evidence_group:\n",
    "                                if idx<len(pp):\n",
    "                                    evidence[2] = pp[idx]\n",
    "                                    evidence[3] = -1\n",
    "                \n",
    "                f_out.write(json.dumps(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf training/train.ns.pages.p5.jsonl\n",
    "prepare_dataset('train', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prepared dataset to training/paper_dev.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [02:58<00:00, 56.11it/s]\n"
     ]
    }
   ],
   "source": [
    "!rm -rf training/paper_dev.ns.pages.p5.jsonl\n",
    "prepare_dataset('paper_dev', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 working/data/training/paper_dev.ns.pages.p5.jsonl\n",
      "  145449 working/data/training/train.ns.pages.p5.jsonl\n",
      "    2193 working/data/training/train.pages.p5.jsonl\n",
      "  157641 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l  working/data/training/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the feature sets\n",
    "\n",
    "Using the training data and dev data we generated, we will create the vectorizers and save them to local files\n",
    "\n",
    "The training and dev data is available at \n",
    "\n",
    "> working/data/training/train.ns.pages.p5.jsonl \n",
    "\n",
    "> working/data/training/paper_dev.ns.pages.p5.jsonl\n",
    "\n",
    "The key information we need from the training samples are the claim text and the texts from the evidence pages\n",
    "\n",
    "For each training example, generate:\n",
    "- a tokenized claim, \n",
    "- the label id, \n",
    "- the label text, \n",
    "- list of wiki pages that were provided as evidence.\n",
    "\n",
    "This is done using a custom formatter `training_line_formatter` we would write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "class LabelSchema:\n",
    "    def __init__(self,labels):\n",
    "        self.labels = {self.preprocess(val):idx for idx,val in enumerate(labels)}\n",
    "        self.idx = {idx:self.preprocess(val) for idx,val in enumerate(labels)}\n",
    "\n",
    "    def get_id(self,label):\n",
    "        if self.preprocess(label) in self.labels:\n",
    "            return self.labels[self.preprocess(label)]\n",
    "        return None\n",
    "\n",
    "    def preprocess(self,item):\n",
    "        return item.lower()\n",
    "\n",
    "class FEVERLabelSchema(LabelSchema):\n",
    "    def __init__(self):\n",
    "        super().__init__([\"supports\", \"refutes\", \"not enough info\"])\n",
    "\n",
    "def nltk_tokenizer(text):\n",
    "    return \" \".join(word_tokenize(text))\n",
    "\n",
    "class training_line_formatter():\n",
    "    def __init__(self):\n",
    "        self.tokenize = nltk_tokenizer\n",
    "        \n",
    "    def format(self, lines):\n",
    "        formatted = []\n",
    "        for line in tqdm(lines):\n",
    "            fl = self.format_line(line)\n",
    "            if fl is not None:\n",
    "                if isinstance(fl,list):\n",
    "                    formatted.extend(fl)\n",
    "                else:\n",
    "                    formatted.append(fl)\n",
    "        return formatted\n",
    "\n",
    "    def format_line(self, line):\n",
    "        label_schema = FEVERLabelSchema()\n",
    "        # get the label, i.e. SUPPORTS etc.\n",
    "        annotation = line[\"label\"]\n",
    "        if annotation is None:\n",
    "            annotation = line[\"verifiable\"]\n",
    "        pages = []\n",
    "\n",
    "        # did we get the closest sentences to the claim text? is this the sentence or the line number from the doc text?\n",
    "        if 'predicted_sentences' in line:\n",
    "            pages.extend([(ev[0], ev[1]) for ev in line[\"predicted_sentences\"]])\n",
    "        elif 'predicted_pages' in line:\n",
    "            pages.extend([(ev[0], -1) for ev in line[\"predicted_pages\"]])\n",
    "        else:\n",
    "            # these are the human annotated evidence available in the original training file\n",
    "            for evidence_group in line[\"evidence\"]:\n",
    "                pages.extend([(ev[2], ev[3]) for ev in evidence_group])\n",
    "\n",
    "    #     if self.filtering is not None:\n",
    "    #         for page, _ in pages:\n",
    "    #             if self.filtering({\"id\": page}) is None:\n",
    "    #                 return None\n",
    "\n",
    "        return {\"claim\": self.tokenize(line[\"claim\"]), \"evidence\": pages, \"label\": label_schema.get_id(annotation),\n",
    "                \"label_text\": annotation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader:\n",
    "    def __init__(self,encoding=\"utf-8\"):\n",
    "        self.enc = encoding\n",
    "\n",
    "    def read(self,file):\n",
    "        with open(file,\"r\",encoding = self.enc) as f:\n",
    "            return self.process(f)\n",
    "\n",
    "    def process(self,f):\n",
    "        pass\n",
    "\n",
    "class JSONLineReader(Reader):\n",
    "    def process(self,fp):\n",
    "        data = []\n",
    "        for line in tqdm(fp.readlines()):\n",
    "            data.append(json.loads(line.strip()))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use k=5 which is the number of closest documents we used to prepare our datasets. This is simply used to load the correct input file for the dataset formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:03<00:00, 43746.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "jlr = JSONLineReader()\n",
    "split = 'train'\n",
    "working_dir = 'working/data/'\n",
    "k = 5\n",
    "training_data_file = working_dir + \"training/{0}.ns.pages.p{1}.jsonl\".format(split, k)\n",
    "data = jlr.read(training_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to format the training data so that we can extract the claim and the body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:24<00:00, 5917.68it/s]\n"
     ]
    }
   ],
   "source": [
    "formatter = training_line_formatter()\n",
    "formatted_train_data = formatter.format(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
       "  'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_train_data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each formatted training example now looks like:\n",
    "    \n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
    "  'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
    "  'label': 0,\n",
    "  'label_text': 'SUPPORTS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
       "  'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'},\n",
       " {'claim': 'Roman Atwood is a content creator .',\n",
       "  'evidence': [('Roman_Atwood', 1), ('Roman_Atwood', 3)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'},\n",
       " {'claim': 'History of art includes architecture , dance , sculpture , music , painting , poetry literature , theatre , narrative , film , photography and graphic arts .',\n",
       "  'evidence': [('History_of_art', 2)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'},\n",
       " {'claim': 'Adrienne Bailon is an accountant .',\n",
       "  'evidence': [('Adrienne_Bailon', 0)],\n",
       "  'label': 1,\n",
       "  'label_text': 'REFUTES'},\n",
       " {'claim': 'System of a Down briefly disbanded in limbo .',\n",
       "  'evidence': [('In_Limbo', -1)],\n",
       "  'label': 2,\n",
       "  'label_text': 'NOT ENOUGH INFO'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_formatted = []\n",
    "data_formatted.extend(filter(lambda record: record is not None, formatted_train_data))\n",
    "data_formatted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145449"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the dev dataset as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 14329.89it/s]\n",
      "100%|██████████| 9999/9999 [00:01<00:00, 5740.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'claim': 'Colin Kaepernick became a starting quarterback during the 49ers 63rd season in the National Football League .',\n",
       "  'evidence': [('Colin_Kaepernick', -1)],\n",
       "  'label': 2,\n",
       "  'label_text': 'NOT ENOUGH INFO'},\n",
       " {'claim': 'Tilda Swinton is a vegan .',\n",
       "  'evidence': [('Swinton_-LRB-surname-RRB-', -1)],\n",
       "  'label': 2,\n",
       "  'label_text': 'NOT ENOUGH INFO'},\n",
       " {'claim': 'Fox 2000 Pictures released the film Soul Food .',\n",
       "  'evidence': [('Soul_Food_-LRB-film-RRB-', 0),\n",
       "   ('Soul_Food_-LRB-film-RRB-', 0),\n",
       "   ('Soul_Food_-LRB-film-RRB-', 0),\n",
       "   ('Soul_Food_-LRB-film-RRB-', 0),\n",
       "   ('Soul_Food_-LRB-film-RRB-', 0)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'},\n",
       " {'claim': 'Anne Rice was born in New Jersey .',\n",
       "  'evidence': [('List_of_Ace_titles_in_numeric_series', -1),\n",
       "   ('List_of_Ace_titles_in_numeric_series', -1)],\n",
       "  'label': 2,\n",
       "  'label_text': 'NOT ENOUGH INFO'},\n",
       " {'claim': 'Telemundo is a English-language television network .',\n",
       "  'evidence': [('Telemundo', 0),\n",
       "   ('Telemundo', 1),\n",
       "   ('Telemundo', 4),\n",
       "   ('Hispanic_and_Latino_Americans', 0),\n",
       "   ('Telemundo', 5)],\n",
       "  'label': 1,\n",
       "  'label_text': 'REFUTES'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "jlr = JSONLineReader()\n",
    "split = 'paper_dev'\n",
    "working_dir = 'working/data/'\n",
    "k = 5\n",
    "dev_data_file = working_dir + \"training/{0}.ns.pages.p{1}.jsonl\".format(split, k)\n",
    "dev_data = jlr.read(dev_data_file)\n",
    "\n",
    "formatter = training_line_formatter()\n",
    "formatted_dev_data = formatter.format(dev_data)\n",
    "\n",
    "dev_data_formatted = []\n",
    "dev_data_formatted.extend(filter(lambda record: record is not None, formatted_dev_data))\n",
    "dev_data_formatted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_data_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the feature set\n",
    "We will use the formatted training and dev data now to generate the features for our training\n",
    "\n",
    "We only have the body ids, we will need to extract the body text given the body ids. We will use the database provided for that.\n",
    "\n",
    "First create a class to handle interactions with the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drqa.retriever import DocDB, utils\n",
    "class FeverDocDB(DocDB):\n",
    "\n",
    "    def __init__(self,path=None):\n",
    "        super().__init__(path)\n",
    "\n",
    "    def get_doc_lines(self, doc_id):\n",
    "        \"\"\"Fetch the raw text of the doc for 'doc_id'.\"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(\n",
    "            \"SELECT lines FROM documents WHERE id = ?\",\n",
    "            (utils.normalize(doc_id),)\n",
    "        )\n",
    "        result = cursor.fetchone()\n",
    "        cursor.close()\n",
    "        return result if result is None else result[0]\n",
    "\n",
    "    def get_non_empty_doc_ids(self):\n",
    "        \"\"\"Fetch all ids of docs stored in the db.\"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(\"SELECT id FROM documents WHERE length(trim(text)) > 0\")\n",
    "        results = [r[0] for r in cursor.fetchall()]\n",
    "        cursor.close()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = '/local/fever-common/data/fever/fever.db'\n",
    "database = FeverDocDB(database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our formatted data looks like this\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
    " 'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
    " 'label': 0,\n",
    " 'label_text': 'SUPPORTS'}\n",
    " \n",
    "We will use the evidence fields to extract the supporting texts. First define some routines to extract the relevant information from the formatted lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class SimpleRandom():\n",
    "    instance = None\n",
    "\n",
    "    def __init__(self,seed):\n",
    "        self.seed = seed\n",
    "        self.random = random.Random(seed)\n",
    "\n",
    "    def next_rand(self,a,b):\n",
    "        return self.random.randint(a,b)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_instance():\n",
    "        if SimpleRandom.instance is None:\n",
    "            SimpleRandom.instance = SimpleRandom(SimpleRandom.get_seed())\n",
    "        return SimpleRandom.instance\n",
    "\n",
    "    @staticmethod\n",
    "    def get_seed():\n",
    "        return int(os.getenv(\"RANDOM_SEED\", 12459))\n",
    "\n",
    "    @staticmethod\n",
    "    def set_seeds():\n",
    "\n",
    "        torch.manual_seed(SimpleRandom.get_seed())\n",
    "        if gpu():\n",
    "            torch.cuda.manual_seed_all(SimpleRandom.get_seed())\n",
    "        np.random.seed(SimpleRandom.get_seed())\n",
    "        random.seed(SimpleRandom.get_seed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ename = \"evidence\"\n",
    "def claims(data):\n",
    "    return [datum[\"claim\"] for datum in data]\n",
    "def body_ids(data):\n",
    "    return [[d[0] for d in datum[ename] ] for datum in data]\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "def bodies(data):\n",
    "    #data = [d for d in flatten(body_ids(data)) if d]\n",
    "    return [database.get_doc_text(id) for id in set(flatten(body_ids(data)))]\n",
    "\n",
    "def texts(data):\n",
    "    return [\" \".join(set(instance)) for instance in body_lines(data)]\n",
    "\n",
    "def body_lines(data):\n",
    "    return [[get_doc_line(d[0],d[1]) for d in datum[ename] ] for datum in data]\n",
    "\n",
    "def get_doc_line(doc,line):\n",
    "    lines = database.get_doc_lines(doc)\n",
    "\n",
    "#     if os.getenv(\"PERMISSIVE_EVIDENCE\",\"n\").lower() in [\"y\",\"yes\",\"true\",\"t\",\"1\"]:\n",
    "#         if lines is None:\n",
    "#             return \"\"\n",
    "\n",
    "    if line > -1:\n",
    "        return lines.split(\"\\n\")[line].split(\"\\t\")[1]\n",
    "    else:\n",
    "        non_empty_lines = [line.split(\"\\t\")[1] for line in lines.split(\"\\n\") if len(line.split(\"\\t\"))>1 and len(line.split(\"\\t\")[1].strip())]\n",
    "        return non_empty_lines[SimpleRandom.get_instance().next_rand(0,len(non_empty_lines)-1)]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a term frequency vectorizer and a TDIDF vectorizer and save them to a file.\n",
    "\n",
    "The vocabulary will be limited to 5000. For each of the claim and the body text, we would produce the vectors which would be of dimension 5000.\n",
    "\n",
    "We will also add the cosine similarity between the claim vector and the body text vector and use it as an additional feature.\n",
    "\n",
    "The dimension of our feature would be then 5000 + 5000 + 1 = 10001\n",
    "\n",
    "Clean up any pre-generated feature vectors we are going to re-run the vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf working/models/ns_nn_sent/dev.pkl\n",
    "# !rm -rf working/models/ns_nn_sent/train.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the vectorizers\n",
    "We will be using the contents of both the training and dev set to build the vectorizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "CLAIMS = claims(data_formatted)\n",
    "BODIES = bodies(data_formatted)\n",
    "dev_claims = claims(dev_data_formatted)\n",
    "dev_bodies = bodies(dev_data_formatted)\n",
    "lim_unigram = 5000\n",
    "stop_words = [\n",
    "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
    "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
    "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
    "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
    "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
    "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
    "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
    "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
    "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
    "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
    "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
    "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
    "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
    "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
    "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
    "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
    "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "        ]\n",
    "bow_vectorizer = CountVectorizer(max_features=lim_unigram,\n",
    "                                         stop_words=stop_words)\n",
    "bow = bow_vectorizer.fit_transform(CLAIMS + BODIES)\n",
    "tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram,\n",
    "                                           stop_words=stop_words).fit(CLAIMS + BODIES + dev_claims + dev_bodies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizers will be saved in a folder in the directory 'ns_nn_sent' so that it can be looked up later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p working/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the claims and the body texts using the vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import hstack\n",
    "def process(data):\n",
    "    claim_bow = bow_vectorizer.transform(claims(data))\n",
    "    claim_tfs = tfreq_vectorizer.transform(claim_bow)\n",
    "    claim_tfidf = tfidf_vectorizer.transform(claims(data))\n",
    "\n",
    "    #get the text from the bodies of all the n-closest docs for the claim\n",
    "    body_texts = texts(data)\n",
    "    body_bow = bow_vectorizer.transform(body_texts)\n",
    "    body_tfs = tfreq_vectorizer.transform(body_bow)\n",
    "    body_tfidf = tfidf_vectorizer.transform(body_texts)\n",
    "\n",
    "    cosines = np.array([cosine_similarity(c, b)[0] for c,b in zip(claim_tfidf,body_tfidf)])\n",
    "\n",
    "    return hstack([body_tfs,claim_tfs,cosines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "model_name = 'ns_nn_sent'\n",
    "base_path = 'working/models/'\n",
    "\n",
    "def load_features(name, data):\n",
    "    features = list()\n",
    "    ffpath = os.path.join(base_path, model_name)\n",
    "    if not os.path.exists(ffpath):\n",
    "        os.mkdir(ffpath)\n",
    "    if (not os.path.exists(os.path.join(ffpath, name + \".pkl\"))):\n",
    "        print(\"Saved features do not exist, creating data...\")\n",
    "        features = process(data)\n",
    "        with open(os.path.join(ffpath, name + \".pkl\"), \"wb+\") as f:\n",
    "            pickle.dump(features, f)\n",
    "    else:\n",
    "        print(\"Loading saved feature from {}\".format(os.path.join(ffpath, name + \".pkl\")))\n",
    "        with open(os.path.join(ffpath, name + \".pkl\"), \"rb\") as f:\n",
    "            features = pickle.load(f)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the labels for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = \"label\"\n",
    "def labels(data):\n",
    "    return [datum[label_name] for datum in data]\n",
    "def out(features,ds):\n",
    "    if ds is not None:\n",
    "        return np.hstack(features) if len(features) > 1 else features[0], labels(ds)\n",
    "    return [[]],[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs to be performed once per dataset. Therefore, we would save the transformed vectors in a file to reuse for each modelling excercise.\n",
    "\n",
    "Check if the saved vectors exist, if not, create them by using the vectorizers and applying a transform on the \n",
    "- claim\n",
    "- lines from the body of the evidence pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved feature from working/models/ns_nn_sent/train.pkl\n"
     ]
    }
   ],
   "source": [
    "train_fs = []\n",
    "features = load_features(\"train\", data_formatted)\n",
    "train_fs.append(features)\n",
    "train_feats = out(train_fs, data_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape = 10001\n"
     ]
    }
   ],
   "source": [
    "input_shape = train_feats[0].shape[1]\n",
    "print(\"input_shape =\", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved feature from working/models/ns_nn_sent/dev.pkl\n"
     ]
    }
   ],
   "source": [
    "dev_fs = []\n",
    "features = load_features(\"dev\", dev_data_formatted)\n",
    "dev_fs.append(features)\n",
    "dev_feats = out(dev_fs, dev_data_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (using PyTorch)\n",
    "It's now time to build the model. We will build a Simple Multi layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,keep_p=.6):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "        self.do = nn.Dropout(1-keep_p)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.do(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.do(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleMLP(\n",
       "  (fc1): Linear(in_features=10001, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (do): Dropout(p=0.4, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleMLP(input_shape,100,3)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up any saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rm -rf working/models/ns_nn_sent/ns_nn_sent.best.save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the logger, the one that will be used to monitor the model training progress\n",
    "\n",
    "The best model will be saved at \n",
    "> working/models/ns_nn_sent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "class LogHelper():\n",
    "    handler = None\n",
    "    @staticmethod\n",
    "    def setup():\n",
    "        FORMAT = '[%(levelname)s] %(asctime)s - %(name)s - %(message)s'\n",
    "        LogHelper.handler = logging.StreamHandler()\n",
    "        LogHelper.handler.setLevel(logging.DEBUG)\n",
    "        LogHelper.handler.setFormatter(logging.Formatter(FORMAT))\n",
    "\n",
    "        LogHelper.get_logger(LogHelper.__name__).info(\"Log Helper set up\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_logger(name,level=logging.DEBUG):\n",
    "        ##note: once a logger is created, repeated calls using the same name will give you the same logger object\n",
    "        l = logging.getLogger(name)\n",
    "        sh = logging.StreamHandler()\n",
    "        l.setLevel(level)\n",
    "        l.addHandler(sh)\n",
    "        return l\n",
    "    \n",
    "class EarlyStopping():\n",
    "    def __init__(self,name,patience=8):\n",
    "        self.patience = patience\n",
    "        self.best_model = None\n",
    "        self.best_score = None\n",
    "\n",
    "        self.best_epoch = 0\n",
    "        self.epoch = 0\n",
    "        #print(\"name is \", EarlyStopping.__name__)\n",
    "        self.name = name\n",
    "        #self.logger = LogHelper.get_logger(EarlyStopping.__name__)\n",
    "        self.logger = LogHelper.get_logger(name)\n",
    "\n",
    "    def __call__(self, model, acc):\n",
    "        self.epoch += 1\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = acc\n",
    "\n",
    "        if acc >= self.best_score:\n",
    "            torch.save(model.state_dict(),\"working/models/ns_nn_sent/{0}.best.save\".format(self.name))\n",
    "            self.best_score = acc\n",
    "            self.best_epoch = self.epoch\n",
    "            self.logger.info(\"Saving best weights from round {0}\".format(self.epoch))\n",
    "            return False\n",
    "\n",
    "        elif self.epoch > self.best_epoch+self.patience:\n",
    "            self.logger.info(\"Early stopping: Terminate\")\n",
    "            return True\n",
    "\n",
    "        self.logger.info(\"Early stopping: Worse Round\")\n",
    "        return False\n",
    "\n",
    "    def set_best_state(self,model):\n",
    "        self.logger.info(\"Loading weights from round {0}\".format(self.best_epoch))\n",
    "        model.load_state_dict(torch.load(\"working/models/ns_nn_sent/{0}.best.save\".format(self.name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset reader\n",
    "\n",
    "We will need to handle the batching of inputs to our model\n",
    "\n",
    "We will define a batcher that deals with the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "def is_gpu():\n",
    "    return os.getenv(\"GPU\",\"no\").lower() in [\"1\",1,\"yes\",\"true\",\"t\"]\n",
    "\n",
    "def gpu():\n",
    "    if is_gpu():\n",
    "        torch.cuda.set_device(int(os.getenv(\"CUDA_DEVICE\", 0)))\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class Batcher():\n",
    "    def __init__(self,data,size):\n",
    "        self.data = data\n",
    "        self.size = size\n",
    "        self.pointer = 0\n",
    "\n",
    "        if isinstance(self.data,coo_matrix):\n",
    "            self.data = self.data.tocsr()\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.pointer == splen(self.data):\n",
    "            self.pointer = 0\n",
    "            raise StopIteration\n",
    "        next = min(splen(self.data),self.pointer+self.size)\n",
    "        to_return = self.data[self.pointer : next]\n",
    "        start,end = self.pointer,next\n",
    "        self.pointer = next\n",
    "        return to_return, splen(to_return), start, end\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "def splen(data):\n",
    "    try:\n",
    "        return data.shape[0]\n",
    "    except:\n",
    "        return len(data)\n",
    "\n",
    "def prepare_with_labels(data,labels):\n",
    "    data = data.todense()\n",
    "    v = torch.FloatTensor(np.array(data))\n",
    "    if gpu():\n",
    "        return Variable(v.cuda()), Variable(torch.LongTensor(labels).cuda())\n",
    "    return Variable(v), Variable(torch.LongTensor(labels))\n",
    "\n",
    "\n",
    "def prepare(data):\n",
    "    data = data.todense()\n",
    "    v = torch.FloatTensor(np.array(data))\n",
    "    if gpu():\n",
    "        return Variable(v.cuda())\n",
    "    return Variable(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate(model,data,labels,batch_size):\n",
    "    predicted = predict(model,data,batch_size)\n",
    "    return accuracy_score(labels,predicted.data.numpy().reshape(-1))\n",
    "\n",
    "def predict(model, data, batch_size):\n",
    "    batcher = Batcher(data, batch_size)\n",
    "\n",
    "    predicted = []\n",
    "    for batch, size, start, end in batcher:\n",
    "        d = prepare(batch)\n",
    "        model.eval()\n",
    "        logits = model(d).cpu()\n",
    "\n",
    "        predicted.extend(torch.max(logits, 1)[1])\n",
    "    return torch.stack(predicted)\n",
    "\n",
    "def train(model, fs, batch_size, lr, epochs,dev=None, clip=None, early_stopping=None,name=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    data, labels = fs\n",
    "    if dev is not None:\n",
    "        dev_data,dev_labels = dev\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_loss = 0\n",
    "        epoch_data = 0\n",
    "\n",
    "        shuffle(data,labels)\n",
    "\n",
    "        batcher = Batcher(data, batch_size)\n",
    "\n",
    "        for batch, size, start, end in batcher:\n",
    "            d,gold = prepare_with_labels(batch,labels[start:end])\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(d)\n",
    "\n",
    "            loss = F.cross_entropy(logits, gold)\n",
    "            loss.backward()\n",
    "\n",
    "            epoch_loss += loss.cpu()\n",
    "            epoch_data += size\n",
    "\n",
    "            if clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Average epoch loss: {0}\".format((epoch_loss/epoch_data).data.numpy()))\n",
    "\n",
    "        #print(\"Epoch Train Accuracy {0}\".format(evaluate(model, data, labels, batch_size)))\n",
    "        if dev is not None:\n",
    "            acc = evaluate(model,dev_data,dev_labels,batch_size)\n",
    "            print(\"Epoch Dev Accuracy {0}\".format(acc))\n",
    "\n",
    "            if early_stopping is not None and early_stopping(model,acc):\n",
    "                break\n",
    "\n",
    "    if dev is not None and early_stopping is not None:\n",
    "        early_stopping.set_best_state(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 0.0016196609940379858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving best weights from round 1\n",
      "  1%|          | 1/90 [00:10<15:24, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6278627862786279\n",
      "Average epoch loss: 0.001501814927905798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving best weights from round 2\n",
      "  2%|▏         | 2/90 [00:20<15:02, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6342634263426342\n",
      "Average epoch loss: 0.0014742235653102398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving best weights from round 3\n",
      "  3%|▎         | 3/90 [00:38<18:25, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6388638863886389\n",
      "Average epoch loss: 0.0014582787407562137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      "  4%|▍         | 4/90 [00:48<16:57, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6374637463746374\n",
      "Average epoch loss: 0.0014515924267470837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      "  6%|▌         | 5/90 [01:08<20:14, 14.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6297629762976298\n",
      "Average epoch loss: 0.001442103530280292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving best weights from round 6\n",
      "  7%|▋         | 6/90 [01:29<22:49, 16.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6465646564656465\n",
      "Average epoch loss: 0.0014398741768673062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      "  8%|▊         | 7/90 [01:48<23:28, 16.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6443644364436444\n",
      "Average epoch loss: 0.001430192613042891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      "  9%|▉         | 8/90 [02:08<24:43, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6458645864586459\n",
      "Average epoch loss: 0.0014347969554364681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 10%|█         | 9/90 [02:18<21:03, 15.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6417641764176417\n",
      "Average epoch loss: 0.0014273609267547727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 11%|█         | 10/90 [02:37<22:19, 16.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6418641864186418\n",
      "Average epoch loss: 0.0014319585170596838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 12%|█▏        | 11/90 [02:57<23:14, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6402640264026402\n",
      "Average epoch loss: 0.0014225579798221588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 13%|█▎        | 12/90 [03:07<19:53, 15.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6407640764076408\n",
      "Average epoch loss: 0.0014181816950440407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 14%|█▍        | 13/90 [03:27<21:24, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6378637863786378\n",
      "Average epoch loss: 0.001419287407770753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 16%|█▌        | 14/90 [03:37<18:38, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6383638363836384\n",
      "Average epoch loss: 0.0014201359590515494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Terminate\n",
      " 16%|█▌        | 14/90 [03:47<20:35, 16.25s/it]\n",
      "Loading weights from round 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.634963496349635\n"
     ]
    }
   ],
   "source": [
    "mname = 'ns_nn_sent'\n",
    "final_model = train(model, train_feats, 500, 1e-2, 90, dev_feats, early_stopping=EarlyStopping(mname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> We achieve a dev set performance of 64% </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training using tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Dropout,Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to build a layer similar to the ones we built using torch.\n",
    "\n",
    "`\n",
    "SimpleMLP(\n",
    "  (fc1): Linear(in_features=10001, out_features=100, bias=True)\n",
    "  (fc2): Linear(in_features=100, out_features=3, bias=True)\n",
    "  (do): Dropout(p=0.4, inplace=False)\n",
    "  (relu): ReLU()\n",
    ")\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training dataset = (145449, 10001)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = train_feats\n",
    "print(\"Shape of the training dataset =\", train_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model\n",
    "Build the model using keras functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to reshape the labels array so that they have the approriate dimensions for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A peek a the reshaped labels:\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "The datatypes of the training dataset, features=<class 'scipy.sparse.coo.coo_matrix'>, labels=<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.zeros(shape=(len(train_y),3))\n",
    "for idx, val in enumerate(train_y):\n",
    "    train_labels[idx][val]=1\n",
    "print(\"A peek a the reshaped labels:\")\n",
    "print(train_labels[:5])\n",
    "print(\"The datatypes of the training dataset, features={}, labels={}\".format(type(train_x), type(train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with sparse matrix in keras\n",
    "Some useful code is [here](https://stackoverflow.com/questions/37609892/keras-sparse-matrix-issue)\n",
    "\n",
    "The type of sparse matrix we have created is `scipy.sparse.coo.coo_matrix`, we will need to convert it to `scipy.sparse.csr.csr_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datatypes of the training dataset, features=<class 'scipy.sparse.csr.csr_matrix'>, labels=<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "train_x=sparse.csr_matrix(train_x)\n",
    "print(\"The datatypes of the training dataset, features={}, labels={}\".format(type(train_x), type(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A peek a the reshaped dev labels:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_x, dev_y = dev_feats\n",
    "dev_x=sparse.csr_matrix(dev_x)\n",
    "dev_labels = np.zeros(shape=(len(dev_y),3))\n",
    "for idx, val in enumerate(dev_y):\n",
    "    dev_labels[idx][val]=1\n",
    "print(\"A peek a the reshaped dev labels:\")\n",
    "dev_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save these sparse matrices \n",
    "As npz files so that we can continue this training on other system without having to pull in the expensive large fever datasets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim_texts.jsonl  matching_page_sentences.jsonl  \u001b[0m\u001b[01;34mtraining\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls working/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_file = \"working/data/train_x.npz\"\n",
    "np.savez(train_x_file, train_x)\n",
    "train_lbl_file = \"working/data/train_labels.npz\"\n",
    "np.savez(train_lbl_file, train_labels)\n",
    "\n",
    "\n",
    "dev_x_file = \"working/data/dev_x.npz\"\n",
    "np.savez(dev_x_file, dev_x)\n",
    "dev_lbl_file = \"working/data/dev_labels.npz\"\n",
    "np.savez(dev_lbl_file, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 45M\n",
      "-rw-r--r-- 1 root root 235K Jul 12 12:49 dev_labels.npz\n",
      "-rw-r--r-- 1 root root 2.3M Jul 12 12:49 dev_x.npz\n",
      "-rw-r--r-- 1 root root 3.4M Jul 12 12:49 train_labels.npz\n",
      "-rw-r--r-- 1 root root  39M Jul 12 12:49 train_x.npz\n",
      "-rw-r--r-- 1 root root    0 Jul  5 07:26 matching_page_sentences.jsonl\n",
      "drwxr-xr-x 5 root root  160 Jul  5 07:26 \u001b[0m\u001b[01;34mtraining\u001b[0m/\n",
      "-rw-r--r-- 1 root root    0 Jul  5 07:26 claim_texts.jsonl\n"
     ]
    }
   ],
   "source": [
    "ls -lth working/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use batching via data generators\n",
    "\n",
    "We did not use data generators for training. Let's use data generators that will feed data to out training.\n",
    "\n",
    "We will need to write a custom generator here since we are using scipy spare matrix and not tensors.\n",
    "\n",
    "The generator will be called repeatedly by the trainer (model.fit) and each time it is called, we will need to return it a set of data from the dataset.\n",
    "\n",
    "The batch size will be controller by the caller, i.e. the trainer, but we will need to keep track of the records we are sending back so that we know when to reset and loop over.\n",
    "\n",
    "The generator must be iterable and would keep a track of the number of batches we will need to create and track the records we are sending.\n",
    "\n",
    "If the number of records is not perfectly divisible by batch_size, we will run into issues with the generator. For now, we will deal with it by dropping the last set of records if there are fewer than batch_size records in the last batch.\n",
    "\n",
    "Since we are dealing with scipy sparse matrix, we would not be able to send in the data as argument to the generator. We will therefore hardcode the values in the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 10001)]     0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 100)         1000200   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 3)           303       \n",
      "=================================================================\n",
      "Total params: 1,000,503\n",
      "Trainable params: 1,000,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "290/290 [==============================] - 15s 51ms/step - loss: 0.8899 - accuracy: 0.6021 - val_loss: 0.8225 - val_accuracy: 0.6252\n",
      "Epoch 2/10\n",
      "290/290 [==============================] - 13s 46ms/step - loss: 0.5947 - accuracy: 0.7667 - val_loss: 0.8406 - val_accuracy: 0.6353\n",
      "Epoch 3/10\n",
      "290/290 [==============================] - 13s 45ms/step - loss: 0.5340 - accuracy: 0.7928 - val_loss: 0.8591 - val_accuracy: 0.6403\n",
      "Epoch 4/10\n",
      "290/290 [==============================] - 12s 43ms/step - loss: 0.4920 - accuracy: 0.8094 - val_loss: 0.8869 - val_accuracy: 0.6463\n",
      "Epoch 5/10\n",
      "290/290 [==============================] - 12s 43ms/step - loss: 0.4620 - accuracy: 0.8242 - val_loss: 0.9135 - val_accuracy: 0.6477\n",
      "Epoch 6/10\n",
      "290/290 [==============================] - 12s 41ms/step - loss: 0.4295 - accuracy: 0.8348 - val_loss: 0.9396 - val_accuracy: 0.6483\n",
      "Epoch 7/10\n",
      "290/290 [==============================] - 11s 38ms/step - loss: 0.4053 - accuracy: 0.8451 - val_loss: 0.9608 - val_accuracy: 0.6474\n",
      "Epoch 8/10\n",
      "290/290 [==============================] - 12s 41ms/step - loss: 0.3768 - accuracy: 0.8583 - val_loss: 0.9900 - val_accuracy: 0.6468\n",
      "Epoch 9/10\n",
      "290/290 [==============================] - 11s 39ms/step - loss: 0.3592 - accuracy: 0.8650 - val_loss: 1.0164 - val_accuracy: 0.6489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9dac2a9ee0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#train_x is the concatenation of the tf vectors for the claim\n",
    "x = train_x\n",
    "labels_3 = train_labels\n",
    "\n",
    "dim = train_x.shape[1]\n",
    "num_examples = train_x.shape[0]\n",
    "lr = 0.001\n",
    "                \n",
    "# This is tf.data.experimental.AUTOTUNE in older tensorflow.\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def generator_fn(n_samples):\n",
    "    \"\"\"Return a function that takes no arguments and returns a generator.\"\"\"\n",
    "    def generator():\n",
    "        num_batches = num_examples/n_samples\n",
    "        counter = 0\n",
    "        if counter == 0:\n",
    "            idx = np.arange(num_examples)\n",
    "            np.random.shuffle(idx)\n",
    "        \n",
    "        while counter < num_batches:\n",
    "            index_batch = idx[n_samples*counter:n_samples*(counter+1)]\n",
    "            counter += 1\n",
    "            rec = x[index_batch, :].todense()\n",
    "            if len(rec) == n_samples:\n",
    "                yield rec, labels_3[index_batch]\n",
    "        counter = 0\n",
    "\n",
    "    return generator\n",
    "\n",
    "samples = 500\n",
    "#we are handling the batching with the samples, set the batch_size to 1, don't let dataset do any batching, the generator already does\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "\n",
    "# Create dataset.\n",
    "gen = generator_fn(n_samples=samples)\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator=gen, \n",
    "    output_types=(np.float32, np.int32), \n",
    "    output_shapes=((samples, dim), (samples, 3))\n",
    ")\n",
    "\n",
    "#we are handling the batching with the samples, set the batch_size to 1, don't let dataset do any batching, the generator already does\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# Prepare model.\n",
    "\n",
    "inp = keras.Input(shape=(None, dim), sparse=False)\n",
    "x1 = Dense(100, activation='relu')(inp)\n",
    "x2 = Dropout(0.4)(x1)\n",
    "x3 = keras.layers.Dense(3, activation='softmax')(x2)\n",
    "model = keras.Model(inp, x3)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=lr), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "# Train. Do not specify batch size because the dataset takes care of that.\n",
    "model.fit(dataset, epochs=epochs, callbacks=[stop_early], validation_data=(dev_x, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6488648653030396"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, dev_acc = model.evaluate(dev_x, dev_labels, verbose=0)\n",
    "dev_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLI model\n",
    "\n",
    "Now that we have a baseline model, we will try an NLI model to see if we can improve on the benchmark we have just set.\n",
    "\n",
    "First, we would have to build our data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
       "  'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'},\n",
       " {'claim': 'Roman Atwood is a content creator .',\n",
       "  'evidence': [('Roman_Atwood', 1), ('Roman_Atwood', 3)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'},\n",
       " {'claim': 'History of art includes architecture , dance , sculpture , music , painting , poetry literature , theatre , narrative , film , photography and graphic arts .',\n",
       "  'evidence': [('History_of_art', 2)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'},\n",
       " {'claim': 'Adrienne Bailon is an accountant .',\n",
       "  'evidence': [('Adrienne_Bailon', 0)],\n",
       "  'label': 1,\n",
       "  'label_text': 'REFUTES'},\n",
       " {'claim': 'System of a Down briefly disbanded in limbo .',\n",
       "  'evidence': [('In_Limbo', -1)],\n",
       "  'label': 2,\n",
       "  'label_text': 'NOT ENOUGH INFO'}]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_formatted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145449, 10001)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145449, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generator():\n",
    "    for data, lbl in zip(x, labels_3):\n",
    "        d = data.todense()\n",
    "        #note:  d is a matrix, this cannot be sent in as a feature to our model to train, we need to reshape this into an array\n",
    "        d = np.asarray(d).reshape(-1)\n",
    "        yield d, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    generator = lambda: get_data_generator()\n",
    "    return tf.data.Dataset.from_generator(\n",
    "            generator, output_signature=(\n",
    "            tf.TensorSpec(shape=(10001, ), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(3, ), dtype=tf.int32,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(10001,), dtype=int32)\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(10001,), dtype=int32)\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(10001,), dtype=int32)\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(10001,), dtype=int32)\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(10001,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for d in get_dataset().take(5):\n",
    "    print(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 3200\n",
    "BATCH_SIZE = 16\n",
    "ds_train = get_dataset()\n",
    "ds_train = ds_train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10001)\n"
     ]
    }
   ],
   "source": [
    "for d in ds_train.take(1):\n",
    "    print(d[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, None, 50)          400050    \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 100)               40400     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 450,853\n",
      "Trainable params: 450,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim = 50\n",
    "vocab_size = 8000\n",
    "inp = keras.Input(shape=(None, ))\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim)\n",
    "\n",
    "x1 = embedding_layer(inp)\n",
    "\n",
    "lstm_layer1 = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim)))(x1)\n",
    "\n",
    "x2 = Dense(100, activation='relu')(lstm_layer1)\n",
    "x3 = Dropout(0.1)(x2)\n",
    "output = keras.layers.Dense(3, activation='softmax')(x3)\n",
    "model = keras.Model(inputs=inp, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=lr), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "      3/Unknown - 46s 14s/step - loss: 1.0982 - accuracy: 0.3333"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-7d7434343b34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_early\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=epochs, callbacks=[stop_early], validation_data=(dev_x, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
