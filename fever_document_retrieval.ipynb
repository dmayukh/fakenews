{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEVER dataset processing\n",
    "\n",
    "<h5>Process the claims in the fever dataset</h5>\n",
    "\n",
    "In this notebook, we will prepare the training dataset and buid a baseline model that would set us up for the NLI tasks\n",
    "\n",
    "We use the following repos for reference code:\n",
    "\n",
    "- [fever-baselines](https://github.com/klimzaporojets/fever-baselines.git)\n",
    "- [fever-allennlp-reader](https://github.com/j6mes/fever-allennlp-reader)\n",
    "- [fever-allennlp](https://github.com/j6mes/fever-allennlp)\n",
    "\n",
    "Note, AllenNLP here is used only for the NLI training, using models such as Decomposable Attention, Elmo + ESIM, ESIM etc. We will not use any of it here.\n",
    "In this notebook, we will first focus on extracting the data from the pre-processed Wiki corpus provided by [fever.ai](https://fever.ai/dataset/fever.html).\n",
    "\n",
    "The data is available in a [docker image](https://hub.docker.com/r/feverai/common), 21GB in size. The container is created and the volume /local/ from it is mounted and made available to our [container](https://github.com/dmayukh/fakenews/Dockerfile) \n",
    "\n",
    "\n",
    "We will install a few dependencies such as:\n",
    "- numpy>=1.15\n",
    "- regex\n",
    "- allennlp==2.5.0\n",
    "- fever-scorer==2.0.39\n",
    "- fever-drqa==1.0.13\n",
    "\n",
    "The following packages are installed by the above dependencies\n",
    "- torchvision-0.9.1\n",
    "- google_cloud_storage-1.38.0\n",
    "- overrides==3.1.0\n",
    "- transformers-4.6.1\n",
    "- spacy-3.0.6\n",
    "- sentencepiece-0.1.96\n",
    "- torch-1.8.1\n",
    "- wandb-0.10.33\n",
    "- lmdb-1.2.1\n",
    "- jsonnet-0.17.0\n",
    "\n",
    "We do not really need allennlp or fever-scorer as of yet, we would only need DrQA. We would prefer to use the DrQA from the official github, but for now we will go with what was prepackaged by the [j6mes](https://pypi.org/project/fever-drqa/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Pre-parsed FEVER Datasets</h4>\n",
    "Create the database from the DB file that contains the preprocessed Wiki pages. This DB was made available to us by FEVER.\n",
    "\n",
    "FeverDocDB is a simple wrapper that opens a SQLlite3 connection to the database and provides methods to execute simple select queries to fetch ids for documents and to fetch lines given a document.\n",
    "\n",
    "We will not require this in the first pass of our work here, since we are only interested in findings the documents closest to a claim text.\n",
    "\n",
    "The function to fetch lines per document is what uses the connection to the database. In order to find the closest documents for a given claim, we use the ranker that uses a <b>pre-created TFIDF index</b> which can locate the document ids given a claim text.\n",
    "\n",
    "The pre-created index is available in '/local/fever-common/data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz'\n",
    "\n",
    "\n",
    "Sample data from training file:\n",
    "\n",
    "> {\"id\": 75397, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\", \"evidence\": [[[92206, 104971, \"Nikolaj_Coster-Waldau\", 7], [92206, 104971, \"Fox_Broadcasting_Company\", 0]]]}\n",
    "\n",
    "A closer look at the evidence:\n",
    "\n",
    "> [[92206, 104971, \"Nikolaj_Coster-Waldau\", 7]\n",
    "\n",
    "92206 and 104971 are the annotation ids, while the \"Nikolaj_Coster-Waldau\" is the evidence page and the line number is 7.\n",
    "\n",
    "\n",
    "#### Formatting the input text\n",
    "\n",
    "The training of the model is done on the evidence provided by the human annotators, therefore we use the 'evidence' to run our training.\n",
    "\n",
    "After formatting, the training examples are written as below that is then used to train the MLP\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
    "  'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
    "  'label': 0,\n",
    "  'label_text': 'SUPPORTS'}\n",
    "\n",
    "The baseline model is a simple MLP that uses the count vectorizer to vectorize the claim text and the evidence page texts. It also uses an additional feature which is the cosine similarity between the vectorized claim text and the vectorized combined texts from all the evidences.\n",
    "\n",
    "The vectorizers are saved to the filesystem that can be used later for transorming the incoming sentences.\n",
    "\n",
    "The trained model is used to run eval on the dev dataset of the same format.\n",
    "\n",
    "\n",
    "<h5>Retrieval of the evidence</h5>\n",
    "\n",
    "We also attempt to extract the evidence from the corresponding pages\n",
    "\n",
    "First, using the tfidf doc ranker, we extract the top 5 pages that are similar to the claim text\n",
    "\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .', 'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)], 'label': 0, 'label_text': 'SUPPORTS', 'predicted_pages': [('Coster', 498.82682448841246), ('Nikolaj', 348.42021460316823), ('The_Other_Woman_-LRB-2014_film-RRB-', 316.8405030379064), ('Nikolaj_Coster-Waldau', 316.8405030379064), ('Nukaaka_Coster-Waldau', 292.47605893902585)]}\n",
    "\n",
    "For each of the pages, we extract the lines from the page text and use 'online tfidf ranker' to fetch the closest matching lines from the text.\n",
    "\n",
    "The training examples are then formatted as below which is then used to run EVAL on the MLP model\n",
    "\n",
    "\n",
    "> {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
    " 'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
    " 'label': 0,\n",
    " 'label_text': 'SUPPORTS',\n",
    " 'predicted_pages': [('Coster', 498.82682448841246),\n",
    "  ('Nikolaj', 348.42021460316823),\n",
    "  ('The_Other_Woman_-LRB-2014_film-RRB-', 316.8405030379064),\n",
    "  ('Nikolaj_Coster-Waldau', 316.8405030379064),\n",
    "  ('Nukaaka_Coster-Waldau', 292.47605893902585)],\n",
    " 'predicted_sentences': [('Nikolaj', 7),\n",
    "  ('The_Other_Woman_-LRB-2014_film-RRB-', 1),\n",
    "  ('Nukaaka_Coster-Waldau', 1),\n",
    "  ('Coster', 63),\n",
    "  ('Nikolaj_Coster-Waldau', 0)]}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = 'data/data'\n",
    "working_dir = 'working/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 13114, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"J. R. R. Tolkien created Gimli.\", \"evidence\": [[[28359, 34669, \"Gimli_-LRB-Middle-earth-RRB-\", 0]], [[28359, 34670, \"Gimli_-LRB-Middle-earth-RRB-\", 1]]]}\n",
      "{\"id\": 152180, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Susan Sarandon is an award winner.\", \"evidence\": [[[176133, 189101, \"Susan_Sarandon\", 1]], [[176133, 189102, \"Susan_Sarandon\", 2]], [[176133, 189103, \"Susan_Sarandon\", 8]]]}\n"
     ]
    }
   ],
   "source": [
    "!tail -2 data/data/fever-data/train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
      "{\"id\": 163803, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Ukrainian Soviet Socialist Republic was a founding participant of the UN.\", \"evidence\": [[[296950, 288668, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[298602, 290067, \"Ukrainian_Soviet_Socialist_Republic\", 7], [298602, 290067, \"United_Nations\", 0]], [[300696, 291816, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344347, 327887, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344994, 328433, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344997, 328435, \"Ukrainian_Soviet_Socialist_Republic\", 7]]]}\n"
     ]
    }
   ],
   "source": [
    "!head -2 data/data/fever-data/paper_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training dataset\n",
    "\n",
    "The training examples have three (3) classes:\n",
    "- SUPPORTS\n",
    "- REFUTES\n",
    "- NOT ENOUGH INFO\n",
    "\n",
    "For the 'NOT ENOUGH INFO' class, the evidences are set to None. This would cause problems with training since we would still like to generate features for the samples which have been put in this class.\n",
    "\n",
    "Next, we will loop over the records in the training dataset to create the training records. Specifically, we would be generating evidences for the samples in the 'NOT ENOUGH INFO' class so that the None values now have some page information.\n",
    "\n",
    "Our strategy for dealing with missing evidences for the 'NOT ENOUGH INFO' class is to find the pages that are closest to the claims based on the tfidf similarity. The tfidf similarity of the documents in the fever DB is already precomputed and make available to us via the index file:\n",
    "\n",
    "> '/local/fever-common/data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the directory where we will save our prepared datasets\n",
    "\n",
    "The raw training data is available at \n",
    "\n",
    "> /local/fever-common/data/fever-data/train.jsonl\n",
    "\n",
    "The raw dev data from the FEVER paper is available at \n",
    "\n",
    "> /local/fever-common/data/fever-data/paper_dev.jsonl\n",
    "\n",
    "We wil generate the training dataset by sampling for NEI examples based on closest document match against our claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p working/data/training/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.dataset.DatasetGenerator import DatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145449 working/data/training/train.ns.pages.p5.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l working/data/training/train.ns.pages.p5.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/145449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to working/data/training/baseline//train.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [25:35<00:00, 94.70it/s] \n"
     ]
    }
   ],
   "source": [
    "ds_generator = DatasetGenerator(dataset_root='data/data/',out_dir='working/data/training/baseline/', database_path='data/data/fever/fever.db')\n",
    "ds_generator.generate_nei_evidences('train', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/9999 [00:00<09:23, 17.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to working/data/training/baseline//paper_dev.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [02:23<00:00, 69.78it/s] \n"
     ]
    }
   ],
   "source": [
    "ds_generator = DatasetGenerator(dataset_root='data/data/',out_dir='working/data/training/baseline/', database_path='data/data/fever/fever.db')\n",
    "ds_generator.generate_nei_evidences('paper_dev', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 working/data/training/baseline/paper_dev.ns.pages.p5.jsonl\n",
      "  145449 working/data/training/baseline/train.ns.pages.p5.jsonl\n",
      "  155448 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l  working/data/training/baseline/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the feature sets\n",
    "\n",
    "Using the training data and dev data we generated, we will create the vectorizers and save them to local files\n",
    "\n",
    "The training and dev data is available at \n",
    "\n",
    "> working/data/training/baseline/train.ns.pages.p5.jsonl \n",
    "\n",
    "> working/data/training/baseline/paper_dev.ns.pages.p5.jsonl\n",
    "\n",
    "The key information we need from the training samples are the claim text and the texts from the evidence pages\n",
    "\n",
    "For each training example, generate:\n",
    "- a tokenized claim, \n",
    "- the label id, \n",
    "- the label text, \n",
    "- list of wiki pages that were provided as evidence.\n",
    "\n",
    "This is done using a custom formatter `training_line_formatter` we would write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.dataset.DatasetReader import DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:01<00:00, 77541.19it/s] \n",
      "100%|██████████| 145449/145449 [00:01<00:00, 139881.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(3,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/training/baseline/train.ns.pages.p5.jsonl'\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=None, database_path='data/data/fever/fever.db')\n",
    "raw, data = dsreader.read()\n",
    "ds_train = dsreader.get_dataset()\n",
    "print(ds_train.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the label encoder from training, we will need them for the dev dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "label_checkpoint_file = 'working/data/training/baseline/label_encoder_train.pkl'\n",
    "with open(label_checkpoint_file, 'wb') as f:\n",
    "    pickle.dump(dsreader.labelencoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 175393.42it/s]\n",
      "100%|██████████| 9999/9999 [00:00<00:00, 196706.67it/s]\n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/training/baseline/paper_dev.ns.pages.p5.jsonl'\n",
    "label_checkpoint_file = 'working/data/training/baseline/label_encoder_train.pkl'\n",
    "#note, use type = 'train' since formatting would be like the train examples\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=label_checkpoint_file, database_path='data/data/fever/fever.db', type='train')\n",
    "raw_dev, data_dev = dsreader.read()\n",
    "ds_dev = dsreader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build feature set \n",
    "\n",
    "We will build a <b>term frequency vectorizer</b> and a TDIDF vectorizer and save them to a file.\n",
    "\n",
    "The vocabulary will be limited to 5000. For each of the claim and the body text, we would produce the vectors which would be of dimension 5000.\n",
    "\n",
    "We will also add the cosine similarity between the claim vector and the body text vector and use it as an additional feature.\n",
    "\n",
    "The dimension of our feature would be then 5000 + 5000 + 1 = 10001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the vectorizers\n",
    "We will be using the contents of both the training and dev set to build the vectorizers. \n",
    "\n",
    "We will need to read the dataset into memory from the td.dataset readers, since CountVectorizers cannot operate in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "max_len = 4  # Sequence length to pad the outputs to.\n",
    "bow_vectorizer = TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len)\n",
    "freq_vectorizer = TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='count')\n",
    "tfidf_vectorizer = TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='tf-idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_train.map(lambda x, y: x[0] + ' ' + x[1])\n",
    "bow_vectorizer.adapt(ds.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_vectorizer.adapt(ds.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.adapt(ds.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x7fcebb5394d0>) with an unsupported type (<class 'tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b0422103807e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'working/data/training/baseline/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m tf.io.write_file(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'bow_vectorizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mwrite_file\u001b[0;34m(filename, contents, name)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m       return write_file_eager_fallback(\n\u001b[0;32m-> 2266\u001b[0;31m           filename, contents, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   2267\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mwrite_file_eager_fallback\u001b[0;34m(filename, contents, name, ctx)\u001b[0m\n\u001b[1;32m   2293\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_file_eager_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m   \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m   \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (<tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x7fcebb5394d0>) with an unsupported type (<class 'tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "### Save the vectorizers\n",
    "import os\n",
    "path = 'working/data/training/baseline/'\n",
    "tf.io.write_file(\n",
    "    path + 'bow_vectorizer.pkl', bow_vectorizer, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'working/data/training/baseline/'\n",
    "# with open(os.path.join(path + 'freq_vectorizer.pkl'), \"wb+\") as f:\n",
    "#     pickle.dump(freq_vectorizer, f)\n",
    "# path = 'working/data/training/baseline/'\n",
    "# with open(os.path.join(path + 'tfidf_vectorizer.pkl'), \"wb+\") as f:\n",
    "#     pickle.dump(tfidf_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'the',\n",
       " 'and',\n",
       " 'in',\n",
       " 'of',\n",
       " 'a',\n",
       " 'is',\n",
       " 'rrb',\n",
       " 'lrb',\n",
       " 'end',\n",
       " 'start',\n",
       " 'by',\n",
       " 'was',\n",
       " 'for',\n",
       " 'as',\n",
       " 'to',\n",
       " 'film',\n",
       " 'on',\n",
       " 'an',\n",
       " 's',\n",
       " 'american',\n",
       " 'with',\n",
       " 'he',\n",
       " 'his',\n",
       " 'born',\n",
       " 'from',\n",
       " 'has',\n",
       " 'series',\n",
       " 'her',\n",
       " 'award',\n",
       " 'best',\n",
       " 'which',\n",
       " 'it',\n",
       " 'at',\n",
       " 'she',\n",
       " 'known',\n",
       " 'television',\n",
       " 'first',\n",
       " 'lsb',\n",
       " 'rsb',\n",
       " 'also',\n",
       " 'actor',\n",
       " 'one',\n",
       " 'directed',\n",
       " 'that',\n",
       " 'album',\n",
       " 'united',\n",
       " 'released',\n",
       " 'world',\n",
       " 'or',\n",
       " 'actress',\n",
       " 'who',\n",
       " 'films',\n",
       " 'states',\n",
       " 'won',\n",
       " 'drama',\n",
       " 'its',\n",
       " 'awards',\n",
       " 'most',\n",
       " 'written',\n",
       " 'role',\n",
       " 'two',\n",
       " 'after',\n",
       " 'are',\n",
       " 'new',\n",
       " 'including',\n",
       " 'academy',\n",
       " 'city',\n",
       " 'comedy',\n",
       " 'band',\n",
       " 'producer',\n",
       " '2016',\n",
       " 'stars',\n",
       " 'singer',\n",
       " '2012',\n",
       " 'received',\n",
       " 'music',\n",
       " 'based',\n",
       " 'john',\n",
       " 'may',\n",
       " 'name',\n",
       " '2015',\n",
       " 'debut',\n",
       " 'been',\n",
       " 'second',\n",
       " 'produced',\n",
       " '2014',\n",
       " 'million',\n",
       " 'starring',\n",
       " 'roles',\n",
       " '2013',\n",
       " 'their',\n",
       " '2011',\n",
       " 'rock',\n",
       " 'three',\n",
       " 'had',\n",
       " 'studio',\n",
       " 'english',\n",
       " 'all',\n",
       " 'british',\n",
       " 'director',\n",
       " 'other',\n",
       " 'such',\n",
       " 'time',\n",
       " 'show',\n",
       " '2010',\n",
       " 'golden',\n",
       " 'character',\n",
       " 'career',\n",
       " 'starred',\n",
       " 'be',\n",
       " '2008',\n",
       " 'only',\n",
       " 'have',\n",
       " 'over',\n",
       " 'april',\n",
       " '2009',\n",
       " 'not',\n",
       " 'year',\n",
       " 'appeared',\n",
       " '2006',\n",
       " 'us',\n",
       " 'became',\n",
       " 'war',\n",
       " 'during',\n",
       " 'country',\n",
       " 'song',\n",
       " 'made',\n",
       " 'season',\n",
       " 'march',\n",
       " 'july',\n",
       " 'four',\n",
       " 'september',\n",
       " 'professional',\n",
       " 'years',\n",
       " 'november',\n",
       " 'were',\n",
       " 'novel',\n",
       " 'october',\n",
       " 'more',\n",
       " '2007',\n",
       " 'david',\n",
       " 'june',\n",
       " 'played',\n",
       " '2005',\n",
       " 'well',\n",
       " 'august',\n",
       " 'several',\n",
       " 'state',\n",
       " 'globe',\n",
       " 'south',\n",
       " 'began',\n",
       " 'same',\n",
       " 'january',\n",
       " 'december',\n",
       " 'james',\n",
       " 'since',\n",
       " 'february',\n",
       " 'nominated',\n",
       " 'into',\n",
       " 'record',\n",
       " 'national',\n",
       " 'number',\n",
       " 'records',\n",
       " '2004',\n",
       " 'group',\n",
       " 'supporting',\n",
       " 'international',\n",
       " '2003',\n",
       " 'football',\n",
       " '2',\n",
       " 'game',\n",
       " 'lead',\n",
       " 'before',\n",
       " 'michael',\n",
       " 'work',\n",
       " 'member',\n",
       " 'york',\n",
       " 'star',\n",
       " 'third',\n",
       " 'later',\n",
       " 'major',\n",
       " 'part',\n",
       " 'created',\n",
       " 'george',\n",
       " 'science',\n",
       " 'team',\n",
       " 'north',\n",
       " 'but',\n",
       " 'player',\n",
       " 'artist',\n",
       " 'nominations',\n",
       " 'story',\n",
       " 'romantic',\n",
       " 'premiered',\n",
       " 'fiction',\n",
       " '2001',\n",
       " 'single',\n",
       " '1999',\n",
       " '2000',\n",
       " 'many',\n",
       " 'movie',\n",
       " 'performance',\n",
       " 'president',\n",
       " 'company',\n",
       " 'five',\n",
       " 'club',\n",
       " 'thriller',\n",
       " 'america',\n",
       " '2017',\n",
       " 'life',\n",
       " 'west',\n",
       " 'largest',\n",
       " 'writer',\n",
       " 'songwriter',\n",
       " 'than',\n",
       " 'being',\n",
       " 'top',\n",
       " '1998',\n",
       " 'both',\n",
       " 'king',\n",
       " 'indian',\n",
       " 'man',\n",
       " 'worldwide',\n",
       " 'him',\n",
       " 'include',\n",
       " 'former',\n",
       " '2002',\n",
       " '10',\n",
       " 'people',\n",
       " 'league',\n",
       " 'action',\n",
       " 'albums',\n",
       " 'early',\n",
       " 'robert',\n",
       " 'author',\n",
       " 'history',\n",
       " 'between',\n",
       " 'called',\n",
       " 'california',\n",
       " 'love',\n",
       " '1',\n",
       " 'musical',\n",
       " 'emmy',\n",
       " 'this',\n",
       " 'french',\n",
       " '1997',\n",
       " 'through',\n",
       " 'kingdom',\n",
       " 'nomination',\n",
       " 'i',\n",
       " 'stage',\n",
       " 'cast',\n",
       " 'when',\n",
       " 'fictional',\n",
       " 'singles',\n",
       " 'school',\n",
       " 'fantasy',\n",
       " 'while',\n",
       " 'about',\n",
       " 'they',\n",
       " 'black',\n",
       " 'play',\n",
       " 'home',\n",
       " 'acting',\n",
       " 'original',\n",
       " 'book',\n",
       " 'disney',\n",
       " 'tv',\n",
       " 'crime',\n",
       " '3',\n",
       " 'no',\n",
       " 'where',\n",
       " 'horror',\n",
       " 'video',\n",
       " 'de',\n",
       " 'release',\n",
       " 'success',\n",
       " '4',\n",
       " '20',\n",
       " 'fame',\n",
       " 'published',\n",
       " 'worked',\n",
       " 'young',\n",
       " 'until',\n",
       " 'family',\n",
       " 'successful',\n",
       " 'following',\n",
       " '16',\n",
       " 'paul',\n",
       " 'age',\n",
       " 'under',\n",
       " 'area',\n",
       " 'republic',\n",
       " '1996',\n",
       " 'named',\n",
       " 'actors',\n",
       " 'earned',\n",
       " 'england',\n",
       " 'marvel',\n",
       " 'island',\n",
       " 'times',\n",
       " 'comics',\n",
       " 'musician',\n",
       " 'picture',\n",
       " 'university',\n",
       " 'girl',\n",
       " 'title',\n",
       " '100',\n",
       " '18',\n",
       " '1994',\n",
       " 'founded',\n",
       " 'up',\n",
       " '1995',\n",
       " 'pictures',\n",
       " 'leading',\n",
       " 'model',\n",
       " 'richard',\n",
       " 'high',\n",
       " 'out',\n",
       " 'century',\n",
       " 'screenwriter',\n",
       " 'london',\n",
       " 'ii',\n",
       " 'martin',\n",
       " '9',\n",
       " 'last',\n",
       " 'some',\n",
       " 'j',\n",
       " 'located',\n",
       " 'my',\n",
       " 'often',\n",
       " '15',\n",
       " 'games',\n",
       " 'formed',\n",
       " '12',\n",
       " 'screenplay',\n",
       " '1993',\n",
       " 'production',\n",
       " 'tom',\n",
       " 'western',\n",
       " '8',\n",
       " '13',\n",
       " '22',\n",
       " 'there',\n",
       " 'death',\n",
       " 'comedydrama',\n",
       " 'critics',\n",
       " 'fourth',\n",
       " 'canadian',\n",
       " 'billboard',\n",
       " 'los',\n",
       " 'tennis',\n",
       " 'includes',\n",
       " 'playing',\n",
       " 'great',\n",
       " 'william',\n",
       " '17',\n",
       " 'association',\n",
       " 'you',\n",
       " 'developed',\n",
       " 'popular',\n",
       " 'championship',\n",
       " 'lee',\n",
       " 'house',\n",
       " 'among',\n",
       " 'day',\n",
       " '5',\n",
       " 'screen',\n",
       " 'r',\n",
       " 'population',\n",
       " 'entertainment',\n",
       " '24',\n",
       " 'political',\n",
       " 'night',\n",
       " 'angeles',\n",
       " '1992',\n",
       " 'set',\n",
       " '21',\n",
       " 'comic',\n",
       " 'six',\n",
       " 'capital',\n",
       " '6',\n",
       " 'wrote',\n",
       " 'east',\n",
       " '11',\n",
       " '7',\n",
       " 'central',\n",
       " '25',\n",
       " '1991',\n",
       " '1986',\n",
       " 'critical',\n",
       " 'big',\n",
       " 'winning',\n",
       " 'empire',\n",
       " 'europe',\n",
       " 'comedian',\n",
       " 'feature',\n",
       " 'final',\n",
       " '1989',\n",
       " 'party',\n",
       " 'can',\n",
       " 'officially',\n",
       " 'chris',\n",
       " 'hall',\n",
       " 'making',\n",
       " 'live',\n",
       " '23',\n",
       " 'seven',\n",
       " 'christopher',\n",
       " 'greatest',\n",
       " 'san',\n",
       " 'features',\n",
       " 'place',\n",
       " 'served',\n",
       " 'grammy',\n",
       " 'grand',\n",
       " '14',\n",
       " 'led',\n",
       " 'fox',\n",
       " 'motion',\n",
       " 'scott',\n",
       " 'canada',\n",
       " 'women',\n",
       " 'animated',\n",
       " 'person',\n",
       " 'commercial',\n",
       " '30',\n",
       " 'jr',\n",
       " 'european',\n",
       " 'queen',\n",
       " 'independent',\n",
       " '26',\n",
       " 'voice',\n",
       " 'fifth',\n",
       " 'network',\n",
       " '1980',\n",
       " 'seasons',\n",
       " 'india',\n",
       " 'featured',\n",
       " 'plays',\n",
       " '1988',\n",
       " '1969',\n",
       " 'solo',\n",
       " 'australian',\n",
       " 'will',\n",
       " 'titles',\n",
       " 'sold',\n",
       " 'then',\n",
       " 'festival',\n",
       " 'jones',\n",
       " 'portrayed',\n",
       " 'dark',\n",
       " 'charles',\n",
       " 'studios',\n",
       " 'along',\n",
       " '1990',\n",
       " 'died',\n",
       " 'peter',\n",
       " 'children',\n",
       " 'elizabeth',\n",
       " 'men',\n",
       " 'rapper',\n",
       " 'cup',\n",
       " 'champion',\n",
       " 'franchise',\n",
       " 'language',\n",
       " 'late',\n",
       " 'books',\n",
       " 'used',\n",
       " 'sequel',\n",
       " 'sitcom',\n",
       " 'jackson',\n",
       " '19',\n",
       " 'office',\n",
       " 'me',\n",
       " '1985',\n",
       " 'hollywood',\n",
       " 'williams',\n",
       " 'adaptation',\n",
       " 'win',\n",
       " 'social',\n",
       " 'referred',\n",
       " 'bafta',\n",
       " 'included',\n",
       " 'held',\n",
       " 'populous',\n",
       " 'hot',\n",
       " 'theatre',\n",
       " 'region',\n",
       " 'mark',\n",
       " 'recording',\n",
       " 'guild',\n",
       " 'gained',\n",
       " 'politician',\n",
       " 'open',\n",
       " 'la',\n",
       " '1975',\n",
       " 'any',\n",
       " 'uk',\n",
       " '27',\n",
       " 'songs',\n",
       " 'commonly',\n",
       " 'novels',\n",
       " 'frank',\n",
       " '1971',\n",
       " 'countries',\n",
       " 'rose',\n",
       " 'arts',\n",
       " 'acclaim',\n",
       " 'went',\n",
       " '1984',\n",
       " 'ranked',\n",
       " 'list',\n",
       " 'wars',\n",
       " 'appearing',\n",
       " 'superhero',\n",
       " 'park',\n",
       " 'female',\n",
       " 'various',\n",
       " 'married',\n",
       " '1981',\n",
       " '28',\n",
       " 'works',\n",
       " 'epic',\n",
       " 'aired',\n",
       " 'system',\n",
       " 'adapted',\n",
       " 'france',\n",
       " 'considered',\n",
       " 'brother',\n",
       " 'highest',\n",
       " 'jane',\n",
       " 'tour',\n",
       " 'media',\n",
       " '1976',\n",
       " 'red',\n",
       " '1987',\n",
       " 'steven',\n",
       " 'each',\n",
       " 'numerous',\n",
       " 'did',\n",
       " 'outstanding',\n",
       " 'child',\n",
       " 'members',\n",
       " 'nba',\n",
       " 'characters',\n",
       " 'came',\n",
       " 'signed',\n",
       " 'against',\n",
       " 'italian',\n",
       " 'main',\n",
       " 'washington',\n",
       " 'joseph',\n",
       " 'eight',\n",
       " 'bill',\n",
       " 'county',\n",
       " '1982',\n",
       " 'africa',\n",
       " 'historical',\n",
       " 'having',\n",
       " 'originally',\n",
       " 'woman',\n",
       " 'short',\n",
       " 'edward',\n",
       " 'hit',\n",
       " '29',\n",
       " 'productions',\n",
       " 'dc',\n",
       " 'reviews',\n",
       " 'episodes',\n",
       " 'thomas',\n",
       " 'brothers',\n",
       " 'walt',\n",
       " 'industry',\n",
       " 'around',\n",
       " 'jack',\n",
       " 'established',\n",
       " 'asia',\n",
       " 'better',\n",
       " 'them',\n",
       " 'pop',\n",
       " '1977',\n",
       " 'union',\n",
       " 'government',\n",
       " 'german',\n",
       " 'taylor',\n",
       " 'old',\n",
       " 'wife',\n",
       " 'modern',\n",
       " 'tony',\n",
       " 'southern',\n",
       " 'guitarist',\n",
       " 'own',\n",
       " '1972',\n",
       " 'basketball',\n",
       " 'germany',\n",
       " 'wrestling',\n",
       " 'th',\n",
       " 'ice',\n",
       " 'simply',\n",
       " '1979',\n",
       " 'wwe',\n",
       " 'steve',\n",
       " 'recorded',\n",
       " 'henry',\n",
       " 'fire',\n",
       " 'ten',\n",
       " 'followed',\n",
       " 'center',\n",
       " 'broadcast',\n",
       " 'sports',\n",
       " 'currently',\n",
       " '1983',\n",
       " 'total',\n",
       " 'professionally',\n",
       " 'episode',\n",
       " 'white',\n",
       " 'art',\n",
       " 'least',\n",
       " 'spanish',\n",
       " 'alongside',\n",
       " '1990s',\n",
       " 'ocean',\n",
       " 'biographical',\n",
       " 'back',\n",
       " '1973',\n",
       " 'period',\n",
       " 'artists',\n",
       " 'development',\n",
       " 'thrones',\n",
       " 'c',\n",
       " 'took',\n",
       " '200',\n",
       " '1967',\n",
       " 'primarily',\n",
       " 'within',\n",
       " 'performed',\n",
       " 'footballer',\n",
       " 'performances',\n",
       " '1963',\n",
       " 'm',\n",
       " 'sea',\n",
       " 'stone',\n",
       " 'long',\n",
       " '3d',\n",
       " 'now',\n",
       " 'doctor',\n",
       " 'becoming',\n",
       " 'son',\n",
       " 'another',\n",
       " 'boston',\n",
       " 'kevin',\n",
       " 'batman',\n",
       " 'green',\n",
       " 'street',\n",
       " 'father',\n",
       " 'days',\n",
       " 'public',\n",
       " 'large',\n",
       " 'copies',\n",
       " 'hbo',\n",
       " 'radio',\n",
       " 'blue',\n",
       " '1978',\n",
       " 'singersongwriter',\n",
       " 'andrew',\n",
       " 'continued',\n",
       " 'sixth',\n",
       " 'australia',\n",
       " 'trilogy',\n",
       " 'do',\n",
       " 'lost',\n",
       " 'activist',\n",
       " 'law',\n",
       " 'general',\n",
       " '1970',\n",
       " 'river',\n",
       " '1962',\n",
       " 'abc',\n",
       " 'bbc',\n",
       " 'human',\n",
       " 'college',\n",
       " 'northern',\n",
       " 'mike',\n",
       " 'bestselling',\n",
       " 'label',\n",
       " 'nbc',\n",
       " 'girls',\n",
       " 'dr',\n",
       " 'japan',\n",
       " 'channel',\n",
       " 'gold',\n",
       " 'widely',\n",
       " 'slam',\n",
       " 'economic',\n",
       " 'teen',\n",
       " 'like',\n",
       " 'host',\n",
       " '31',\n",
       " '1960',\n",
       " 'acclaimed',\n",
       " 'tim',\n",
       " 'become',\n",
       " 'smith',\n",
       " 'road',\n",
       " 'power',\n",
       " 'mary',\n",
       " 'francisco',\n",
       " '1968',\n",
       " 'columbia',\n",
       " 'democratic',\n",
       " 'ever',\n",
       " 'chart',\n",
       " 'wilson',\n",
       " 'china',\n",
       " 'royal',\n",
       " 'nine',\n",
       " 'never',\n",
       " 'would',\n",
       " 'howard',\n",
       " 'broadway',\n",
       " 'events',\n",
       " 'summer',\n",
       " 'primetime',\n",
       " 'allen',\n",
       " 'portrayal',\n",
       " 'planet',\n",
       " 'good',\n",
       " 'whose',\n",
       " 'prince',\n",
       " 'joe',\n",
       " 'd',\n",
       " 'sometimes',\n",
       " 'recognition',\n",
       " 'special',\n",
       " 'korean',\n",
       " 'v',\n",
       " 'adventure',\n",
       " 'kennedy',\n",
       " 'jim',\n",
       " 'boys',\n",
       " 'featuring',\n",
       " 'harry',\n",
       " 'throughout',\n",
       " 'patrick',\n",
       " 'although',\n",
       " 'positive',\n",
       " 'players',\n",
       " 'movies',\n",
       " 'leader',\n",
       " 'oscar',\n",
       " 'due',\n",
       " 'choice',\n",
       " 'found',\n",
       " 'championships',\n",
       " 'chicago',\n",
       " 't',\n",
       " 'working',\n",
       " 'african',\n",
       " 'whom',\n",
       " 'nation',\n",
       " 'space',\n",
       " 'winner',\n",
       " 'box',\n",
       " 'global',\n",
       " 'business',\n",
       " 'rights',\n",
       " 'eastern',\n",
       " 'civil',\n",
       " 'middle',\n",
       " 'highestgrossing',\n",
       " 'greek',\n",
       " 'organization',\n",
       " 'brown',\n",
       " '1974',\n",
       " 'nt',\n",
       " 'jon',\n",
       " 'male',\n",
       " 'presidential',\n",
       " 'appearances',\n",
       " 'reached',\n",
       " 'billion',\n",
       " 'next',\n",
       " 'different',\n",
       " 'ireland',\n",
       " 'land',\n",
       " '1964',\n",
       " 'annual',\n",
       " 'douglas',\n",
       " 'beginning',\n",
       " 'left',\n",
       " 'water',\n",
       " 'what',\n",
       " 'paris',\n",
       " 'super',\n",
       " 'little',\n",
       " 'given',\n",
       " 'iii',\n",
       " 'town',\n",
       " 'formerly',\n",
       " 'raised',\n",
       " 'magazine',\n",
       " 'lady',\n",
       " 'opera',\n",
       " 'started',\n",
       " 'era',\n",
       " 'inc',\n",
       " 'boy',\n",
       " 'consecutive',\n",
       " 'writing',\n",
       " 'executive',\n",
       " '1980s',\n",
       " 'dead',\n",
       " 'miniseries',\n",
       " 'multiple',\n",
       " 'cowritten',\n",
       " 'shows',\n",
       " 'air',\n",
       " 'animation',\n",
       " 'acted',\n",
       " 'f',\n",
       " 'b',\n",
       " 'form',\n",
       " 'adam',\n",
       " 'texas',\n",
       " 'romance',\n",
       " 'jennifer',\n",
       " 'dancer',\n",
       " '1966',\n",
       " 'islands',\n",
       " 'irish',\n",
       " 'drummer',\n",
       " 'ben',\n",
       " 'ford',\n",
       " 'brian',\n",
       " 'involved',\n",
       " 'composed',\n",
       " 'just',\n",
       " 'carter',\n",
       " 'debuted',\n",
       " 'federal',\n",
       " 'ryan',\n",
       " 'nations',\n",
       " 'matt',\n",
       " 'achieved',\n",
       " 'stephen',\n",
       " 'spain',\n",
       " 'russell',\n",
       " 'real',\n",
       " 'bob',\n",
       " 'awarded',\n",
       " 'mother',\n",
       " 'fashion',\n",
       " 'anne',\n",
       " 'notable',\n",
       " 'order',\n",
       " 'st',\n",
       " '1970s',\n",
       " 'governor',\n",
       " 'japanese',\n",
       " 'others',\n",
       " 'reality',\n",
       " 'francis',\n",
       " 'highly',\n",
       " 'way',\n",
       " 'seventh',\n",
       " 'baseball',\n",
       " 'date',\n",
       " 'version',\n",
       " 'founder',\n",
       " '20th',\n",
       " 'critically',\n",
       " 'knight',\n",
       " 'line',\n",
       " 'shakespeare',\n",
       " 'act',\n",
       " 'olympic',\n",
       " 'these',\n",
       " 'vocalist',\n",
       " 'bay',\n",
       " 'however',\n",
       " 'military',\n",
       " 'filmfare',\n",
       " 'friends',\n",
       " '1953',\n",
       " 'previously',\n",
       " 'alan',\n",
       " 'down',\n",
       " 'hosted',\n",
       " 'grossed',\n",
       " 'directorial',\n",
       " 'tournament',\n",
       " 'roman',\n",
       " 'battle',\n",
       " 'anthony',\n",
       " 'novelist',\n",
       " 'mystery',\n",
       " '1960s',\n",
       " 'use',\n",
       " 'mr',\n",
       " 'service',\n",
       " 'eighth',\n",
       " 'coast',\n",
       " 'style',\n",
       " 'sport',\n",
       " 'distributed',\n",
       " 'culture',\n",
       " 'johnson',\n",
       " 'earth',\n",
       " 'event',\n",
       " 'louis',\n",
       " 'daniel',\n",
       " '1954',\n",
       " 'sister',\n",
       " 'miller',\n",
       " 'term',\n",
       " 'education',\n",
       " 'further',\n",
       " 'beauty',\n",
       " 'mixed',\n",
       " 'range',\n",
       " 'run',\n",
       " 'dance',\n",
       " 'psychological',\n",
       " 'daughter',\n",
       " '1955',\n",
       " 'subsequently',\n",
       " 'fx',\n",
       " 'personality',\n",
       " 'davis',\n",
       " 'lord',\n",
       " 'mtv',\n",
       " 'joined',\n",
       " 'half',\n",
       " 'take',\n",
       " 'ensemble',\n",
       " 'upon',\n",
       " 'punk',\n",
       " 'christian',\n",
       " 'grant',\n",
       " 'atlantic',\n",
       " 'saint',\n",
       " 'finals',\n",
       " 'small',\n",
       " 'much',\n",
       " 'according',\n",
       " 'army',\n",
       " 'sarah',\n",
       " 'lion',\n",
       " 'competition',\n",
       " 'potter',\n",
       " 'cable',\n",
       " 'religion',\n",
       " 'harris',\n",
       " 'force',\n",
       " 'ep',\n",
       " 'project',\n",
       " 'chinese',\n",
       " 'though',\n",
       " 'mountain',\n",
       " 'justice',\n",
       " 'filmmaker',\n",
       " 'addition',\n",
       " 'sales',\n",
       " 'heavyweight',\n",
       " '1961',\n",
       " 'guest',\n",
       " 'kapoor',\n",
       " 'cultural',\n",
       " 'italy',\n",
       " 'initially',\n",
       " 'field',\n",
       " 'uefa',\n",
       " 'marie',\n",
       " 'roll',\n",
       " 'future',\n",
       " 'titled',\n",
       " 'how',\n",
       " 'moore',\n",
       " 'wrestler',\n",
       " 'design',\n",
       " 'captain',\n",
       " 'beatles',\n",
       " 'driver',\n",
       " 'together',\n",
       " 'sam',\n",
       " 'legal',\n",
       " 'economy',\n",
       " 'again',\n",
       " '1945',\n",
       " 'prize',\n",
       " 'jeff',\n",
       " 'earning',\n",
       " 'program',\n",
       " 'influential',\n",
       " 'sovereign',\n",
       " 'credited',\n",
       " 'charlie',\n",
       " 'guitar',\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: (2,), types: tf.string>\n",
      "<MapDataset shapes: (3,), types: tf.int32>\n",
      "<BatchDataset shapes: (((32,), (32,)), (32, 3)), types: ((tf.string, tf.string), tf.int32)>\n",
      "((TensorSpec(shape=(32,), dtype=tf.string, name=None), TensorSpec(shape=(32,), dtype=tf.string, name=None)), TensorSpec(shape=(32, 3), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 60\n",
    "BUFFER_SIZE = 32000\n",
    "\n",
    "hypothesis = ds_train.map(lambda x, y: x[0])\n",
    "evidence = ds_train.map(lambda x, y: x[1])\n",
    "labels = ds_train.map(lambda x, y: y)\n",
    "print(data)\n",
    "print(labels)\n",
    "features = tf.data.Dataset.zip((hypothesis,evidence))\n",
    "d = tf.data.Dataset.zip((features,labels))\n",
    "dataset_train = d.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_train)\n",
    "print(dataset_train.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: (2,), types: tf.string>\n",
      "<MapDataset shapes: (3,), types: tf.int32>\n",
      "<BatchDataset shapes: (((32,), (32,)), (32, 3)), types: ((tf.string, tf.string), tf.int32)>\n",
      "((TensorSpec(shape=(32,), dtype=tf.string, name=None), TensorSpec(shape=(32,), dtype=tf.string, name=None)), TensorSpec(shape=(32, 3), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 60\n",
    "BUFFER_SIZE = 32000\n",
    "\n",
    "hypothesis = ds_dev.map(lambda x, y: x[0])\n",
    "evidence = ds_dev.map(lambda x, y: x[1])\n",
    "labels = ds_dev.map(lambda x, y: y)\n",
    "print(data)\n",
    "print(labels)\n",
    "features = tf.data.Dataset.zip((hypothesis,evidence))\n",
    "d = tf.data.Dataset.zip((features,labels))\n",
    "dataset_dev = d.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_dev)\n",
    "print(dataset_dev.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[b'[START] the flash aired in the nineties . [END]'\n",
      "  b'The Flash is a 1990 American television series developed by the writing team of Danny Bilson and Paul De Meo that aired on CBS . The Flash is a 1990 American television series developed by the writing team of Danny Bilson and Paul De Meo that aired on CBS . The Flash is a 1990 American television series developed by the writing team of Danny Bilson and Paul De Meo that aired on CBS . The Flash is a 1990 American television series developed by the writing team of Danny Bilson and Paul De Meo that aired on CBS . The Flash is a 1990 American television series developed by the writing team of Danny Bilson and Paul De Meo that aired on CBS .']\n",
      " [b'[START] winter passing had mixed reviews . [END]'\n",
      "  b'The film premiered in 2005 to mixed reviews , and was not released in the United Kingdom until 2013 , when it was released under the new title Happy Endings .']\n",
      " [b'[START] m . s . reddy produced ramayanam . [END]'\n",
      "  b'Ramayanam is a 1996 mythological Telugu film directed by Gunasekhar and produced by M. S. Reddy .']\n",
      " [b'[START] steven knight writes and cooks . [END]'\n",
      "  b'Stephen Knight -LRB- poet -RRB- -LRB- born 1960 -RRB- , Welsh writer Steven Knight -LRB- born 1959 -RRB- , British writer and co-creator of Who Wants to Be a Millionaire ?']\n",
      " [b'[START] robert browning had mastery of epic poetry . [END]'\n",
      "  b'The collection Dramatis Personae and the book-length epic poem The Ring and the Book followed , and made him a leading British poet .']\n",
      " [b'[START] charles i of england was a race car driver . [END]'\n",
      "  b'Joachim Sauter -LRB- born 1959 -RRB- , German media artist and designer']\n",
      " [b'[START] christian bale has only been featured in comedy films . [END]'\n",
      "  b\"Bale first caught the public eye at the age of 13 , when he was cast in the starring role of Steven Spielberg 's Empire of the Sun -LRB- 1987 -RRB- . Empire of the Sun is a 1987 American epic coming-of-age war film based on J. G. Ballard 's semi-autobiographical novel of the same name . Bale went on to receive greater commercial recognition for his starring role as Batman in Christopher Nolan 's Batman Begins -LRB- 2005 -RRB- , The Dark Knight -LRB- 2008 -RRB- and The Dark Knight Rises -LRB- 2012 -RRB- . The Dark Knight Rises is a 2012 British-American superhero film directed by Christopher Nolan , who co-wrote the screenplay with his brother Jonathan Nolan , and the story with David S. Goyer . His portrayal of Dicky Eklund in the David O. Russell-directed biographical film The Fighter -LRB- 2010 -RRB- , earned him critical acclaim and a number of awards , including the Academy Award for Best Supporting Actor . The Fighter is a 2010 American biographical sports drama film directed by David O. Russell , and starring Mark Wahlberg , Christian Bale , Amy Adams and Melissa Leo .\"]\n",
      " [b'[START] cowboy was performed during the super bowl xxxviii halftime show . [END]'\n",
      "  b'The song was performed during the Super Bowl XXXVIII halftime show in 2004 .']], shape=(8, 2), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for d in dataset_train.take(1):\n",
    "    print(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "evidence (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hypothesis (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization_12 (TextVect (None, 5000)         0           hypothesis[0][0]                 \n",
      "                                                                 evidence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization_11 (TextVect (None, 5000)         0           hypothesis[0][0]                 \n",
      "                                                                 evidence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_16 (Dot)                    (None, 1)            0           text_vectorization_12[34][0]     \n",
      "                                                                 text_vectorization_12[35][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 10001)        0           text_vectorization_11[35][0]     \n",
      "                                                                 text_vectorization_11[34][0]     \n",
      "                                                                 dot_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 100)          1000200     concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 100)          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 3)            303         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,000,503\n",
      "Trainable params: 1,000,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "4545/4545 [==============================] - 172s 31ms/step - loss: 0.6558 - accuracy: 0.7328 - val_loss: 0.8126 - val_accuracy: 0.6318\n",
      "Epoch 2/10\n",
      "4545/4545 [==============================] - 172s 31ms/step - loss: 0.5242 - accuracy: 0.7940 - val_loss: 0.8922 - val_accuracy: 0.6258\n",
      "Epoch 3/10\n",
      "4545/4545 [==============================] - 172s 31ms/step - loss: 0.4771 - accuracy: 0.8134 - val_loss: 0.9190 - val_accuracy: 0.6378\n",
      "Epoch 4/10\n",
      "4545/4545 [==============================] - 171s 31ms/step - loss: 0.4474 - accuracy: 0.8248 - val_loss: 1.0011 - val_accuracy: 0.6339\n",
      "Epoch 5/10\n",
      "4545/4545 [==============================] - 171s 31ms/step - loss: 0.4248 - accuracy: 0.8341 - val_loss: 1.0052 - val_accuracy: 0.6358\n",
      "Epoch 6/10\n",
      "4545/4545 [==============================] - 171s 31ms/step - loss: 0.4038 - accuracy: 0.8424 - val_loss: 1.0677 - val_accuracy: 0.6372\n",
      "Epoch 7/10\n",
      "  38/4545 [..............................] - ETA: 2:43 - loss: 0.3658 - accuracy: 0.8586"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "inp1 = keras.Input(shape=(None, ), dtype=tf.string, name = \"hypothesis\")\n",
    "inp2 = keras.Input(shape=(None, ), dtype=tf.string, name = \"evidence\")\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "claim_tfs = freq_vectorizer(inp1)\n",
    "body_tfs = freq_vectorizer(inp2)\n",
    "claim_tfidf = tfidf_vectorizer(inp1)\n",
    "body_tfidf = tfidf_vectorizer(inp2)\n",
    "\n",
    "cosine_layer = keras.layers.Dot((1,1), normalize=True)\n",
    "cosine_similarity = cosine_layer((claim_tfidf, body_tfidf))\n",
    "\n",
    "w = keras.layers.concatenate([body_tfs, claim_tfs, cosine_similarity], axis = 1)\n",
    "\n",
    "x1 = keras.layers.Dense(100, activation='relu')(w)\n",
    "x2 = keras.layers.Dropout(0.4)(x1)\n",
    "x3 = keras.layers.Dense(3, activation='softmax')(x2)\n",
    "model = keras.Model([inp1, inp2], x3)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=lr), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "checkpoint_filepath = 'working/data/training/baseline/checkpoint_mlp'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "# Train. Do not specify batch size because the dataset takes care of that.\n",
    "history = model.fit(dataset_train, epochs=10, callbacks=[stop_early], validation_data=dataset_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the dataset generator to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    <ipython-input-46-6aca85572743>:30 None  *\n        lambda x, y: transform(x))\n    <ipython-input-47-254a20fa8d48>:20 transform  *\n        claim_bow = bow_vectorizer.transform(text[0])\n\n    AttributeError: 'TextVectorization' object has no attribute 'transform'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-254a20fa8d48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbody_tfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclaim_tfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcosines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1923\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   1924\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 1925\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1926\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4485\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4486\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4487\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4488\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   4489\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3710\u001b[0m     \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3712\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3713\u001b[0m       \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3714\u001b[0m       \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3133\u001b[0m     \"\"\"\n\u001b[1;32m   3134\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3135\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3136\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3098\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3100\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3101\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3685\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3686\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3687\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3688\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3689\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3615\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3617\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3618\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3619\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    <ipython-input-46-6aca85572743>:30 None  *\n        lambda x, y: transform(x))\n    <ipython-input-47-254a20fa8d48>:20 transform  *\n        claim_bow = bow_vectorizer.transform(text[0])\n\n    AttributeError: 'TextVectorization' object has no attribute 'transform'\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 60\n",
    "BUFFER_SIZE = 32000\n",
    "\n",
    "# claim_bow = bow_vectorizer.transform(train_claims)\n",
    "# claim_tfs = tfreq_vectorizer.transform(claim_bow)\n",
    "# claim_tfidf = tfidf_vectorizer.transform(train_claims)\n",
    "\n",
    "# #get the text from the bodies of all the n-closest docs for the claim\n",
    "# #body_texts = texts(data)\n",
    "# body_bow = bow_vectorizer.transform(train_bodies)\n",
    "# body_tfs = tfreq_vectorizer.transform(body_bow)\n",
    "# body_tfidf = tfidf_vectorizer.transform(train_bodies)\n",
    "\n",
    "# cosines = np.array([cosine_similarity(c, b)[0] for c,b in zip(claim_tfidf,body_tfidf)])\n",
    "\n",
    "# return hstack([body_tfs,claim_tfs,cosines])\n",
    "\n",
    "def transform(text):\n",
    "    claim_bow = bow_vectorizer.transform(text[0])\n",
    "    claim_tfs = freq_vectorizer.transform(text[0])\n",
    "    claim_tfidf = tfidf_vectorizer.transform(text[0])\n",
    "    body_bow = bow_vectorizer.transform(text[1])\n",
    "    body_tfs = freq_vectorizer.transform(text[1])\n",
    "    body_tfidf = tfidf_vectorizer.transform(text[1])\n",
    "    cosines = np.array([cosine_similarity(c, b)[0] for c,b in zip(claim_tfidf,body_tfidf)])\n",
    "    \n",
    "    return hstack([body_tfs,claim_tfs,cosines])\n",
    "\n",
    "data = ds_train.map(lambda x, y: transform(x))\n",
    "labels = ds_train.map(lambda x, y: y)\n",
    "print(data)\n",
    "print(labels)\n",
    "d = tf.data.Dataset.zip((data,labels))\n",
    "dataset_train = d.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_train)\n",
    "print(dataset_train.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "claims = train_claims\n",
    "bodies = train_bodies\n",
    "dev_claims = dev_claims\n",
    "dev_bodies = dev_bodies\n",
    "\n",
    "lim_unigram = 5000\n",
    "stop_words = [\n",
    "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
    "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
    "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
    "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
    "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
    "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
    "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
    "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
    "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
    "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
    "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
    "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
    "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
    "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
    "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
    "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
    "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "        ]\n",
    "bow_vectorizer = CountVectorizer(max_features=lim_unigram,\n",
    "                                         stop_words=stop_words)\n",
    "bow = bow_vectorizer.fit_transform(claims + bodies)\n",
    "tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram,\n",
    "                                           stop_words=stop_words).fit(claims + bodies + dev_claims + dev_bodies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizers will be saved in a folder in the directory 'ns_nn_sent' so that it can be looked up later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p working/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_encoder_train.pkl  paper_dev.ns.pages.p5.jsonl  train.ns.pages.p5.jsonl\n"
     ]
    }
   ],
   "source": [
    "ls working/data/training/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('working/data/training/baseline/train_labels.pkl'), \"wb+\") as f:\n",
    "    pickle.dump(train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('working/data/training/baseline/dev_labels.pkl'), \"wb+\") as f:\n",
    "    pickle.dump(dev_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_file = 'working/data/training/baseline/train_labels.pkl'\n",
    "if os.path.exists(train_labels_file):\n",
    "    with open(os.path.join(train_labels_file), \"rb\") as f:\n",
    "                train_labels = pickle.load(f)\n",
    "else:\n",
    "    print(\"Saved file not found, processing again...\")\n",
    "    train_labels = []\n",
    "    for d, l in ds_train.batch(1):\n",
    "        train_labels.append(l.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_labels_file = 'working/data/training/baseline/dev_labels.pkl'\n",
    "if os.path.exists(dev_labels_file):\n",
    "    with open(os.path.join(dev_labels_file), \"rb\") as f:\n",
    "                dev_labels = pickle.load(f)\n",
    "else:\n",
    "    print(\"Saved file not found, processing again...\")\n",
    "    dev_labels = []\n",
    "    for d, l in ds_dev.batch(1):\n",
    "        dev_labels.append(l.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.46857671]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats[0].tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the claims and the body texts using the vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import hstack\n",
    "def process_train():\n",
    "    claim_bow = bow_vectorizer.transform(train_claims)\n",
    "    claim_tfs = tfreq_vectorizer.transform(claim_bow)\n",
    "    claim_tfidf = tfidf_vectorizer.transform(train_claims)\n",
    "\n",
    "    #get the text from the bodies of all the n-closest docs for the claim\n",
    "    #body_texts = texts(data)\n",
    "    body_bow = bow_vectorizer.transform(train_bodies)\n",
    "    body_tfs = tfreq_vectorizer.transform(body_bow)\n",
    "    body_tfidf = tfidf_vectorizer.transform(train_bodies)\n",
    "\n",
    "    cosines = np.array([cosine_similarity(c, b)[0] for c,b in zip(claim_tfidf,body_tfidf)])\n",
    "\n",
    "    return hstack([body_tfs,claim_tfs,cosines])\n",
    "\n",
    "def process_dev():\n",
    "    claim_bow = bow_vectorizer.transform(dev_claims)\n",
    "    claim_tfs = tfreq_vectorizer.transform(claim_bow)\n",
    "    claim_tfidf = tfidf_vectorizer.transform(dev_claims)\n",
    "\n",
    "    #get the text from the bodies of all the n-closest docs for the claim\n",
    "    #body_texts = texts(data)\n",
    "    body_bow = bow_vectorizer.transform(dev_bodies)\n",
    "    body_tfs = tfreq_vectorizer.transform(body_bow)\n",
    "    body_tfidf = tfidf_vectorizer.transform(dev_bodies)\n",
    "\n",
    "    cosines = np.array([cosine_similarity(c, b)[0] for c,b in zip(claim_tfidf,body_tfidf)])\n",
    "\n",
    "    return hstack([body_tfs,claim_tfs,cosines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "model_name = 'ns_nn_sent'\n",
    "base_path = 'working/models/'\n",
    "\n",
    "def load_features(name):\n",
    "    features = list()\n",
    "    ffpath = os.path.join(base_path, model_name)\n",
    "    if not os.path.exists(ffpath):\n",
    "        os.mkdir(ffpath)\n",
    "    if (not os.path.exists(os.path.join(ffpath, name + \".pkl\"))):\n",
    "        print(\"Saved features do not exist, creating data...\")\n",
    "        if name == 'train':\n",
    "            features = process_train()\n",
    "        else:\n",
    "            features = process_dev()\n",
    "        with open(os.path.join(ffpath, name + \".pkl\"), \"wb+\") as f:\n",
    "            pickle.dump(features, f)\n",
    "    else:\n",
    "        print(\"Loading saved feature from {}\".format(os.path.join(ffpath, name + \".pkl\")))\n",
    "        with open(os.path.join(ffpath, name + \".pkl\"), \"rb\") as f:\n",
    "            features = pickle.load(f)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the labels for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out(features,labels):\n",
    "    if features is not None:\n",
    "        return np.hstack(features) if len(features) > 1 else features[0], labels\n",
    "    return [[]],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_name = \"label\"\n",
    "# def labels(data):\n",
    "#     return [datum[label_name] for datum in data]\n",
    "# def out(features,ds):\n",
    "#     if ds is not None:\n",
    "#         return np.hstack(features) if len(features) > 1 else features[0], labels(ds)\n",
    "#     return [[]],[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs to be performed once per dataset. Therefore, we would save the transformed vectors in a file to reuse for each modelling excercise.\n",
    "\n",
    "Check if the saved vectors exist, if not, create them by using the vectorizers and applying a transform on the \n",
    "- claim\n",
    "- lines from the body of the evidence pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved feature from working/models/ns_nn_sent/train.pkl\n"
     ]
    }
   ],
   "source": [
    "train_fs = []\n",
    "train_features = load_features(\"train\")\n",
    "train_fs.append(train_features)\n",
    "#train_features = sparse.csr_matrix(train_features)\n",
    "train_feats = out(train_fs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape = 10001\n"
     ]
    }
   ],
   "source": [
    "input_shape = train_feats[0].shape[1]\n",
    "print(\"input_shape =\", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved feature from working/models/ns_nn_sent/dev.pkl\n"
     ]
    }
   ],
   "source": [
    "dev_fs = []\n",
    "dev_features = load_features(\"dev\")\n",
    "dev_fs.append(dev_features)\n",
    "#dev_features = sparse.csr_matrix(dev_features)\n",
    "dev_feats = out(dev_fs, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<9999x10001 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 207568 stored elements in COOrdinate format>,\n",
       " [array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([1, 0, 0], dtype=int32),\n",
       "  array([0, 1, 0], dtype=int32),\n",
       "  array([0, 0, 1], dtype=int32),\n",
       "  ...])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_feats[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape = 10001\n"
     ]
    }
   ],
   "source": [
    "input_shape = dev_feats[0].shape[1]\n",
    "print(\"input_shape =\", input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (using PyTorch)\n",
    "It's now time to build the model. We will build a Simple Multi layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,keep_p=.6):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "        self.do = nn.Dropout(1-keep_p)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.do(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.do(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleMLP(\n",
       "  (fc1): Linear(in_features=10001, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (do): Dropout(p=0.4, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleMLP(input_shape,100,3)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up any saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rm -rf working/models/ns_nn_sent/ns_nn_sent.best.save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Define the logger, the one that will be used to monitor the model training progress\n",
    "\n",
    "The best model will be saved at \n",
    "> working/models/ns_nn_sent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "class LogHelper():\n",
    "    handler = None\n",
    "    @staticmethod\n",
    "    def setup():\n",
    "        FORMAT = '[%(levelname)s] %(asctime)s - %(name)s - %(message)s'\n",
    "        LogHelper.handler = logging.StreamHandler()\n",
    "        LogHelper.handler.setLevel(logging.DEBUG)\n",
    "        LogHelper.handler.setFormatter(logging.Formatter(FORMAT))\n",
    "\n",
    "        LogHelper.get_logger(LogHelper.__name__).info(\"Log Helper set up\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_logger(name,level=logging.DEBUG):\n",
    "        ##note: once a logger is created, repeated calls using the same name will give you the same logger object\n",
    "        l = logging.getLogger(name)\n",
    "        sh = logging.StreamHandler()\n",
    "        l.setLevel(level)\n",
    "        l.addHandler(sh)\n",
    "        return l\n",
    "    \n",
    "class EarlyStopping():\n",
    "    def __init__(self,name,patience=8):\n",
    "        self.patience = patience\n",
    "        self.best_model = None\n",
    "        self.best_score = None\n",
    "\n",
    "        self.best_epoch = 0\n",
    "        self.epoch = 0\n",
    "        #print(\"name is \", EarlyStopping.__name__)\n",
    "        self.name = name\n",
    "        #self.logger = LogHelper.get_logger(EarlyStopping.__name__)\n",
    "        self.logger = LogHelper.get_logger(name)\n",
    "\n",
    "    def __call__(self, model, acc):\n",
    "        self.epoch += 1\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = acc\n",
    "\n",
    "        if acc >= self.best_score:\n",
    "            torch.save(model.state_dict(),\"working/models/ns_nn_sent/{0}.best.save\".format(self.name))\n",
    "            self.best_score = acc\n",
    "            self.best_epoch = self.epoch\n",
    "            self.logger.info(\"Saving best weights from round {0}\".format(self.epoch))\n",
    "            return False\n",
    "\n",
    "        elif self.epoch > self.best_epoch+self.patience:\n",
    "            self.logger.info(\"Early stopping: Terminate\")\n",
    "            return True\n",
    "\n",
    "        self.logger.info(\"Early stopping: Worse Round\")\n",
    "        return False\n",
    "\n",
    "    def set_best_state(self,model):\n",
    "        self.logger.info(\"Loading weights from round {0}\".format(self.best_epoch))\n",
    "        model.load_state_dict(torch.load(\"working/models/ns_nn_sent/{0}.best.save\".format(self.name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dataset reader\n",
    "\n",
    "We will need to handle the batching of inputs to our model\n",
    "\n",
    "We will define a batcher that deals with the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "def is_gpu():\n",
    "    return os.getenv(\"GPU\",\"no\").lower() in [\"1\",1,\"yes\",\"true\",\"t\"]\n",
    "\n",
    "def gpu():\n",
    "    if is_gpu():\n",
    "        torch.cuda.set_device(int(os.getenv(\"CUDA_DEVICE\", 0)))\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class Batcher():\n",
    "    def __init__(self,data,size):\n",
    "        self.data = data\n",
    "        self.size = size\n",
    "        self.pointer = 0\n",
    "\n",
    "        if isinstance(self.data,coo_matrix):\n",
    "            self.data = self.data.tocsr()\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.pointer == splen(self.data):\n",
    "            self.pointer = 0\n",
    "            raise StopIteration\n",
    "        next = min(splen(self.data),self.pointer+self.size)\n",
    "        to_return = self.data[self.pointer : next]\n",
    "        start,end = self.pointer,next\n",
    "        self.pointer = next\n",
    "        return to_return, splen(to_return), start, end\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "def splen(data):\n",
    "    try:\n",
    "        return data.shape[0]\n",
    "    except:\n",
    "        return len(data)\n",
    "\n",
    "def prepare_with_labels(data,labels):\n",
    "    data = data.todense()\n",
    "    v = torch.FloatTensor(np.array(data))\n",
    "    if gpu():\n",
    "        return Variable(v.cuda()), Variable(torch.LongTensor(labels).cuda())\n",
    "    return Variable(v), Variable(torch.LongTensor(labels))\n",
    "\n",
    "\n",
    "def prepare(data):\n",
    "    data = data.todense()\n",
    "    v = torch.FloatTensor(np.array(data))\n",
    "    if gpu():\n",
    "        return Variable(v.cuda())\n",
    "    return Variable(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate(model,data,labels,batch_size):\n",
    "    predicted = predict(model,data,batch_size)\n",
    "    return accuracy_score(labels,predicted.data.numpy().reshape(-1))\n",
    "\n",
    "def predict(model, data, batch_size):\n",
    "    batcher = Batcher(data, batch_size)\n",
    "\n",
    "    predicted = []\n",
    "    for batch, size, start, end in batcher:\n",
    "        d = prepare(batch)\n",
    "        model.eval()\n",
    "        logits = model(d).cpu()\n",
    "\n",
    "        predicted.extend(torch.max(logits, 1)[1])\n",
    "    return torch.stack(predicted)\n",
    "\n",
    "def train(model, fs, batch_size, lr, epochs,dev=None, clip=None, early_stopping=None,name=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    data, labels = fs\n",
    "    if dev is not None:\n",
    "        dev_data,dev_labels = dev\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_loss = 0\n",
    "        epoch_data = 0\n",
    "\n",
    "        shuffle(data,labels)\n",
    "\n",
    "        batcher = Batcher(data, batch_size)\n",
    "\n",
    "        for batch, size, start, end in batcher:\n",
    "            d,gold = prepare_with_labels(batch,labels[start:end])\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(d)\n",
    "\n",
    "            loss = F.cross_entropy(logits, gold)\n",
    "            loss.backward()\n",
    "\n",
    "            epoch_loss += loss.cpu()\n",
    "            epoch_data += size\n",
    "\n",
    "            if clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Average epoch loss: {0}\".format((epoch_loss/epoch_data).data.numpy()))\n",
    "\n",
    "        #print(\"Epoch Train Accuracy {0}\".format(evaluate(model, data, labels, batch_size)))\n",
    "        if dev is not None:\n",
    "            acc = evaluate(model,dev_data,dev_labels,batch_size)\n",
    "            print(\"Epoch Dev Accuracy {0}\".format(acc))\n",
    "\n",
    "            if early_stopping is not None and early_stopping(model,acc):\n",
    "                break\n",
    "\n",
    "    if dev is not None and early_stopping is not None:\n",
    "        early_stopping.set_best_state(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 0.0016196609940379858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving best weights from round 1\n",
      "  1%|          | 1/90 [00:10<15:24, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6278627862786279\n",
      "Average epoch loss: 0.001501814927905798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving best weights from round 2\n",
      "  2%|▏         | 2/90 [00:20<15:02, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6342634263426342\n",
      "Average epoch loss: 0.0014742235653102398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving best weights from round 3\n",
      "  3%|▎         | 3/90 [00:38<18:25, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6388638863886389\n",
      "Average epoch loss: 0.0014582787407562137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      "  4%|▍         | 4/90 [00:48<16:57, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6374637463746374\n",
      "Average epoch loss: 0.0014515924267470837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      "  6%|▌         | 5/90 [01:08<20:14, 14.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6297629762976298\n",
      "Average epoch loss: 0.001442103530280292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving best weights from round 6\n",
      "  7%|▋         | 6/90 [01:29<22:49, 16.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6465646564656465\n",
      "Average epoch loss: 0.0014398741768673062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      "  8%|▊         | 7/90 [01:48<23:28, 16.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6443644364436444\n",
      "Average epoch loss: 0.001430192613042891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      "  9%|▉         | 8/90 [02:08<24:43, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6458645864586459\n",
      "Average epoch loss: 0.0014347969554364681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 10%|█         | 9/90 [02:18<21:03, 15.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6417641764176417\n",
      "Average epoch loss: 0.0014273609267547727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 11%|█         | 10/90 [02:37<22:19, 16.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6418641864186418\n",
      "Average epoch loss: 0.0014319585170596838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 12%|█▏        | 11/90 [02:57<23:14, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6402640264026402\n",
      "Average epoch loss: 0.0014225579798221588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 13%|█▎        | 12/90 [03:07<19:53, 15.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6407640764076408\n",
      "Average epoch loss: 0.0014181816950440407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 14%|█▍        | 13/90 [03:27<21:24, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6378637863786378\n",
      "Average epoch loss: 0.001419287407770753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Worse Round\n",
      " 16%|█▌        | 14/90 [03:37<18:38, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.6383638363836384\n",
      "Average epoch loss: 0.0014201359590515494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping: Terminate\n",
      " 16%|█▌        | 14/90 [03:47<20:35, 16.25s/it]\n",
      "Loading weights from round 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Dev Accuracy 0.634963496349635\n"
     ]
    }
   ],
   "source": [
    "mname = 'ns_nn_sent'\n",
    "final_model = train(model, train_feats, 500, 1e-2, 90, dev_feats, early_stopping=EarlyStopping(mname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> We achieve a dev set performance of 64% </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training using tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Dropout,Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to build a layer similar to the ones we built using torch.\n",
    "\n",
    "`\n",
    "SimpleMLP(\n",
    "  (fc1): Linear(in_features=10001, out_features=100, bias=True)\n",
    "  (fc2): Linear(in_features=100, out_features=3, bias=True)\n",
    "  (do): Dropout(p=0.4, inplace=False)\n",
    "  (relu): ReLU()\n",
    ")\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training dataset = (145449, 10001)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = train_feats\n",
    "print(\"Shape of the training dataset =\", train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'He then played Detective John Amsterdam in the short-lived Fox television series New Amsterdam -LRB- 2008 -RRB- , as well as appearing as Frank Pike in the 2009 Fox television film Virtuality , originally intended as a pilot . The Fox Broadcasting Company -LRB- often shortened to Fox and stylized as FOX -RRB- is an American English language commercial broadcast television network that is owned by the Fox Entertainment Group subsidiary of 21st Century Fox .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for d in ds_train.map(lambda x, y: x).take(1):\n",
    "    print(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(2,), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(3,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(3,), dtype=tf.int32, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = train_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_features.tocsr())\n",
    "train_features = train_features.tocsr().todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = sparse.csr_matrix(train_features).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((train_features, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    <ipython-input-76-b1b99179c462>:25 None  *\n        lambda x, y: process_train(x))\n    <ipython-input-75-407de16f6ff3>:11 process_train  *\n        claim_bow = bow_vectorizer.transform(text[0])\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1255 transform  *\n        _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1113 _count_vocab  *\n        for doc in raw_documents:\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:419 for_stmt\n        iter_, extra_test, body, get_state, set_state, symbol_names, opts)\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:484 _known_len_tf_for_stmt\n        n = py_builtins.len_(iter_)\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/operators/py_builtins.py:249 len_\n        return _tf_tensor_len(s)\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/operators/py_builtins.py:277 _tf_tensor_len\n        'len requires a non-scalar tensor, got one of shape {}'.format(shape))\n\n    ValueError: len requires a non-scalar tensor, got one of shape Tensor(\"Shape:0\", shape=(0,), dtype=int32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-b1b99179c462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbody_tfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclaim_tfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcosines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprocess_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenize_and_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenize_and_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1923\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   1924\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 1925\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1926\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4485\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4486\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4487\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4488\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   4489\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3710\u001b[0m     \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3712\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3713\u001b[0m       \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3714\u001b[0m       \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3133\u001b[0m     \"\"\"\n\u001b[1;32m   3134\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3135\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3136\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3098\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3100\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3101\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3685\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3686\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3687\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3688\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3689\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3615\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3617\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3618\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3619\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-76-b1b99179c462>:25 None  *\n        lambda x, y: process_train(x))\n    <ipython-input-75-407de16f6ff3>:11 process_train  *\n        claim_bow = bow_vectorizer.transform(text[0])\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1255 transform  *\n        _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1113 _count_vocab  *\n        for doc in raw_documents:\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:419 for_stmt\n        iter_, extra_test, body, get_state, set_state, symbol_names, opts)\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:484 _known_len_tf_for_stmt\n        n = py_builtins.len_(iter_)\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/operators/py_builtins.py:249 len_\n        return _tf_tensor_len(s)\n    /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/operators/py_builtins.py:277 _tf_tensor_len\n        'len requires a non-scalar tensor, got one of shape {}'.format(shape))\n\n    ValueError: len requires a non-scalar tensor, got one of shape Tensor(\"Shape:0\", shape=(0,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 60\n",
    "BUFFER_SIZE = 32000\n",
    "\n",
    "\n",
    "\n",
    "print(h)\n",
    "print(e)\n",
    "f = tf.data.Dataset.zip((h,e))\n",
    "d = tf.data.Dataset.zip((f,l))\n",
    "dataset_train = d.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_train)\n",
    "print(dataset_train.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "TypeError: Cannot iterate over a scalar tensor.\nTraceback (most recent call last):\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 247, in __call__\n    return func(device, token, args)\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 135, in __call__\n    ret = self._func(*args)\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-61-3d3327444ca7>\", line 11, in process_data\n    claim_bow = bow_vectorizer.transform(text[0])\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 1255, in transform\n    _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 1113, in _count_vocab\n    for doc in raw_documents:\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 526, in __iter__\n    raise TypeError(\"Cannot iterate over a scalar tensor.\")\n\nTypeError: Cannot iterate over a scalar tensor.\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-28896e6bf1a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2728\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2729\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: TypeError: Cannot iterate over a scalar tensor.\nTraceback (most recent call last):\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 247, in __call__\n    return func(device, token, args)\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 135, in __call__\n    ret = self._func(*args)\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-61-3d3327444ca7>\", line 11, in process_data\n    claim_bow = bow_vectorizer.transform(text[0])\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 1255, in transform\n    _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 1113, in _count_vocab\n    for doc in raw_documents:\n\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 526, in __iter__\n    raise TypeError(\"Cannot iterate over a scalar tensor.\")\n\nTypeError: Cannot iterate over a scalar tensor.\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "for d in dataset_train.take(1):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "Build the model using keras functional API\n",
    "\n",
    "We will need to reshape the labels array so that they have the approriate dimensions for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x is the concatenation of the tf vectors for the claim\n",
    "x = train_x\n",
    "labels_3 = train_labels\n",
    "\n",
    "dim = train_x.shape[1]\n",
    "num_examples = train_x.shape[0]\n",
    "lr = 0.001\n",
    "                \n",
    "# This is tf.data.experimental.AUTOTUNE in older tensorflow.\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def generator_fn(n_samples):\n",
    "    \"\"\"Return a function that takes no arguments and returns a generator.\"\"\"\n",
    "    def generator():\n",
    "        num_batches = num_examples/n_samples\n",
    "        counter = 0\n",
    "        if counter == 0:\n",
    "            idx = np.arange(num_examples)\n",
    "            np.random.shuffle(idx)\n",
    "        \n",
    "        while counter < num_batches:\n",
    "            index_batch = idx[n_samples*counter:n_samples*(counter+1)]\n",
    "            counter += 1\n",
    "            rec = x[index_batch, :].todense()\n",
    "            if len(rec) == n_samples:\n",
    "                yield rec, labels_3[index_batch]\n",
    "        counter = 0\n",
    "\n",
    "    return generator\n",
    "\n",
    "samples = 500\n",
    "#we are handling the batching with the samples, set the batch_size to 1, don't let dataset do any batching, the generator already does\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "\n",
    "# Create dataset.\n",
    "gen = generator_fn(n_samples=samples)\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator=gen, \n",
    "    output_types=(np.float32, np.int32), \n",
    "    output_shapes=((samples, dim), (samples, 3))\n",
    ")\n",
    "\n",
    "#we are handling the batching with the samples, set the batch_size to 1, don't let dataset do any batching, the generator already does\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# Prepare model.\n",
    "\n",
    "inp = keras.Input(shape=(None, dim), sparse=False)\n",
    "x1 = Dense(100, activation='relu')(inp)\n",
    "x2 = Dropout(0.4)(x1)\n",
    "x3 = keras.layers.Dense(3, activation='softmax')(x2)\n",
    "model = keras.Model(inp, x3)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=lr), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "# Train. Do not specify batch size because the dataset takes care of that.\n",
    "model.fit(dataset, epochs=epochs, callbacks=[stop_early], validation_data=(dev_x, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A peek a the reshaped labels:\n",
      "[[1. 1. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 0.]]\n",
      "The datatypes of the training dataset, features=<class 'scipy.sparse.coo.coo_matrix'>, labels=<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.zeros(shape=(len(train_y),3))\n",
    "for idx, val in enumerate(train_y):\n",
    "    train_labels[idx][val]=1\n",
    "print(\"A peek a the reshaped labels:\")\n",
    "print(train_labels[:5])\n",
    "print(\"The datatypes of the training dataset, features={}, labels={}\".format(type(train_x), type(train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with sparse matrix in keras\n",
    "Some useful code is [here](https://stackoverflow.com/questions/37609892/keras-sparse-matrix-issue)\n",
    "\n",
    "The type of sparse matrix we have created is `scipy.sparse.coo.coo_matrix`, we will need to convert it to `scipy.sparse.csr.csr_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datatypes of the training dataset, features=<class 'scipy.sparse.csr.csr_matrix'>, labels=<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "train_x=sparse.csr_matrix(train_x)\n",
    "print(\"The datatypes of the training dataset, features={}, labels={}\".format(type(train_x), type(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A peek a the reshaped dev labels:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_x, dev_y = dev_feats\n",
    "dev_x=sparse.csr_matrix(dev_x)\n",
    "dev_labels = np.zeros(shape=(len(dev_y),3))\n",
    "for idx, val in enumerate(dev_y):\n",
    "    dev_labels[idx][val]=1\n",
    "print(\"A peek a the reshaped dev labels:\")\n",
    "dev_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save these sparse matrices \n",
    "As npz files so that we can continue this training on other system without having to pull in the expensive large fever datasets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_labels.npz   embedding_mappings_300d.npz  test_y_tests.npz\n",
      "dev_x.npz        fever_vocab.txt              train_labels.npz\n",
      "dev_y_preds.npz  \u001b[0m\u001b[01;34mout\u001b[0m/                         \u001b[01;34mtraining\u001b[0m/\n",
      "dev_y_tests.npz  test_y_preds.npz\n"
     ]
    }
   ],
   "source": [
    "ls working/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_file = \"working/data/train_x.npz\"\n",
    "np.savez(train_x_file, train_x)\n",
    "train_lbl_file = \"working/data/train_labels.npz\"\n",
    "np.savez(train_lbl_file, train_labels)\n",
    "\n",
    "\n",
    "dev_x_file = \"working/data/dev_x.npz\"\n",
    "np.savez(dev_x_file, dev_x)\n",
    "dev_lbl_file = \"working/data/dev_labels.npz\"\n",
    "np.savez(dev_lbl_file, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 45M\n",
      "-rw-r--r-- 1 root root 235K Jul 12 12:49 dev_labels.npz\n",
      "-rw-r--r-- 1 root root 2.3M Jul 12 12:49 dev_x.npz\n",
      "-rw-r--r-- 1 root root 3.4M Jul 12 12:49 train_labels.npz\n",
      "-rw-r--r-- 1 root root  39M Jul 12 12:49 train_x.npz\n",
      "-rw-r--r-- 1 root root    0 Jul  5 07:26 matching_page_sentences.jsonl\n",
      "drwxr-xr-x 5 root root  160 Jul  5 07:26 \u001b[0m\u001b[01;34mtraining\u001b[0m/\n",
      "-rw-r--r-- 1 root root    0 Jul  5 07:26 claim_texts.jsonl\n"
     ]
    }
   ],
   "source": [
    "ls -lth working/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use batching via data generators\n",
    "\n",
    "We did not use data generators for training. Let's use data generators that will feed data to out training.\n",
    "\n",
    "We will need to write a custom generator here since we are using scipy spare matrix and not tensors.\n",
    "\n",
    "The generator will be called repeatedly by the trainer (model.fit) and each time it is called, we will need to return it a set of data from the dataset.\n",
    "\n",
    "The batch size will be controller by the caller, i.e. the trainer, but we will need to keep track of the records we are sending back so that we know when to reset and loop over.\n",
    "\n",
    "The generator must be iterable and would keep a track of the number of batches we will need to create and track the records we are sending.\n",
    "\n",
    "If the number of records is not perfectly divisible by batch_size, we will run into issues with the generator. For now, we will deal with it by dropping the last set of records if there are fewer than batch_size records in the last batch.\n",
    "\n",
    "Since we are dealing with scipy sparse matrix, we would not be able to send in the data as argument to the generator. We will therefore hardcode the values in the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 10001)]     0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 100)         1000200   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 3)           303       \n",
      "=================================================================\n",
      "Total params: 1,000,503\n",
      "Trainable params: 1,000,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "290/290 [==============================] - 11s 34ms/step - loss: 1.8831 - accuracy: 0.4416 - val_loss: 1.3960 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "290/290 [==============================] - 10s 34ms/step - loss: 3.7363 - accuracy: 0.4730 - val_loss: 1.3941 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 6.0169 - accuracy: 0.4781 - val_loss: 1.4533 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "290/290 [==============================] - 10s 36ms/step - loss: 8.2871 - accuracy: 0.4784 - val_loss: 1.4823 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "184/290 [==================>...........] - ETA: 3s - loss: 10.0913 - accuracy: 0.4773"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fe32bb7352c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Train. Do not specify batch size because the dataset takes care of that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_early\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#train_x is the concatenation of the tf vectors for the claim\n",
    "x = train_x\n",
    "labels_3 = train_labels\n",
    "\n",
    "dim = train_x.shape[1]\n",
    "num_examples = train_x.shape[0]\n",
    "lr = 0.001\n",
    "                \n",
    "# This is tf.data.experimental.AUTOTUNE in older tensorflow.\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def generator_fn(n_samples):\n",
    "    \"\"\"Return a function that takes no arguments and returns a generator.\"\"\"\n",
    "    def generator():\n",
    "        num_batches = num_examples/n_samples\n",
    "        counter = 0\n",
    "        if counter == 0:\n",
    "            idx = np.arange(num_examples)\n",
    "            np.random.shuffle(idx)\n",
    "        \n",
    "        while counter < num_batches:\n",
    "            index_batch = idx[n_samples*counter:n_samples*(counter+1)]\n",
    "            counter += 1\n",
    "            rec = x[index_batch, :].todense()\n",
    "            if len(rec) == n_samples:\n",
    "                yield rec, labels_3[index_batch]\n",
    "        counter = 0\n",
    "\n",
    "    return generator\n",
    "\n",
    "samples = 500\n",
    "#we are handling the batching with the samples, set the batch_size to 1, don't let dataset do any batching, the generator already does\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "\n",
    "# Create dataset.\n",
    "gen = generator_fn(n_samples=samples)\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator=gen, \n",
    "    output_types=(np.float32, np.int32), \n",
    "    output_shapes=((samples, dim), (samples, 3))\n",
    ")\n",
    "\n",
    "#we are handling the batching with the samples, set the batch_size to 1, don't let dataset do any batching, the generator already does\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# Prepare model.\n",
    "\n",
    "inp = keras.Input(shape=(None, dim), sparse=False)\n",
    "x1 = Dense(100, activation='relu')(inp)\n",
    "x2 = Dropout(0.4)(x1)\n",
    "x3 = keras.layers.Dense(3, activation='softmax')(x2)\n",
    "model = keras.Model(inp, x3)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=lr), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "# Train. Do not specify batch size because the dataset takes care of that.\n",
    "model.fit(dataset, epochs=epochs, callbacks=[stop_early], validation_data=(dev_x, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6488648653030396"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, dev_acc = model.evaluate(dev_x, dev_labels, verbose=0)\n",
    "dev_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBSOLETE: do not refer\n",
    "#### NLI model\n",
    "\n",
    "Now that we have a baseline model, we will try an NLI model to see if we can improve on the benchmark we have just set.\n",
    "\n",
    "First, we would have to build our data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company .',\n",
       "  'evidence': [('Nikolaj_Coster-Waldau', 7), ('Fox_Broadcasting_Company', 0)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'},\n",
       " {'claim': 'Roman Atwood is a content creator .',\n",
       "  'evidence': [('Roman_Atwood', 1), ('Roman_Atwood', 3)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'},\n",
       " {'claim': 'History of art includes architecture , dance , sculpture , music , painting , poetry literature , theatre , narrative , film , photography and graphic arts .',\n",
       "  'evidence': [('History_of_art', 2)],\n",
       "  'label': 0,\n",
       "  'label_text': 'SUPPORTS'},\n",
       " {'claim': 'Adrienne Bailon is an accountant .',\n",
       "  'evidence': [('Adrienne_Bailon', 0)],\n",
       "  'label': 1,\n",
       "  'label_text': 'REFUTES'},\n",
       " {'claim': 'System of a Down briefly disbanded in limbo .',\n",
       "  'evidence': [('In_Limbo', -1)],\n",
       "  'label': 2,\n",
       "  'label_text': 'NOT ENOUGH INFO'}]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_formatted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145449, 10001)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145449, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generator():\n",
    "    for data, lbl in zip(x, labels_3):\n",
    "        d = data.todense()\n",
    "        #note:  d is a matrix, this cannot be sent in as a feature to our model to train, we need to reshape this into an array\n",
    "        d = np.asarray(d).reshape(-1)\n",
    "        yield d, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    generator = lambda: get_data_generator()\n",
    "    return tf.data.Dataset.from_generator(\n",
    "            generator, output_signature=(\n",
    "            tf.TensorSpec(shape=(10001, ), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(3, ), dtype=tf.int32,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(10001,), dtype=int32)\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(10001,), dtype=int32)\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(10001,), dtype=int32)\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(10001,), dtype=int32)\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(10001,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for d in get_dataset().take(5):\n",
    "    print(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 3200\n",
    "BATCH_SIZE = 16\n",
    "ds_train = get_dataset()\n",
    "ds_train = ds_train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10001)\n"
     ]
    }
   ],
   "source": [
    "for d in ds_train.take(1):\n",
    "    print(d[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, None, 50)          400050    \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 100)               40400     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 450,853\n",
      "Trainable params: 450,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim = 50\n",
    "vocab_size = 8000\n",
    "inp = keras.Input(shape=(None, ))\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim)\n",
    "\n",
    "x1 = embedding_layer(inp)\n",
    "\n",
    "lstm_layer1 = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim)))(x1)\n",
    "\n",
    "x2 = Dense(100, activation='relu')(lstm_layer1)\n",
    "x3 = Dropout(0.1)(x2)\n",
    "output = keras.layers.Dense(3, activation='softmax')(x3)\n",
    "model = keras.Model(inputs=inp, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=lr), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "      3/Unknown - 46s 14s/step - loss: 1.0982 - accuracy: 0.3333"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-7d7434343b34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_early\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=epochs, callbacks=[stop_early], validation_data=(dev_x, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
