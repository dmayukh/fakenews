{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reported-employer",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "swiss-effects",
   "metadata": {},
   "source": [
    "#### Evaluate the model on test dataset\n",
    "\n",
    "The test dataset will use predicted pages and predicted sentences. \n",
    "\n",
    "The predictions are generated via a seperate process in the our pipeline which must be executed before this step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-three",
   "metadata": {},
   "source": [
    "#### Structure of the test dataset\n",
    "\n",
    "format: \n",
    "- id: id of the claim\n",
    "- label: the text label of the example (e.g. SUPPORTS, REFUTES or NOT ENOUGH INFO)\n",
    "- claim: the claim text\n",
    "- evidence: array of evidence groups\n",
    "- evidence group: [evidence id, N/A, Document Id, evidence tag, [array of closest sentences/lines, array of those line ids in the page]]\n",
    "\n",
    "We will need to read this data, format and extract the evidence, the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "female-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efficient-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.dataset.DatasetReader import DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pointed-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(mda.src.readers.DatasetReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-donor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "occupied-administrator",
   "metadata": {},
   "source": [
    "#### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "billion-suggestion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:01<00:00, 82409.02it/s]\n",
      "100%|██████████| 145449/145449 [00:01<00:00, 138378.30it/s]\n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/training/train.ns.pages.p5.jsonl'\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=None, database_path='data/data/fever/fever.db')\n",
    "raw, data = dsreader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "later-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = dsreader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "applied-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the label encoder\n",
    "import pickle\n",
    "with open('working/data/training/label_encoder_train.pkl', 'wb') as f:\n",
    "    pickle.dump(dsreader.labelencoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "minus-carnival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "udev            7.7G     0  7.7G   0% /dev\n",
      "tmpfs           1.6G  792K  1.6G   1% /run\n",
      "/dev/nvme0n1p1  126G  126G   51M 100% /\n",
      "tmpfs           7.7G  8.0K  7.7G   1% /dev/shm\n",
      "tmpfs           5.0M     0  5.0M   0% /run/lock\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup\n",
      "/dev/loop0       34M   34M     0 100% /snap/amazon-ssm-agent/3552\n",
      "/dev/loop1       56M   56M     0 100% /snap/core18/1988\n",
      "/dev/loop2       33M   33M     0 100% /snap/snapd/12159\n",
      "/dev/loop3       56M   56M     0 100% /snap/core18/2074\n",
      "/dev/loop4       33M   33M     0 100% /snap/snapd/12398\n",
      "tmpfs           1.6G     0  1.6G   0% /run/user/1000\n"
     ]
    }
   ],
   "source": [
    "!df -h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "colored-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf tmp/attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "colonial-priority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_encoder_train.npz\n",
      "label_encoder_train.pkl\n",
      "paper_dev.ns.pages.p5.jsonl\n",
      "paper_dev_pipeline.ps.pages.p5.jsonl\n",
      "paper_dev_predicted_pipeline.ps.pages.p5.jsonl\n",
      "paper_test.ns.pages.p5.jsonl\n",
      "paper_test_pipeline.ns.pages.p5.jsonl\n",
      "\u001b[0m\u001b[01;31mpaper_test_pipeline.ps.pages.p5.jsonl.gz\u001b[0m\n",
      "paper_test_predicted_pipeline.ps.pages.p5.jsonl\n",
      "train.ns.pages.p5.jsonl\n",
      "train.pages.p5.jsonl\n"
     ]
    }
   ],
   "source": [
    "ls working/data/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "french-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip working/data/training/paper_test_pipeline.ps.pages.p5.jsonl.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-klein",
   "metadata": {},
   "source": [
    "### Load test data\n",
    "Use the saved label encodings from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "comparable-laugh",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 15905.80it/s]\n",
      "100%|██████████| 9999/9999 [00:00<00:00, 135253.44it/s]\n"
     ]
    }
   ],
   "source": [
    "infile = 'working/data/training/paper_test_pipeline.ps.pages.p5.jsonl'\n",
    "label_checkpoint_file = 'working/data/training/label_encoder_train.pkl'\n",
    "dsreader = DatasetReader(in_file=infile,label_checkpoint_file=label_checkpoint_file, database_path='data/data/fever/fever.db', type='test')\n",
    "raw_test, test_data = dsreader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "developmental-kenya",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(3,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "ds_test = dsreader.get_dataset()\n",
    "print(ds_test.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-series",
   "metadata": {},
   "source": [
    "#### Load the BERT tokenizer\n",
    "\n",
    "The FEVER vocab file is build using tokens that were concatenations of the train and the dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fourth-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "import tensorflow_text as text\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "vocab_file_out = 'working/data/fever_vocab.txt'\n",
    "pt_tokenizer = text.BertTokenizer(vocab_file_out, **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-trader",
   "metadata": {},
   "source": [
    "#### Prepare the tensor dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "therapeutic-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dense-aquarium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: (60,), types: tf.int64>\n",
      "<MapDataset shapes: (60,), types: tf.int64>\n",
      "<BatchDataset shapes: (((64, 60), (64, 60)), (64, 3)), types: ((tf.int64, tf.int64), tf.int32)>\n",
      "((TensorSpec(shape=(64, 60), dtype=tf.int64, name=None), TensorSpec(shape=(64, 60), dtype=tf.int64, name=None)), TensorSpec(shape=(64, 3), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 60\n",
    "BUFFER_SIZE = 32000\n",
    "def tokenize_and_pad(text, max_len):\n",
    "    segment = pt_tokenizer.tokenize(text).merge_dims(1, -1)\n",
    "    inp = segment.to_tensor(shape=[None, max_len])\n",
    "    return inp[0]\n",
    "\n",
    "h = ds_test.map(lambda x, y: tokenize_and_pad(x[0], MAX_SEQ_LEN))\n",
    "e = ds_test.map(lambda x, y: tokenize_and_pad(x[1], MAX_SEQ_LEN))\n",
    "l = ds_test.map(lambda x, y: y)\n",
    "print(h)\n",
    "print(e)\n",
    "f = tf.data.Dataset.zip((h,e))\n",
    "d = tf.data.Dataset.zip((f,l))\n",
    "# do not shuffle\n",
    "dataset_test = d.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_test)\n",
    "print(dataset_test.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "plain-lesson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim_texts.jsonl  embedding_mappings_300d.npz    test_y_preds.npz  \u001b[0m\u001b[01;34mtraining\u001b[0m/\n",
      "\u001b[01;34mdev\u001b[0m/               fever_vocab.txt                test_y_tests.npz\n",
      "dev_labels.npz     matching_page_sentences.jsonl  train_labels.npz\n",
      "dev_x.npz          \u001b[01;34mout\u001b[0m/                           train_x.npz\n"
     ]
    }
   ],
   "source": [
    "ls working/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-genius",
   "metadata": {},
   "source": [
    "#### Load the prefilled embedding matrix from glove 300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecological-flight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arr_0']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile = np.load(\"working/data/embedding_mappings_300d.npz\")\n",
    "npzfile.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "horizontal-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = npzfile['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-opposition",
   "metadata": {},
   "source": [
    "#### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "civic-swiss",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "hypothesis (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "evidence (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    2400300     hypothesis[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    2400300     evidence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 300)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 300)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, None, 600)    1442400     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 600)    1442400     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, None, None)   0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, None, None)   0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, None)   0           permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, None)   0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, 600)    0           lambda_1[0][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 600)    0           lambda[0][0]                     \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, None, 600)    0           bidirectional[0][0]              \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, None, 600)    0           bidirectional[0][0]              \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, None, 600)    0           bidirectional_1[0][0]            \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 600)    0           bidirectional_1[0][0]            \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 2400)   0           bidirectional[0][0]              \n",
      "                                                                 dot_1[0][0]                      \n",
      "                                                                 subtract[0][0]                   \n",
      "                                                                 multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 2400)   0           bidirectional_1[0][0]            \n",
      "                                                                 dot_2[0][0]                      \n",
      "                                                                 subtract_1[0][0]                 \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Compresser (TimeDistributed)    (None, None, 300)    720300      concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 300)    0           Compresser[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 300)    0           Compresser[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "finaldecoder (Bidirectional)    (None, None, 600)    1442400     dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 600)          0           finaldecoder[0][0]               \n",
      "                                                                 finaldecoder[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 600)          0           finaldecoder[0][0]               \n",
      "                                                                 finaldecoder[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2400)         0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "                                                                 global_average_pooling1d[1][0]   \n",
      "                                                                 global_max_pooling1d[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 2400)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense300_ (Dense)               (None, 100)          240100      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100)          0           dense300_[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "judge300_ (Dense)               (None, 3)            303         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,088,503\n",
      "Trainable params: 5,287,903\n",
      "Non-trainable params: 4,800,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "\n",
    "vocab_size= 8000\n",
    "dim = 300\n",
    "inp1 = keras.Input(shape=(None, ), name = \"hypothesis\")\n",
    "inp2 = keras.Input(shape=(None, ), name = \"evidence\")\n",
    "\n",
    "embedding_hyp_layer = Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False)\n",
    "embedding_evi_layer = Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False)\n",
    "\n",
    "\n",
    "x_hyp = embedding_hyp_layer(inp1)\n",
    "x_hyp = tf.keras.layers.Dropout(0.5)(x_hyp)\n",
    "\n",
    "x_evi = embedding_evi_layer(inp2)\n",
    "x_evi = tf.keras.layers.Dropout(0.5)(x_evi)\n",
    "\n",
    "\n",
    "lstm_layer1 = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim), return_sequences=True))(x_hyp)\n",
    "\n",
    "lstm_layer2 = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim), return_sequences=True))(x_evi)\n",
    "\n",
    "\n",
    "F_p, F_h = lstm_layer1, lstm_layer2\n",
    "Eph = keras.layers.Dot(axes=(2, 2))([F_h, F_p])  # [batch_size, Hsize, Psize]\n",
    "Eh = Lambda(lambda x: keras.activations.softmax(x))(Eph)  # [batch_size, Hsize, Psize]\n",
    "Ep = keras.layers.Permute((2, 1))(Eph)  # [batch_size, Psize, Hsize)\n",
    "Ep = Lambda(lambda x: keras.activations.softmax(x))(Ep)  # [batch_size, Psize, Hsize]\n",
    "\n",
    "# 4, Normalize score matrix, encoder premesis and get alignment\n",
    "PremAlign = keras.layers.Dot((2, 1))([Ep, lstm_layer2]) # [-1, Psize, dim]\n",
    "HypoAlign = keras.layers.Dot((2, 1))([Eh, lstm_layer1]) # [-1, Hsize, dim]\n",
    "mm_1 = keras.layers.Multiply()([lstm_layer1, PremAlign])\n",
    "mm_2 = keras.layers.Multiply()([lstm_layer2, HypoAlign])\n",
    "sb_1 = keras.layers.Subtract()([lstm_layer1, PremAlign])\n",
    "sb_2 = keras.layers.Subtract()([lstm_layer2, HypoAlign])\n",
    "    \n",
    "\n",
    "# concat [a_, a~, a_ * a~, a_ - a~], isto za b_, b~\n",
    "PremAlign = keras.layers.Concatenate()([lstm_layer1, PremAlign, sb_1, mm_1,])  # [batch_size, Psize, 2*unit]\n",
    "HypoAlign = keras.layers.Concatenate()([lstm_layer2, HypoAlign, sb_2, mm_2])  # [batch_size, Hsize, 2*unit]\n",
    "\n",
    "\n",
    "# ff layer w/RELU activation\n",
    "Compresser = tf.keras.layers.TimeDistributed(Dense(300,\n",
    "                                   kernel_regularizer=l2(0.0),\n",
    "                                   bias_regularizer=l2(0.0),\n",
    "                                   activation='relu'),\n",
    "                             name='Compresser')\n",
    "\n",
    "PremAlign = Compresser(PremAlign)\n",
    "HypoAlign = Compresser(HypoAlign)\n",
    "    \n",
    "\n",
    "Decoder = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim), return_sequences=True), name='finaldecoder')\n",
    "\n",
    "\n",
    "PremAlign = Dropout(0.5)(PremAlign)\n",
    "HypoAlign = Dropout(0.5)(HypoAlign)\n",
    "final_p = Decoder(PremAlign)\n",
    "final_h = Decoder(HypoAlign)\n",
    "\n",
    "\n",
    "AveragePooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "MaxPooling = tf.keras.layers.GlobalMaxPooling1D()\n",
    "\n",
    "# AveragePooling = Lambda(lambda x: K.mean(x, axis=1)) # outs [-1, dim]\n",
    "# MaxPooling = Lambda(lambda x: K.max(x, axis=1)) # outs [-1, dim]\n",
    "avg_p = AveragePooling(final_p)\n",
    "avg_h = AveragePooling(final_h)\n",
    "max_p = MaxPooling(final_p)\n",
    "max_h = MaxPooling(final_h)\n",
    "# concat of avg and max pooling for hypothesis and premise\n",
    "Final = keras.layers.Concatenate()([avg_p, max_p, avg_h, max_h])\n",
    "# dropout layer\n",
    "Final = Dropout(0.5)(Final)\n",
    "# ff layer w/tanh activation\n",
    "Final = Dense(100,\n",
    "              kernel_regularizer=l2(0.0),\n",
    "              bias_regularizer=l2(0.0),\n",
    "              name='dense300_',\n",
    "              activation='tanh')(Final)\n",
    "\n",
    "# last dropout factor\n",
    "factor = 1\n",
    "# if self.LastDropoutHalf:\n",
    "#     factor = 2\n",
    "Final = Dropout(0.5 / factor)(Final)\n",
    "\n",
    "# softmax classifier\n",
    "Final = Dense(3,\n",
    "              activation='softmax',\n",
    "              name='judge300_')(Final)\n",
    "model = tf.keras.Model(inputs=[inp1, inp2], outputs=Final)\n",
    "\n",
    "LearningRate = 4e-4\n",
    "GradientClipping = 10.0\n",
    "\n",
    "# Optimizer = keras.optimizers.Adam(lr = LearningRate,\n",
    "#             clipnorm = GradientClipping)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-conclusion",
   "metadata": {},
   "source": [
    "#### Check the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "outside-admission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 25s 144ms/step - loss: 1.7682 - accuracy: 0.5732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7682045698165894, 0.5732171535491943]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath = 'tmp/attention_esim/checkpoint_fever_rte_esim'\n",
    "model.load_weights(checkpoint_filepath)\n",
    "model.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-operator",
   "metadata": {},
   "source": [
    "#### Calculate the FEVER score\n",
    "\n",
    "- Strictly correct: when all the evidences predicted are correct and the predicted label is correct\n",
    "- Correct: when only the predicted label is correct "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-morocco",
   "metadata": {},
   "source": [
    "#### Compute the precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "rational-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "narrative-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred_proba, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "growing-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'working/data/test_y_preds.npz'\n",
    "np.savez(outfile, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "colonial-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_y = dataset_test.map(lambda f, l: l)\n",
    "y_test_onehot = []\n",
    "for d in ds_y.batch(1):\n",
    "    for d1 in d:\n",
    "        y_test_onehot.append(d1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dietary-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([np.argmax(a, axis=1) for a in y_test_onehot]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "regulated-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'working/data/test_y_tests.npz'\n",
    "np.savez(outfile, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "computational-retention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS', 'REFUTES']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d['label_text'] for d in test_data[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "streaming-button",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.83      0.62      3328\n",
      "           1       0.75      0.33      0.46      3328\n",
      "           2       0.64      0.57      0.60      3328\n",
      "\n",
      "    accuracy                           0.57      9984\n",
      "   macro avg       0.63      0.57      0.56      9984\n",
      "weighted avg       0.63      0.57      0.56      9984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "#['NOT ENOUGH INFO', 'REFUTES', 'SUPPORTS'] == [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "received-investigation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, \"Grease_gun_-LRB-tool-RRB-\", -1, [[\"Hand-powered , where there is no trigger mechanism , and the grease is forced through the aperture by the back-pressure built up by pushing on the butt of the grease gun , which slides a piston through the body of the tool , pumping grease out of the aperture .\"], []]], [-1, null, \"Grease_gun_-LRB-tool-RRB-\", -2, [[], []]], [-1, null, \"Nasal_sebum\", -2, [[], []]], [-1, null, \"Grease\", -2, [[], []]], [-1, null, \"Thermal_interface_material\", -2, [[], []]]]]}\n"
     ]
    }
   ],
   "source": [
    "!head -1 working/data/training/paper_test_pipeline.ps.pages.p5.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "incoming-complex",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_dev.jsonl   shared_task_dev.jsonl   train.jsonl\n",
      "paper_test.jsonl  shared_task_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "ls /local/fever-common/data/fever-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-exercise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-belarus",
   "metadata": {},
   "source": [
    "For fever score, we will need to compare the labels and the evidences.\n",
    "\n",
    "First we need to extract the true labels and evidences from the original training dataset.\n",
    "\n",
    "In the original dataset, we will need to sample data for the NEI class just like we did for our original training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aggregate-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(mda.src.readers.DatasetReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broadband-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.src.readers.DatasetGenerator import DatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "medium-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_generator = DatasetGenerator(dataset_root='/local/fever-common/',out_dir='working/data/out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "terminal-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory working/data/out/\n",
      "Writing data to working/data/out//paper_test.ns.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [02:59<00:00, 55.72it/s] \n"
     ]
    }
   ],
   "source": [
    "ds_generator.generate_nei_evidences('paper_test', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-crowd",
   "metadata": {},
   "source": [
    "### In dataset type B\n",
    "\n",
    "We have predicted pages and predicted sentences per page for each claim. We need the predictions for those pages and sentences to compute the FEVER score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "raising-speaker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[133128,\n",
       "   None,\n",
       "   'Grease_gun_-LRB-tool-RRB-',\n",
       "   -1,\n",
       "   [['Hand-powered , where there is no trigger mechanism , and the grease is forced through the aperture by the back-pressure built up by pushing on the butt of the grease gun , which slides a piston through the body of the tool , pumping grease out of the aperture .'],\n",
       "    []]],\n",
       "  [-1, None, 'Grease_gun_-LRB-tool-RRB-', -2, [[], []]],\n",
       "  [-1, None, 'Nasal_sebum', -2, [[], []]],\n",
       "  [-1, None, 'Grease', -2, [[], []]],\n",
       "  [-1, None, 'Thermal_interface_material', -2, [[], []]]]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test[:1][0]['evidence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-basics",
   "metadata": {},
   "source": [
    "### In dataset type A\n",
    "\n",
    "We also need the original annotated pages and the sentences from the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "handled-therapist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 182167.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#!head -2 working/data/training/paper_test.ns.pages.p5.jsonl\n",
    "#re-read the original data, for the annotated evidences\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "class Reader:\n",
    "    def __init__(self,encoding=\"utf-8\"):\n",
    "        self.enc = encoding\n",
    "    def read(self,file):\n",
    "        with open(file,\"r\",encoding = self.enc) as f:\n",
    "            return self.process(f)\n",
    "    def process(self,f):\n",
    "        pass\n",
    "\n",
    "class JSONLineReader(Reader):\n",
    "    def process(self,fp):\n",
    "        data = []\n",
    "        for line in tqdm(fp.readlines()):\n",
    "            data.append(json.loads(line.strip()))\n",
    "        return data\n",
    "    \n",
    "jlr = JSONLineReader()\n",
    "split = 'paper_test'\n",
    "working_dir = 'working/data/'\n",
    "k = 5\n",
    "test_data_file = working_dir + \"training/{0}.ns.pages.p{1}.jsonl\".format(split, k)\n",
    "data_orig = jlr.read(test_data_file)\n",
    "orig_evidences = [d['evidence'] for d in data_orig[:len(y_test)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-colorado",
   "metadata": {},
   "source": [
    "Generate the final predictions, for the label, the predicted pages and the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "horizontal-prison",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9984it [00:00, 81103.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to training/paper_test_predicted_pipeline.ps.pages.p5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "split = 'paper_test_predicted'\n",
    "k = 5\n",
    "with open(working_dir + \"training/{0}_pipeline.ps.pages.p{1}.jsonl\".format(split,k),\"w+\") as f_out:\n",
    "    print(\"Saving to training/{0}_pipeline.ps.pages.p{1}.jsonl\".format(split,k))\n",
    "    for rec, orig, true_label, predicted_label in tqdm(zip(raw_test[:len(y_test)], orig_evidences, y_test, y_pred)):\n",
    "        evs = []\n",
    "        for evidence_group in rec['evidence']:\n",
    "            for evidence in evidence_group:\n",
    "                #print(evidence)\n",
    "                if evidence[0] > -1:\n",
    "                    ev = [evidence[0], evidence[1], evidence[2], evidence[4][1]]\n",
    "                    evs.append(ev)\n",
    "\n",
    "        out = {'true_label': str(true_label), 'predicted_label': str(predicted_label), 'orig': orig, 'pred': evs}\n",
    "\n",
    "        f_out.write(json.dumps(out) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "suspended-mississippi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"true_label\": \"0\", \"predicted_label\": \"0\", \"orig\": [[[133128, null, \"Grease_gun_-LRB-tool-RRB-\", -1]]], \"pred\": [[133128, null, \"Grease_gun_-LRB-tool-RRB-\", []]]}\n"
     ]
    }
   ],
   "source": [
    "!head -1 working/data/training/paper_test_predicted_pipeline.ps.pages.p5.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-drive",
   "metadata": {},
   "source": [
    "Load the predictions from the file to use in fever scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "black-coverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9984/9984 [00:00<00:00, 19628.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "jlr = JSONLineReader()\n",
    "split = 'paper_test_predicted'\n",
    "working_dir = 'working/data/'\n",
    "k = 5\n",
    "test_data_file = working_dir + \"training/{0}_pipeline.ps.pages.p{1}.jsonl\".format(split, k)\n",
    "predicted_results = jlr.read(test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "random-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fever_score(predicted_results):\n",
    "    strictly_correct = 0\n",
    "    correct = 0\n",
    "    cnt = 0\n",
    "    for d in tqdm(predicted_results):\n",
    "        true_label = d['true_label']\n",
    "        predicted_label = d['predicted_label']\n",
    "        true_evidence = d['orig']\n",
    "        predicted_evidence = d['pred']\n",
    "        te = {}\n",
    "        pe = {}\n",
    "        #is correct?\n",
    "        if (true_label == predicted_label):\n",
    "            correct += 1\n",
    "            # is strictly correct?\n",
    "            if (true_label != '0') and (true_label==predicted_label):\n",
    "                for eg in true_evidence:\n",
    "                    for e in eg:\n",
    "                        if e[2] in te:\n",
    "                            te[e[2]].append(e[3])\n",
    "                        else:\n",
    "                            te[e[2]]= [e[3]]    \n",
    "\n",
    "                for e in predicted_evidence:\n",
    "                    if e[2] in pe:\n",
    "                        pe[e[2]].append(e[3])\n",
    "                    else:\n",
    "                        pe[e[2]]= [e[3]]\n",
    "\n",
    "                # for each annotated evidence, see if we predicted the evidences\n",
    "                # did we correctly predict all pages?\n",
    "                all_pages = all([k1 in pe.keys() for k1 in te.keys()])\n",
    "                if all_pages:\n",
    "                    #for the pages we predicted, did we predict all the sentences?\n",
    "                    for k in te.keys():\n",
    "                        if k in pe: # the page is predicted\n",
    "                            true_sents = np.unique(te[k])\n",
    "                            pre_sents = np.unique(pe[k][0])\n",
    "                            #if all the true sentences were predicted\n",
    "                            match = all([actual_sent in pre_sents for actual_sent in true_sents])\n",
    "                            #if match and (len(true_sents) == len(pre_sents)):\n",
    "                            #we are predicting 5 lines per page, so the count may not match with the true evidence lines\n",
    "                            if match:\n",
    "                                strictly_correct += 1\n",
    "            elif (true_label == '0') and (true_label == predicted_label): # not enough info\n",
    "                    strictly_correct += 1\n",
    "    noevscore = np.round(correct/len(predicted_results)*100,2)\n",
    "    score = np.round(strictly_correct/len(predicted_results)*100,2)\n",
    "    print(\"noevscore={}, score={}\".format(noevscore, score))\n",
    "    return noevscore, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "straight-fountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9984/9984 [00:00<00:00, 175724.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noevscore=57.32, score=34.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(57.32, 34.92)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fever_score(predicted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-affiliate",
   "metadata": {},
   "source": [
    "#### From original FEVER paper\n",
    "Finally, we predict entailment\n",
    "using the Decomposable Attention model trained\n",
    "with the NEARESTP strategy. The classification\n",
    "accuracy is <b>31.87%</b>. Ignoring the requirement for\n",
    "correct evidence (NoScoreEv) the accuracy is\n",
    "<b>50.91%</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-patient",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
