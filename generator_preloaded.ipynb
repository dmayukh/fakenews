{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "random-elephant",
   "metadata": {},
   "source": [
    "#### Set the file names and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "delayed-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings\n",
    "glove_zip_file = \"glove.6B.zip\"\n",
    "glove_vectors_file = \"glove.6B.50d.txt\"\n",
    "\n",
    "#data files\n",
    "snli_zip_file = \"snli_1.0.zip\"\n",
    "snli_dev_file = \"snli_1.0_dev.txt\"\n",
    "snli_full_dataset_file = \"snli_1.0_train.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-potato",
   "metadata": {},
   "source": [
    "#### A data loader from zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "perceived-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "zip_file_name = 'entailment/data/snli_1.0.zip'\n",
    "output_file_name = 'snli_1.0/snli_1.0_train.txt'\n",
    "\n",
    "def load_data(num_samples=10):\n",
    "    counter = 0\n",
    "    columns = ['gold_label','sentence1','sentence2']\n",
    "    indices = [-1, -1, -1]\n",
    "    data = []\n",
    "    with zipfile.ZipFile(zip_file_name) as z:\n",
    "        for info in z.infolist():\n",
    "            if output_file_name in info.filename:\n",
    "                # read the file\n",
    "                print(\"Reading lines from file {}\".format(output_file_name))\n",
    "                with io.TextIOWrapper(z.open(output_file_name), encoding=\"utf-8\") as f:\n",
    "                    for line in tqdm(f):\n",
    "                        terms = line.split('\\t')\n",
    "                        if np.min(indices) == -1: # this is the first line\n",
    "                            indices = [np.where(np.array(terms) == val)[0] for val in columns]\n",
    "                            counter += 1\n",
    "                        else:\n",
    "                            idx = [i[0] for i in indices]\n",
    "                            #do not include the '-' label\n",
    "                            if np.array(terms)[idx][0] != '-':\n",
    "                                data.append(np.array(terms)[idx])\n",
    "                                counter += 1\n",
    "                        if (num_samples > -1) & (counter > num_samples):\n",
    "                            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interim-casting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5006it [00:00, 37071.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines from file snli_1.0/snli_1.0_train.txt\n",
      "Read 5000 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['neutral',\n",
       "  'A person on a horse jumps over a broken down airplane.',\n",
       "  'A person is training his horse for a competition.'],\n",
       " ['contradiction',\n",
       "  'A person on a horse jumps over a broken down airplane.',\n",
       "  'A person is at a diner, ordering an omelette.'],\n",
       " ['entailment',\n",
       "  'A person on a horse jumps over a broken down airplane.',\n",
       "  'A person is outdoors, on a horse.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(5000)\n",
    "data = [[d[0], d[1],d[2]] for d in data[:]]\n",
    "print(\"Read {} records\".format(len(data)))\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-youth",
   "metadata": {},
   "source": [
    "The d type instances in AWS has issues with LSTM, we need to change these settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "needed-confusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-computer",
   "metadata": {},
   "source": [
    "#### Pre-process the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "physical-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "def preprocess(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "        w = w.strip()\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "def build_tokenizer(data):\n",
    "    #combine the hypothesis and the evidence into one setence with the seperators in between.\n",
    "    all_texts = [\" \".join((preprocess(d[1]),preprocess(d[2]))) for d in data]\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "    lang_tokenizer.fit_on_texts(all_texts)\n",
    "    return lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-postage",
   "metadata": {},
   "source": [
    "#### The tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "stable-colleague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of hypothesis = (5000, 61) and evidence = (5000, 29)\n"
     ]
    }
   ],
   "source": [
    "lang_tokenizer = build_tokenizer(data)\n",
    "hyp_tokens = lang_tokenizer.texts_to_sequences([preprocess(d[1]) for d in data])\n",
    "hyp_tokens = tf.keras.preprocessing.sequence.pad_sequences(hyp_tokens, padding='post')\n",
    "evi_tokens = lang_tokenizer.texts_to_sequences([preprocess(d[2]) for d in data])\n",
    "evi_tokens = tf.keras.preprocessing.sequence.pad_sequences(evi_tokens, padding='post')\n",
    "print(\"Shape of hypothesis = {} and evidence = {}\".format(hyp_tokens.shape, evi_tokens.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-lucas",
   "metadata": {},
   "source": [
    "##### One hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "metropolitan-basket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A peek a the reshaped labels:\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "The datatypes of the training dataset, features=<class 'numpy.ndarray'>, labels=<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "labels = [d[0] for d in data]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels_enc = le.transform(labels)\n",
    "\n",
    "train_labels = np.zeros(shape=(len(labels_enc),3))\n",
    "for idx, val in enumerate(labels_enc):\n",
    "    train_labels[idx][val]=1\n",
    "print(\"A peek a the reshaped labels:\")\n",
    "print(train_labels[:5])\n",
    "print(\"The datatypes of the training dataset, features={}, labels={}\".format(type(labels_enc), type(train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-entertainment",
   "metadata": {},
   "source": [
    "#### Prepare the embeddings\n",
    "Load the glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "conservative-isolation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.50d.txt\n",
      "glove.6B.100d.txt\n",
      "Reading lines from file glove.6B.100d.txt\n",
      "glove.6B.200d.txt\n",
      "glove.6B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "glove_zip_file = \"entailment/data/glove.6B.zip\"\n",
    "glove_vectors_file = \"glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "with zipfile.ZipFile(glove_zip_file) as z:\n",
    "        for info in z.infolist():\n",
    "            print(info.filename)\n",
    "            if glove_vectors_file in info.filename:\n",
    "                # read the file\n",
    "                print(\"Reading lines from file {}\".format(glove_vectors_file))\n",
    "                with io.TextIOWrapper(z.open(glove_vectors_file), encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        terms = line.split()\n",
    "                        word = terms[0]\n",
    "                        coefs = np.asarray(terms[1:], dtype='float32')\n",
    "                        \n",
    "                        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prospective-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'the' is a very common word, find the max length\n",
    "max_length = embeddings_index[\"the\"].shape[0]\n",
    "\n",
    "embedding_matrix = np.zeros((len(lang_tokenizer.word_index) + 1, max_length))\n",
    "for word, i in lang_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-undergraduate",
   "metadata": {},
   "source": [
    "#### Build the inputs to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "thick-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_tensor_train_h, input_tensor_val_h, input_tensor_train_e, input_tensor_val_e,\\\n",
    "    target_tensor_train, target_tensor_val \\\n",
    "    = train_test_split(hyp_tokens, evi_tokens, train_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "impossible-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset_12 = tf.data.Dataset.from_tensor_slices((input_tensor_train_h, input_tensor_train_e))\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices(target_tensor_train)\n",
    "train_dataset = tf.data.Dataset.zip((dataset_12, dataset_label))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "dataset_12_val = tf.data.Dataset.from_tensor_slices((input_tensor_val_h, input_tensor_val_e))\n",
    "dataset_label_val = tf.data.Dataset.from_tensor_slices(target_tensor_val)\n",
    "val_dataset = tf.data.Dataset.zip((dataset_12_val, dataset_label_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "foreign-spank",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(64, 61), dtype=tf.int32, name=None),\n",
       "  TensorSpec(shape=(64, 29), dtype=tf.int32, name=None)),\n",
       " TensorSpec(shape=(64, 3), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-wrestling",
   "metadata": {},
   "source": [
    "#### Build the model network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "latin-virtue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    401300      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 100)    401300      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 200)          160800      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 200)          160800      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 400)          0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           6416        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            51          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,130,667\n",
      "Trainable params: 328,067\n",
      "Non-trainable params: 802,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size= len(lang_tokenizer.word_index)\n",
    "dim = embedding_matrix.shape[1]\n",
    "#dim = 50 #keep it same as the dim of the embedding matrix so that we can compare\n",
    "embedding_hyp_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True)\n",
    "embedding_evi_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True)\n",
    "\n",
    "inp1 = keras.Input(shape=(None,))\n",
    "inp2 = keras.Input(shape=(None,))\n",
    "x_hyp = embedding_hyp_layer(inp1)\n",
    "x_evi = embedding_evi_layer(inp2)\n",
    "\n",
    "#this throws an error in the d type instances in AWS, works on p type instances\n",
    "#hyp_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(dim))(x_hyp)\n",
    "hyp_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim)))(x_hyp)\n",
    "hyp_evi = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim)))(x_evi)\n",
    "\n",
    "#tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(64))),\n",
    "w = keras.layers.concatenate([hyp_lstm, hyp_evi], axis = 1)\n",
    "\n",
    "x3 = tf.keras.layers.Dense(16, activation='relu')(w)\n",
    "x4 = tf.keras.layers.Dropout(0.1)(x3)\n",
    "output = tf.keras.layers.Dense(3, activation='softmax')(x4)\n",
    "    \n",
    "model = keras.Model(inputs=[inp1, inp2], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "exceptional-seeking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 10s 122ms/step - loss: 1.0822 - accuracy: 0.4013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f79f5ee2810>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-bleeding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dated-purse",
   "metadata": {},
   "source": [
    "#### Try a simple model for looking under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "virtual-gothic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 61), (64, 3)), types: (tf.int32, tf.float64)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset_1 = tf.data.Dataset.from_tensor_slices(input_tensor_train_h)\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices(target_tensor_train)\n",
    "train_ds = tf.data.Dataset.zip((dataset_1, dataset_label))\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "contained-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, None, 100)         401300    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                3216      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 565,367\n",
      "Trainable params: 164,067\n",
      "Non-trainable params: 401,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size= len(lang_tokenizer.word_index)\n",
    "dim = embedding_matrix.shape[1]\n",
    "#dim = 50 #keep it same as the dim of the embedding matrix so that we can compare\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True)\n",
    "\n",
    "inp1 = keras.Input(shape=(None,))\n",
    "#inp2 = keras.Input(shape=(None,))\n",
    "x = embedding(inp1)\n",
    "lstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim)))(x)\n",
    "x1 = tf.keras.layers.Dense(16, activation='relu')(lstm_layer)\n",
    "x2 = tf.keras.layers.Dropout(0.1)(x1)\n",
    "output = tf.keras.layers.Dense(3, activation='softmax')(x2)\n",
    "    \n",
    "model = keras.Model(inputs=[inp1], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "narrow-thing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 6s 85ms/step - loss: 1.1036 - accuracy: 0.3241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f79ec5187d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-freight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
