{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLI\n",
    "\n",
    "A large annotated corpus for learning natural language inference by [Bowman et al.](https://arxiv.org/pdf/1508.05326v1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_zip_file = \"glove.6B.zip\"\n",
    "glove_vectors_file = \"glove.6B.50d.txt\"\n",
    "\n",
    "snli_zip_file = \"snli_1.0.zip\"\n",
    "snli_dev_file = \"snli_1.0_dev.txt\"\n",
    "snli_full_dataset_file = \"snli_1.0_train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31mglove.6B.zip\u001b[0m  labels.npz  labels.txt  \u001b[01;31msnli_1.0.zip\u001b[0m  vocab.txt\n"
     ]
    }
   ],
   "source": [
    "ls entailment/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset reader\n",
    "\n",
    "The dataset is too large to fit into the memory, we will need to build a dataset reader. \n",
    "\n",
    "We will include the text preprocessor into the python generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "def preprocess(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
    "        w = w.strip()\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "zip_file_name = 'entailment/data/snli_1.0.zip'\n",
    "output_file_name = 'snli_1.0/snli_1.0_train.txt'\n",
    "\n",
    "def read_zip_csv(filename):\n",
    "    counter = 0\n",
    "    columns = ['gold_label','sentence1','sentence2']\n",
    "    indices = [-1, -1, -1]\n",
    "    with zipfile.ZipFile(zip_file_name) as z:\n",
    "        for info in z.infolist():\n",
    "            if output_file_name in info.filename:\n",
    "                # read the file\n",
    "                print(\"Reading lines from file {}\".format(output_file_name))\n",
    "                with io.TextIOWrapper(z.open(output_file_name), encoding=\"utf-8\") as f:\n",
    "                    for line in tqdm(f):\n",
    "                        terms = line.split('\\t')\n",
    "                        if np.min(indices) == -1: # this is the first line\n",
    "                            indices = [np.where(np.array(terms) == val)[0] for val in columns]\n",
    "                            counter += 1\n",
    "                        else:\n",
    "                            idx = [i[0] for i in indices]\n",
    "                            #do not include the '-' label\n",
    "                            if np.array(terms)[idx][0] != '-':\n",
    "                                data = np.array(terms)[idx]\n",
    "                                #print(\"d1=\", data[1])\n",
    "                                lbl, hyp, evi = data\n",
    "                                data = [lbl, preprocess(hyp), preprocess(evi)]\n",
    "                                yield data\n",
    "\n",
    "\n",
    "n_features = 3\n",
    "def get_dataset():\n",
    "    generator = lambda: read_zip_csv(zip_file_name)\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator, (tf.string), (n_features), ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 904.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines from file snli_1.0/snli_1.0_train.txt\n",
      "tf.Tensor(b'<start> a person on a horse jumps over a broken down airplane . <end>', shape=(), dtype=string)\n",
      "tf.Tensor(b'<start> a person on a horse jumps over a broken down airplane . <end>', shape=(), dtype=string)\n",
      "tf.Tensor(b'<start> a person on a horse jumps over a broken down airplane . <end>', shape=(), dtype=string)\n",
      "tf.Tensor(b'<start> children smiling and waving at camera <end>', shape=(), dtype=string)\n",
      "tf.Tensor(b'<start> children smiling and waving at camera <end>', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for d in get_dataset().take(5):\n",
    "    print(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 820.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines from file snli_1.0/snli_1.0_train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([b'neutral',\n",
       "       b'<start> a person on a horse jumps over a broken down airplane . <end>',\n",
       "       b'<start> a person is training his horse for a competition . <end>'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(raw_ds)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts are available in two columns, the hypothesis and the evidence. We will need to concatenate them if we wish to send them to a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_ds = raw_ds.map(lambda x: tf.strings.join([x[1], x[2]], separator=\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need tensorflow_text for using pre built bert tokenizers.\n",
    "\n",
    "The tensorflow text needs tensorflow 2.5.0, so we will need to install the tensorflow-gpu version 2.5.0 as well, when using the Deep learning AMI in AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bert_vocab_from_dataset can efficiently handle processing of the texts in the training dataset, as we send them in to the vocab builder batch by batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\", \"<start>\", \"<end>\"]\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "414it [00:00, 4131.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines from file snli_1.0/snli_1.0_train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550153it [02:10, 4222.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 29s, sys: 24.4 s, total: 4min 54s\n",
      "Wall time: 3min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    mapped_ds.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '<start>', '<end>', '!', ',', '.', '<']\n",
      "['##s', 'running', 'yellow', 'has', 'riding', 'ball', 'out', 'brown', 'hat', 'next']\n"
     ]
    }
   ],
   "source": [
    "print(pt_vocab[:10])\n",
    "print(pt_vocab[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    print(\"Saving vocab file to {}\".format(filepath))\n",
    "    with open(filepath, 'w') as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vocab file to entailment/data/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "vocab_file_out = 'entailment/data/vocab.txt'\n",
    "write_vocab_file(vocab_file_out, pt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7882 entailment/data/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l entailment/data/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: our basic tokenizer had 14863 words only from 200K documents. We have restricted the vocab to only 8K here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "zip_file_name = 'entailment/data/snli_1.0.zip'\n",
    "output_file_name = 'snli_1.0/snli_1.0_train.txt'\n",
    "\n",
    "def load_data(num_samples=10):\n",
    "    counter = 0\n",
    "    columns = ['gold_label','sentence1','sentence2']\n",
    "    indices = [-1, -1, -1]\n",
    "    data = []\n",
    "    with zipfile.ZipFile(zip_file_name) as z:\n",
    "        for info in z.infolist():\n",
    "            if output_file_name in info.filename:\n",
    "                # read the file\n",
    "                print(\"Reading lines from file {}\".format(output_file_name))\n",
    "                with io.TextIOWrapper(z.open(output_file_name), encoding=\"utf-8\") as f:\n",
    "                    for line in tqdm(f):\n",
    "                        terms = line.split('\\t')\n",
    "                        if np.min(indices) == -1: # this is the first line\n",
    "                            indices = [np.where(np.array(terms) == val)[0] for val in columns]\n",
    "                            counter += 1\n",
    "                        else:\n",
    "                            idx = [i[0] for i in indices]\n",
    "                            #do not include the '-' label\n",
    "                            if np.array(terms)[idx][0] != '-':\n",
    "                                data.append(np.array(terms)[idx])\n",
    "                                counter += 1\n",
    "                        if (num_samples > -1) & (counter > num_samples):\n",
    "                            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3566it [00:00, 35658.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines from file snli_1.0/snli_1.0_train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300382it [00:08, 35900.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 300000 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = load_data(300000)\n",
    "print(\"Read {} records\".format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['neutral',\n",
       "        'A person on a horse jumps over a broken down airplane.',\n",
       "        'A person is training his horse for a competition.'], dtype='<U161'),\n",
       " array(['contradiction',\n",
       "        'A person on a horse jumps over a broken down airplane.',\n",
       "        'A person is at a diner, ordering an omelette.'], dtype='<U161'),\n",
       " array(['entailment',\n",
       "        'A person on a horse jumps over a broken down airplane.',\n",
       "        'A person is outdoors, on a horse.'], dtype='<U161')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[d[0], d[1],d[2]] for d in data[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['neutral',\n",
       "  'A person on a horse jumps over a broken down airplane.',\n",
       "  'A person is training his horse for a competition.'],\n",
       " ['contradiction',\n",
       "  'A person on a horse jumps over a broken down airplane.',\n",
       "  'A person is at a diner, ordering an omelette.'],\n",
       " ['entailment',\n",
       "  'A person on a horse jumps over a broken down airplane.',\n",
       "  'A person is outdoors, on a horse.']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the input data\n",
    "\n",
    "- Tokenize the sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "def preprocess(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
    "        w = w.strip()\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "def build_tokenizer(data):\n",
    "    #combine the hypothesis and the evidence into one setence with the seperators in between.\n",
    "    all_texts = [\" \".join((preprocess(d[1]),preprocess(d[2]))) for d in data]\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "    lang_tokenizer.fit_on_texts(all_texts)\n",
    "    return lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_tokenizer = build_tokenizer(data)\n",
    "hyp_tokens = lang_tokenizer.texts_to_sequences([preprocess(d[1]) for d in data])\n",
    "hyp_tokens = tf.keras.preprocessing.sequence.pad_sequences(hyp_tokens, padding='post')\n",
    "evi_tokens = lang_tokenizer.texts_to_sequences([preprocess(d[2]) for d in data])\n",
    "evi_tokens = tf.keras.preprocessing.sequence.pad_sequences(evi_tokens, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 84)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 62)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evi_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels will need to be on hot encoded so that it can be fed into training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [d[0] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels_enc = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A peek a the reshaped labels:\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "The datatypes of the training dataset, features=<class 'numpy.ndarray'>, labels=<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.zeros(shape=(len(labels_enc),3))\n",
    "for idx, val in enumerate(labels_enc):\n",
    "    train_labels[idx][val]=1\n",
    "print(\"A peek a the reshaped labels:\")\n",
    "print(train_labels[:5])\n",
    "print(\"The datatypes of the training dataset, features={}, labels={}\".format(type(labels_enc), type(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((hyp_tokens, evi_tokens), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(features, train_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16451"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lang_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the embeddings\n",
    "Load the glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.50d.txt\n",
      "glove.6B.100d.txt\n",
      "Reading lines from file glove.6B.100d.txt\n",
      "glove.6B.200d.txt\n",
      "glove.6B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "glove_zip_file = \"entailment/data/glove.6B.zip\"\n",
    "glove_vectors_file = \"glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "with zipfile.ZipFile(glove_zip_file) as z:\n",
    "        for info in z.infolist():\n",
    "            print(info.filename)\n",
    "            if glove_vectors_file in info.filename:\n",
    "                # read the file\n",
    "                print(\"Reading lines from file {}\".format(glove_vectors_file))\n",
    "                with io.TextIOWrapper(z.open(glove_vectors_file), encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        terms = line.split()\n",
    "                        word = terms[0]\n",
    "                        coefs = np.asarray(terms[1:], dtype='float32')\n",
    "                        \n",
    "                        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['neutral', 'A person on a horse jumps over a broken down airplane.', 'A person is training his horse for a competition.']]\n",
      "[[   3    2   45   10    2  199  210   81    2 1040   41  799    5    4\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(data[:1])\n",
    "print(hyp_tokens[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word 'a' in the tokenized sequence is 2\n",
      "Index of word 'person' in the tokenized sequence is 45\n",
      "Index of word 'on' in the tokenized sequence is 10\n",
      "Index of word 'a' in the tokenized sequence is 2\n",
      "Index of word 'horse' in the tokenized sequence is 199\n",
      "Index of word 'jumps' in the tokenized sequence is 210\n",
      "Index of word 'over' in the tokenized sequence is 81\n",
      "Index of word 'a' in the tokenized sequence is 2\n",
      "Index of word 'broken' in the tokenized sequence is 1040\n",
      "Index of word 'down' in the tokenized sequence is 41\n",
      "Index of word 'airplane' in the tokenized sequence is 799\n",
      "Index of word '.' in the tokenized sequence is 5\n"
     ]
    }
   ],
   "source": [
    "sent = 'A person on a horse jumps over a broken down airplane .'\n",
    "for word in sent.lower().split(' '):\n",
    "    print(\"Index of word '{}' in the tokenized sequence is {}\".format(word, lang_tokenizer.word_index.get(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6905e-01,  2.7387e-01,  5.6794e-01,  1.9879e-03, -1.2258e-01,\n",
       "       -4.7564e-01,  5.3575e-01,  5.3408e-02,  3.3973e-01,  3.9381e-01,\n",
       "        5.3866e-01, -7.9248e-02,  2.1101e-01,  3.9848e-01, -2.4034e-01,\n",
       "       -2.1135e-01,  2.4204e-01,  7.5500e-01,  8.7943e-01,  7.6776e-03,\n",
       "        2.2761e-01,  1.3173e-01,  2.4464e-01, -7.9466e-01,  5.3546e-01,\n",
       "       -1.3146e-01, -1.0728e+00,  3.3607e-01,  1.4914e-01, -1.8328e-01,\n",
       "       -2.7835e-01,  2.2114e-01, -8.6374e-03,  7.1159e-01, -3.7257e-01,\n",
       "        9.2297e-01,  6.7150e-01,  4.6155e-01,  1.0223e+00,  3.3371e-02,\n",
       "       -4.2123e-01, -2.5116e-01,  2.3897e-01,  2.9825e-01,  1.9633e-01,\n",
       "        4.7368e-01,  1.1481e-01, -2.4439e-01, -4.5919e-01, -2.7042e-01,\n",
       "       -3.1669e-01,  5.7177e-01, -2.9423e-01,  8.1875e-01,  4.4011e-01,\n",
       "       -7.7701e-01,  8.0021e-02,  8.4484e-01,  2.2236e+00,  2.9603e-01,\n",
       "        2.6407e-01,  8.9465e-01,  3.3492e-01,  4.6542e-01, -4.2811e-01,\n",
       "       -3.2176e-01,  6.6258e-02, -1.0479e-01, -7.3056e-01,  3.8247e-01,\n",
       "       -6.4507e-01, -1.0068e+00,  9.5788e-02, -2.2178e-01,  3.7206e-01,\n",
       "        1.4360e-01,  5.4153e-01, -3.7319e-02, -8.0340e-01,  4.9769e-01,\n",
       "       -4.8540e-01,  9.6025e-02,  1.7554e-01,  2.2751e-01, -6.3573e-01,\n",
       "       -7.6998e-01,  6.8933e-01, -6.7634e-01, -8.4612e-01, -1.8800e-01,\n",
       "        2.8685e-01,  2.0765e-02,  4.1986e-01, -3.7801e-02,  6.3915e-02,\n",
       "        4.1061e-01, -1.9975e-01,  1.0633e-02,  5.6534e-01, -9.6648e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.get('airplane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of sequence is 100\n"
     ]
    }
   ],
   "source": [
    "embeddings_index[\"the\"].shape\n",
    "max_length = embeddings_index[\"the\"].shape[0]\n",
    "print(\"max length of sequence is {}\".format(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(lang_tokenizer.word_index) + 1, max_length))\n",
    "for word, i in lang_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25675, 100)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.35000005e-01, -6.13709986e-01, -2.74360001e-01, -9.24369991e-02,\n",
       "        1.90660000e-01,  5.62789977e-01, -1.74840003e-01,  1.53009999e+00,\n",
       "       -5.56100011e-01, -5.51329970e-01,  1.88119993e-01,  2.20029995e-01,\n",
       "       -4.19979990e-01, -7.03310013e-01,  3.01330000e-01, -7.82289982e-01,\n",
       "        4.47290003e-01,  7.39270002e-02, -1.32660002e-01, -2.14939993e-02,\n",
       "        4.37739998e-01, -3.12860012e-01, -4.06029999e-01, -5.79559982e-01,\n",
       "        9.12649989e-01,  1.44369996e+00, -1.72869995e-01, -1.52089998e-01,\n",
       "       -6.05459988e-01, -3.57780010e-01,  5.54880016e-02, -4.70630005e-02,\n",
       "        1.38840005e-01, -6.96669966e-02, -2.48850003e-01,  1.94619998e-01,\n",
       "        4.12220001e-01, -2.62349993e-01,  5.19490004e-01, -3.32349986e-01,\n",
       "       -3.63640010e-01,  7.51440004e-02, -3.47119987e-01,  3.85729998e-01,\n",
       "        5.62600017e-01, -3.08789998e-01, -1.88109994e-01, -9.43270046e-03,\n",
       "        2.23690003e-01, -9.54269990e-02,  6.86900020e-02, -1.80600002e-01,\n",
       "       -9.72819999e-02, -1.43860001e-03, -3.71749997e-01, -1.44330001e+00,\n",
       "       -2.71120012e-01,  5.36300004e-01,  1.25839996e+00, -7.31230006e-02,\n",
       "       -5.22870004e-01,  1.25080001e+00,  2.61539996e-01, -6.85880005e-01,\n",
       "       -1.33729994e-01, -7.27100000e-02,  5.72430015e-01, -8.78790021e-01,\n",
       "       -4.89419997e-01, -3.07119995e-01, -5.62680006e-01, -1.22180000e-01,\n",
       "        2.62360007e-01, -3.31779987e-01, -6.35919988e-01, -4.64980006e-01,\n",
       "        3.04569989e-01,  8.82650018e-02,  3.81220013e-01, -7.46150017e-01,\n",
       "       -8.61370027e-01, -4.55449998e-01,  3.47939998e-01,  6.36699975e-01,\n",
       "        3.20270002e-01,  2.28699997e-01, -1.05630003e-01, -6.54910028e-01,\n",
       "        7.02440023e-01,  3.71560007e-01, -2.65720002e-02, -1.09209999e-01,\n",
       "       -2.99120009e-01,  1.43419996e-01, -1.14600003e+00,  1.05489999e-01,\n",
       "        1.16889998e-01,  4.73589987e-01,  3.22519988e-01,  4.99879986e-01])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[771]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         1645200   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                3216      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 1,809,267\n",
      "Trainable params: 164,067\n",
      "Non-trainable params: 1,645,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size= len(lang_tokenizer.word_index)\n",
    "dim = embedding_matrix.shape[1]\n",
    "#dim = 50 #keep it same as the dim of the embedding matrix so that we can compare\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True)\n",
    "inp = keras.Input(shape=(None,))\n",
    "x1 = embedding_layer(inp)\n",
    "x2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(dim))(x1)\n",
    "x3 = tf.keras.layers.Dense(16, activation='relu')(x2)\n",
    "x4 = tf.keras.layers.Dropout(0.1)(x3)\n",
    "output = tf.keras.layers.Dense(3, activation='softmax')(x4)\n",
    "    \n",
    "model = keras.Model(inp, output)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'tmp/checkpoint_bowman'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_tensor_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-bd232df52b13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBUFFER_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_tensor_train' is not defined"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-1ceebda5dccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "    print(example.shape)\n",
    "    print(example)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1093/1093 [==============================] - 210s 189ms/step - loss: 1.0121 - accuracy: 0.4789 - val_loss: 0.8743 - val_accuracy: 0.5969\n",
      "Epoch 2/20\n",
      "1093/1093 [==============================] - 214s 196ms/step - loss: 0.8627 - accuracy: 0.6046 - val_loss: 0.8212 - val_accuracy: 0.6214\n",
      "Epoch 3/20\n",
      "1093/1093 [==============================] - 224s 205ms/step - loss: 0.7943 - accuracy: 0.6482 - val_loss: 0.7832 - val_accuracy: 0.6458\n",
      "Epoch 4/20\n",
      "1093/1093 [==============================] - 234s 214ms/step - loss: 0.7450 - accuracy: 0.6747 - val_loss: 0.7818 - val_accuracy: 0.6500\n",
      "Epoch 5/20\n",
      "1093/1093 [==============================] - 247s 225ms/step - loss: 0.7073 - accuracy: 0.7004 - val_loss: 0.7463 - val_accuracy: 0.6698\n",
      "Epoch 6/20\n",
      "1093/1093 [==============================] - 249s 228ms/step - loss: 0.6720 - accuracy: 0.7152 - val_loss: 0.7509 - val_accuracy: 0.6760\n",
      "Epoch 7/20\n",
      "1093/1093 [==============================] - 241s 221ms/step - loss: 0.6377 - accuracy: 0.7341 - val_loss: 0.7563 - val_accuracy: 0.6828\n",
      "Epoch 8/20\n",
      "1093/1093 [==============================] - 229s 209ms/step - loss: 0.6022 - accuracy: 0.7538 - val_loss: 0.7635 - val_accuracy: 0.6823\n",
      "Epoch 9/20\n",
      "1093/1093 [==============================] - 254s 232ms/step - loss: 0.5783 - accuracy: 0.7634 - val_loss: 0.7689 - val_accuracy: 0.6839\n",
      "Epoch 10/20\n",
      "1093/1093 [==============================] - 224s 205ms/step - loss: 0.5435 - accuracy: 0.7800 - val_loss: 0.8061 - val_accuracy: 0.6839\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=20,\n",
    "                    callbacks=[stop_early, model_checkpoint_callback],\n",
    "                    validation_data=val_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Metrics for training SNLI dataset, 70K samples (glove 100d)')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABMm0lEQVR4nO3dd3hUVfrA8e+bRgiEHmoIoYn03pUiFuwiqCAKqMhiW9dV176irrusvf+UtYGAHeyiogIivTdRKYGE3gk95f39cW5wiCkTmMmkvJ/nyZOZO7e8986deeece+45oqoYY4wxgRAW6gCMMcaUHJZUjDHGBIwlFWOMMQFjScUYY0zAWFIxxhgTMJZUjDHGBIwlFY+I3C8irwd4nf8SkZ0isjWQ6z0VIvK1iAwN9LzFlYi8LSL/CnUcxn8i0ktEUoKw3jIiskpEavo5v4pIo0DHUVSISJKInO09/quIjPZnuSKdVLydOiYi1bJNX+K9oYl+rMOvE1BV/62qw08h3OzbrQvcCTRTVb9OUj/Weconsaqer6pjAz1vQXlJfL2IHBCRFBF53+e1aSJyxDuGWdPOFpEkn+fHT/hs6w3KF45PXAE7RwK1He9YHvD5OywimVmfG+/L8k0R2S8iW0Xk7z7LJnrnVYT3XETkRRFZLSJ1Ar93RdoIYIaqFpkfgdmJyGMislxE0kVkVA6vXy0iG0TkoIh8IiJVfF7L9TzwwxjgGhGpnt+MRTqpeNYDg7KeiEhLoGwgN5D1gQqwesAuVd1eWPEEaT8Cziv9XAucrarlgQ7A99lmOwg8VNixFUfeD6LyWX/Af4FpqrrTm2UU0Bh3TvYG/iEifbOvR0QEeA3oBfRU1U2FEX8R8hfgnVAHkY81wD+AL7O/ICLNce/ftUAN4BDwis8so/DjPMiJqh4BvgaG+DNzkf0DkoAHgfk+054CHgAUSPSmlfGmbwS2Aa/iEk854DCQCRzw/mp7B/cjYDywHxjuTRvvs50zgFnAXiAZGOZNvwBYBaQCm4C7coj77Gzbfdubfgmw0lvnNKBptn29B1gGHAUisq1zhrfPB711XoX78Kd4y23FfSAqA18AO4A93uN4n/VMA4Z7j4cBM71jtweXwM8/yXnrezGmAlOBl32PZ7Z9eQl4Lo/3fRrwsLeuRj7HNCnb8To7h2V7ASl5rLstsMhb9/vAe8C/vNdyPXbA40AGcMQ7/i9505/3zo/9wELgTJ9tdQIWeK9tA57xea0Lf5xfS4FeeW2nAJ8ZAdYCQ32mbQLO9Xn+GPCe9zjRO6/KAGO9Y1M1j/XntU8f4s7Dfd650NzntbdxX3Bfe/v1M1ATeM471quBttne3/twn7U9wFtAdE7vMe4z/bH3vq0H/upPvNn2KwH3mY3wmVYV+Nxbdj7wL2Cmz+vKH+dnRWCcF8MG3PdWmHdc9wItfJaL87ZV3Xt+EbDEm28W0MqP93k8MCrbtH8DE32eNwSOAbH5nQfe82u92HfhvmOT8PmMAYOBH/ONrSAnbGH/Ze0U8CvQFAjHfYDrcWJSeQ74DKgCxHonwn9yOgG9aaOANOAy740vi09S8U6wVFwJKdI7udp4r23B++LAfQm1yyX27Cf+abiEcI63zn/gfnVE+ezrEqAuUDaXdR4/iX22kY77ZVrG24+qQH8gxjsWHwKf+CwzjRMTRRpwo3dsbwI2A3IS887GJZwoXELeT+5J5RpgN3A3rpQSnu31abhE/4zPe3LKScWLbQNwh/ceDPD2KSup+H3ssu1LVSACV925lT++/GYD13qPywNdvMd1cB/cC3Dn3zne87jctlOAz0wP3Jd2eZ9zVIEaPvMMAJZ7jxO91z8C5gKV8ll/jvvkPb/eO25lcJ/JJT6vvQ3sBNoD0cAPuAQwxDuf/oXPF5b3/q7AfR6q4JJQ1vt0/D32jt9C4J/e+9sAWAecl1+82fbrQmBltmnveX8xQDPcd09uSWUc8Km3/4nAb8AN3mtvAo/7LHcLMMV73A7YDnT2jsNQb9/L5PM+5JRUPgXuyTbtgHfM8zsPmnnz9vDev2dw3y2+SaUdsDu/c7A4VH+B+wU+BPfhW43LuMDxIvuNwB2qultVU3EZe2A+65ytqp+oaqaqHs722mBgqqq+q6ppqrpLVZd4r6UBzUSkgqruUdVFfu7DVcCXqvqdqqbhvoDLAt185nlBVZNziCcvmcDDqnpUVQ97sX6sqoe8Y/E40DOP5Teo6v9UNQP3S7UWrujs97wikgB0BP6pqsdUdSYuyedIVccDtwHnAdOB7SJybw6z/ge42CvWB0IXXDJ5zntfP8L9As2Kq6DHDlUd7y2XrqpP4z6QTbyX04BGIlJNVQ+o6hxv+jXAV6r6lXf+fYf7NX1BAPZxKPCRqh7wnpf3/u/zmWcf7svP17nAB6q6N5/157ZPqOqbqpqqqkdxP9Jai0hFn2Unq+pCdVUpk4EjqjrOO5/ex5Uifb3kfR52496LQfxZR1wyftQ799YB/+OPz3+u8WZTCfdDEgARCcf9wHjYOx9W4c75P/HmvQq4z9v/JOBp3C9/gInZYr/amwbuu+s1VZ2rqhnqrmEexZ2rBVWeE99n+OO9zu88GAB8oaozvPfvIdx3i69UXIksT8UpqVyN+7U8LttrcbhfEgtFZK+I7AWmeNPzkpzHa3VxVQg56Y/78G8Qkeki0jWf7WSpjfuVDICqZnox+F4MzSum3OzwPqQAiEiMiLzmXazbj6uGqOSd+Dk5flFSVQ95D8sXcN7auF8wh3zmzXNfVHWCqp6N+zCPBB4VkfOyzbMDV1X2aF7rKoDawCb1fnZ5jr8nJ3HsEJE7ReQXEdnnnXsVgayGJTfgSqirRWS+iFzkTa8HXJF1vnrLnYFL0idNRMoCV3Dil19WcqngM60CPl+gnouAh0Xk+nw2k+M+iUi4iIwWkbXesUvy5vdtZLPN5/HhHJ5nP+98z6ENuPcvu3pA7WzH8n7++GGU23uQ3R5OTLRxuNKnbwy5ndPV+KMU7Btv1mf7B6CsiHQWkXpAG1xSzYr/zmzx181lX/NzgBPfZ/jjvc7vPKiNz/6p6kFc6dlXLH9OWn9SLJKKqm7AFZUvACZle3kn7oRsrqqVvL+K6i5Ygivy5bjaPDaZjKuPzCmW+ap6KVAd+AT4wL+9YDPuBAKOl7Dq4lPqyiem3GRf5k7cL+XOqloBV5wFV9ceLFuAKiIS4zOtbm4z+/JKDB/iriW1yGGWJ3EXFdufcpQuzjresc+S4PM4v2N3wrEWkTNx17OuBCqraiXch04AVPV3VR2EO1f+C3wkIuVw59c7PudrJVUtp6qjc9pOAVyOq1acljVBVfd4+93aZ77WuGt7vmYBFwPPi8jVuW0gj326GrgUV01ZEVcFBKd23vmeQwm4z1B2ycD6bMcyVlUvyCfe7JYBDXwau+zAVf/E5xKPr524ElE9n2kJeJ9t7wfkB7jSytW4EkHWl3kyrmrMN/4YVX03l23lZSU+77OINMCVnH/z4zzY4rt/3me5arb1N8Vd/8tTsUgqnhuAs7wMepz3hv0PeDaruZuI1PH51bsNqJqtGJ6fCcDZInKliESISFURaSMiUSIyWEQqelVY+3EXVf3xAXChiPQRkUjcF9hR3IfZX9twdcZ5icUl2b1ec8KHC7D+k+Il/QXAKO8YdcV9QeVIRIaJyIUiEisiYSJyPtAcV6effd17cVUJ/8hhVZEiEu3zl1/rt9m4L4q/eu/r5bgLuVnyO3bZj3+st74dQISI/BOfX4Iico2IxHnn6F5vcgauPvxiETnP+4UfLa4pdNYX2J/eZ3HNjEfls39DgXHZSmLgSvcPikhlETkdV+XydvaFVXU6LjGNEZEBOW0gj32KxZ3Pu3A1B//OJ1Z/3CIi8d57cT+uiiy7ecB+EblHRMp6x7OFiHTMJ94TqGoK8Dve+eBVyU3CndMx3nHLseWTN+8HwOPeOV0P+Dvufc4yEVdFNpg/qr7AfXeN9EoxIiLlsj4bOW1LRCJFJBr33R3hnTtZJekJuPPqTC9xPgpM8klgeZ0HHwEXicgZIhLlLZs9P/TENbTIU7FJKqq6VlUX5PLyPbiL3nO8ovdUvHptVV0NvAus84qX+RYrVXUjrlR0J+6X3xL+yPDXAknedkbi6sf9if9Xb94Xcb9sLgYuVtVj/izvGQWM9fbjylzmeQ53rWYnMAdXFVgYBgNdcV8q/8J9ARzNZd79uC+JjbgP+hPATd61mJw8T87J+ytcEsj6G5VXgN6xvhxXjboH9yH3Lfk+R97H7nlggIjsEZEXgG9wH7LfcNUdRzixiqQvsFJEDnjLDlTVI6qajPtVfz8uISXjGi2E5bIdcL8if85t38TdU3IWf64eBpcc13oxTgeeVNUczwvv+s5VwNsiktMPgxz3ydvuBtyv81W443eqJgLf4i68r8OdV9njzcB9ltrgajN2Aq/zR91/bvHmJKs5bpZbvfVktax8l9zP6dtwDXHW4VpJTsRdoM+Kc673em18vpi977QbcdW8e3DfY8Ny2Qa4JHQYV+p5wHt8rbeulbjvpAm4i/+xwM0+y+Z6HnjL3uLFvcWL5fj9Xl4iu4Bcriv5kj//qDHm1Im7mXG1qga9pFTSeSWYD1XV3+t3xZ64G12Hq+rUQtxmGWAx0EdVt+Tw+n+Bmqo6tLBiKipE5DagrqrmVGNwgmJxs5wp+rzqht24X4vn4n6J+9Wtg8mbVzVTahJKqHitnpplPfeqiKKA5bhWZjfgmrqXOqr6or/zWlIxgVITV5VUFVdsvklVF4c2JGNOSSyuyqs2rjrpady9ICYPVv1ljDEmYIrNhXpjjDFFX4mq/qpWrZomJiaGOgxjjCk2Fi5cuFNV87tZ3G8lKqkkJiayYEFurY6NMcZkJyIb8p/Lf1b9ZYwxJmAsqRhjjAkYSyrGGGMCpkRdUzHGlFxpaWmkpKRw5EhuvayYvERHRxMfH09kZGRQt2NJxRhTLKSkpBAbG0tiYiIndjRt8qOq7Nq1i5SUFOrXrx/UbVn1lzGmWDhy5AhVq1a1hHISRISqVasWSinPkooxptiwhHLyCuvYlfqkkpaRyavT17Jww55Qh2KMMcVeqU8qx9IzGTsriQcmLyctI/uQzMYYA3v37uWVV145qWUvuOAC9u7d6/f8o0aN4qmnnjqpbRUFpT6plCsTwahLmrN6aypv/bw+1OEYY4qgvJJKRkbeg79+9dVXVKpUKQhRFU1BTSoi0ldEfhWRNSJybw6vVxaRySKyTETmiUgLn9eSRGS5iCwRkaD2vXJe85qc3bQGz373Oyl7DgVzU8aYYujee+9l7dq1tGnThrvvvptp06bRu3dvrr76alq2bAnAZZddRvv27WnevDljxow5vmxiYiI7d+4kKSmJpk2bcuONN9K8eXPOPfdcDh8+nOd2lyxZQpcuXWjVqhX9+vVjzx5XTf/CCy/QrFkzWrVqxcCBAwGYPn06bdq0oU2bNrRt25bU1NS8Vh00QWtS7I2b/DJwDm58jfki8pmqrvKZ7X5giar28wbEeRno4/N6b1XdGawYfY26pBnnPDODUZ+t4vWhHQpjk8aYk/TI5ytZtXl/QNfZrHYFHr64eY6vjR49mhUrVrBkyRIApk2bxrx581ixYsXxJrpvvvkmVapU4fDhw3Ts2JH+/ftTtWrVE9bz+++/8+677/K///2PK6+8ko8//phrrsl9RPIhQ4bw4osv0rNnT/75z3/yyCOP8NxzzzF69GjWr19PmTJljletPfXUU7z88st0796dAwcOEB0dfeoH5SQEs6TSCVijquu8scHfw40G6KsZ8D0cH0s+UURqBDGmXMVXjuGOcxoz9ZdtfLNyayhCMMYUI506dTrhno8XXniB1q1b06VLF5KTk/n999//tEz9+vVp06YNAO3btycpKSnX9e/bt4+9e/fSs2dPAIYOHcqMGTMAaNWqFYMHD2b8+PFERLiyQffu3fn73//OCy+8wN69e49PL2zB3GodINnneQrQOds8S4HLgZki0gmoB8QD2wAFvhURBV5T1THkQERGACMAEhISTing67rXZ9KiTYz6bCVnNKpGuTJ2b6gxRVFuJYrCVK5cueOPp02bxtSpU5k9ezYxMTH06tUrx3tCypQpc/xxeHh4vtVfufnyyy+ZMWMGn332GY899hgrV67k3nvv5cILL+Srr76iS5cuTJ06ldNPP/2k1n8qgllSyalRdPZhJkcDlUVkCXAbsBhI917rrqrtgPOBW0SkR04bUdUxqtpBVTvExZ3akACR4WE83q8lW/cf4dnvfjuldRljSo7Y2Ng8r1Hs27ePypUrExMTw+rVq5kzZ84pb7NixYpUrlyZn376CYB33nmHnj17kpmZSXJyMr179+aJJ55g7969HDhwgLVr19KyZUvuueceOnTowOrVq085hpMRzJ/iKUBdn+fxwGbfGVR1P3AdgLg7c9Z7f6jqZu//dhGZjKtOmxHEeAFoX68ygzol8NasJPq1q0Pz2hWDvUljTBFXtWpVunfvTosWLTj//PO58MILT3i9b9++vPrqq7Rq1YomTZrQpUuXgGx37NixjBw5kkOHDtGgQQPeeustMjIyuOaaa9i3bx+qyh133EGlSpV46KGH+PHHHwkPD6dZs2acf/75AYmhoII2Rr2IRAC/4S68bwLmA1er6kqfeSoBh1T1mIjcCJypqkNEpBwQpqqp3uPvgEdVdUpe2+zQoYMGYpCufYfS6PPMNOpUjmHSTd0ID7O7eI0JtV9++YWmTZuGOoxiLadjKCILVTVgrZOCVv2lqunArcA3wC/AB6q6UkRGishIb7amwEoRWY2r5rrdm14Dd51lKTAP+DK/hBJIFWMiefDCZixN3svEeRsLa7PGGFPsBfVKtKp+BXyVbdqrPo9nA41zWG4d0DqYseXn0ja1+XBhMk9MWc15zWtQPTY0zfOMMaY4KfV31OdGRHjs0hYcTcvkX1/8EupwjDGmWLCkkocGceW5uXdDPlu6mRm/7Qh1OMYYU+RZUsnHyJ4NqV+tHA99uoIjaXn38WOMMaWdJZV8REeG8/hlLdiw6xCv/Lgm1OEYY0yRZknFD90aVaNf2zr83/S1rNl+INThGGOKgfLlyxdoeklhScVP91/QlLKR4Tz4yXKCdW+PMcYUd5ZU/BQXW4Z7z2/KnHW7mbRoU6jDMcYUonvuueeE8VRGjRrF008/zYEDB+jTpw/t2rWjZcuWfPrpp36vU1W5++67adGiBS1btuT9998HYMuWLfTo0YM2bdrQokULfvrpJzIyMhg2bNjxeZ999tmA72OgWI+JBTCwY10+WpjM41/9Qp+m1akUExXqkIwpnb6+F7YuD+w6a7aE80fn+NLAgQP529/+xs033wzABx98wJQpU4iOjmby5MlUqFCBnTt30qVLFy655BK/xoOfNGkSS5YsYenSpezcuZOOHTvSo0cPJk6cyHnnnccDDzxARkYGhw4dYsmSJWzatIkVK1YAFGgkycJmJZUCCAsTHu/Xkn2H0xj9dWg6azPGFL62bduyfft2Nm/ezNKlS6lcuTIJCQmoKvfffz+tWrXi7LPPZtOmTWzbts2vdc6cOZNBgwYRHh5OjRo16NmzJ/Pnz6djx4689dZbjBo1iuXLlxMbG0uDBg1Yt24dt912G1OmTKFChQpB3uOTZyWVAmpaqwLDz6jPazPW0b99PB0Tq4Q6JGNKn1xKFME0YMAAPvroI7Zu3Xp8tMUJEyawY8cOFi5cSGRkJImJiTl2eZ+T3K7N9ujRgxkzZvDll19y7bXXcvfddzNkyBCWLl3KN998w8svv8wHH3zAm2++GbB9CyQrqZyE289uTJ1KZXlg8nLSMjJDHY4xphAMHDiQ9957j48++ogBAwYArsv76tWrExkZyY8//siGDRv8Xl+PHj14//33ycjIYMeOHcyYMYNOnTqxYcMGqlevzo033sgNN9zAokWL2LlzJ5mZmfTv35/HHnuMRYsWBWs3T5mVVE5CTFQEj1zSnOHjFvD6T+u5qVfDUIdkjAmy5s2bk5qaSp06dahVqxYAgwcP5uKLL6ZDhw60adOmQINi9evXj9mzZ9O6dWtEhCeeeIKaNWsyduxYnnzySSIjIylfvjzjxo1j06ZNXHfddWRmuh+x//nPf4Kyj4EQtK7vQyFQXd/7a8S4Bcz4fQff3dGTulViCm27xpRG1vX9qSvWXd+XBqMuaU64CP/8dIXdu2KMMVhSOSW1K5XljnNO48dfdzBlxdZQh2OMMSEX1KQiIn1F5FcRWSMi9+bwemURmSwiy0Rknoi08HfZomJYt0Sa1arAqM9XcuBoeqjDMaZEsxqBk1dYxy5oSUVEwoGXcSM6NgMGiUizbLPdDyxR1VbAEOD5AixbJESEh/F4vxZsTz3K09/+GupwjCmxoqOj2bVrlyWWk6Cq7Nq1i+jo4A82GMzWX52ANd4ojojIe8ClwCqfeZoB/wFQ1dUikigiNYAGfixbZLRNqMzgzgmMnZVE/3bxtKhTMdQhGVPixMfHk5KSwo4dNrbRyYiOjiY+Pj7o28k3qYhIQyBFVY+KSC+gFTBOVffms2gdINnneQrQOds8S4HLcePRdwLqAfF+LpsV3whgBEBCQkJ+uxM0d593Ot+s3Mb9k5cz+ebuhIfl302DMcZ/kZGR1K9fP9RhmHz4U/31MZAhIo2AN4D6wEQ/lsvpWzV7uXU0UFlElgC3AYuBdD+XdRNVx6hqB1XtEBcX50dYwVGxbCQPXdSMZSn7GD/H/xugjDGmJPEnqWSqajrQD3hOVe8AavmxXApQ1+d5PLDZdwZV3a+q16lqG9w1lThgvT/LFkUXt6rFmY2r8eQ3v7Jtv39dNRhjTEniT1JJE5FBwFDgC29apB/LzQcai0h9EYkCBgKf+c4gIpW81wCGAzNUdb8/yxZFIsJjl7bgWEYmj35RJC//GGNMUPmTVK4DugKPq+p6EakPjM9vIa90cyvwDfAL8IGqrhSRkSIy0putKbBSRFbjWnrdnteyBdu10EisVo5bezfiy2VbmPbr9lCHY4wxhapA3bSISGWgrqouC15IJ6+wu2nJzdH0DM5//ifSMjL57o6eREeGhzokY4zJUaF30yIi00SkgohUwbXWektEnglUACVRmYhwHr+sJcm7D/PiD7+HOhxjjCk0/lR/VfSuc1wOvKWq7YGzgxtW8de1YVUub1eHMTPW8fu21FCHY4wxhcKfpBIhIrWAK/njQr3xwwMXNKVcmQgemGwdThpjSgd/ksqjuAvma1V1vog0AKxOxw9Vy5fhvvNPZ17Sbj5cmBLqcIwxJujyTSqq+qGqtlLVm7zn61S1f/BDKxmuaF+XDvUq85+vfmH3wWOhDscYY4LKnwv18V5PwttFZJuIfCwiwe9ApjDtXAOZGUFZdViY8Hi/lqQeSec/X/0SlG0YY0xR4U/111u4Gw9r4/rk+tybVjIc2g1vnAPjL4cDwemorknNWIaf2YAPF6Ywd92uoGzDGGOKAn+SSpyqvqWq6d7f27juVEqGspXhnEdg4xx47UzYMCsom7m9T2PiK5flgU9WcCw9MyjbMMaYUPMnqewUkWtEJNz7uwYoOT+3RaDdEBg+FSLLwtsXwcznIMCttcpGhfPYpS1Ys/0A//tpXUDXbYwxRYU/SeV6XHPircAWYIA3rWSp2RJGTIemF8HUh+HdQXB4T0A30fv06pzfoiYvfP87G3YdDOi6jTGmKPCn9ddGVb1EVeNUtbqqXqaqJbNv9+gKcMVY6PtfWDMVXusBmxYGdBMPX9yciDDhn5+utHtXjDElTq6DdInIi+QyhgmAqv41KBGFmgh0GQnxHeDDYfBmXzjv39BxuHvtFNWsGM2d5zbh0S9W8eXyLVzUqvapx2yMMUVEXiWVBcDCPP5KtvgO8JcZ0KAXfHUXfHwDHA1MdytDuyXSok4FHv18FfuPpAVkncYYUxQUqJfioi4ovRRnZsLPz8IP/4IqDeDKcVCj+SmvdlnKXi59+WeGdKnHI5e2CECgxhhTcIXeS3GpFxYGZ94JQz93JZX/9YHFE055ta3iKzGkSz3GzdnA0uS9px6nMcYUAUFNKiLSV0R+FZE1InJvDq9XFJHPRWSpiKwUket8XksSkeUiskREQj9ISuIZ8JefXLXYpzfDp7fAsUOntMo7z2tCXPky3D95OekZdu+KMab486eblions2IRCQdexo3o2AwYJCLNss12C7BKVVsDvYCnfYYXBuitqm0CWTQ7JbE1YMin0ONuWDweXj/bdfFykipER/LPi5uxcvN+xs0umQ3qjDGliz8llbki8qGIXCBSoOZPnYA1XgeUx4D3gEuzzaNArLfe8sBuIL0A2yh8YeFw1oMw+GNI3QJjesHKySe9ugtb1qLnaXE8/e2vbNl3OHBxGmNMCPiTVE4DxgDXAmtE5N8icpofy9UBkn2ep3jTfL2EG6d+M7AcuF1Vs+qBFPhWRBaKyIjcNiIiI0RkgYgs2LEjOH135ajx2TDyJ6h+umt6/NXdkH60wKsRER67tAXpmcqjn68KfJzGGFOI/Ln5UVX1O1UdBAwHhgLzRGS6iHTNY9GcSjXZm5qdByzBdVbZBnhJRCp4r3VX1Xa46rNbRKRHLvGNUdUOqtohLq6QuySrGA/DvoIut8C8Me6elr0bC7yahKox/LVPY75esZXxc6wazBhTfPlzTaWqiNzuXSy/C7gNqAbcCUzMY9EUoK7P83hcicTXdcAkL3GtAdYDpwOo6mbv/3ZgMq46reiJiIK+/4Yr34Fda+DVM+HXKQVezY1nNqBXkzge/GQFz3z7q91tb4wplvyp/poNVAAuU9ULVXWS11vxAuDVPJabDzQWkfrexfeBuC70fW0E+gCISA2gCbBORMqJSKw3vRxwLrCiIDtW6JpdAiOmQaW68O5V8N3DkOH/5aGoiDD+N6QDV3aI54Uf1nDXh8tIsxZhxphiJtduWnw0UVUVkQoiEquqx28rV9X/5raQqqaLyK24oYjDgTdVdaWIjPRefxV4DHhbRJbjqsvuUdWd3pDFk712ARHARFUt+M//wla1IdzwHUy5F35+DlLmQ/83oEItvxaPDA/jv/1bUbtSWZ6b+jvbU4/wyuB2xEZHBjduY4wJkHzvqBeRDrhBuWJxX/x7getVtch11RKUO+pP1tL34Yu/QVQ56P+66+6lAD6Yn8x9k5fTpEYsb13XkRoVooMSpjGmdAvFHfVvAjeraqKq1sPdW1JyRn4MltZXwY0/QtkqMO4ymP6E6/LFT1d2rMsbQzuQtOsgl78yi9+3BabfMWOMCSZ/kkqqqv6U9URVZwL2DeeP6qfDjT9Ayyvgx8dhwgA4uNPvxXs1qc4Hf+nK0fRM+v/fLBuK2BhT5PmTVOaJyGsi0ktEeorIK8A0EWknIu2CHWCxV6Y8XD4GLnoWkn5yrcM2zvV78RZ1KjL55m5Uiy3DtW/M44tl2RvQGWNM0eHPNZUf83hZVfWswIZ08orUNZWcbF4CHw6FfSlw9iPQ9Ra/x2jZe+gYw8cuYMGGPTx4YVOGn9kguLEaY0qFQF9Tsa7vC9vhva4zytVfwOkXwaUvQ9lKfi16JC2DO95fwtcrtnJd90QevLAZ4WGnPnCYMab0KvQL9V5Pws9kdYUiIk+LSMVABVDqlK0EV42Hcx+H36bAmJ6uBOOH6MhwXr66Hdd3r89bPydx68RFHEnLCGq4xhhTEP62/koFrvT+9mOtv06NCHS71XXxkpEGb5wLC94EP0qNYWHCPy9uxoMXNmXKyq1c8/pc9hw8VghBG2NM/vxJKg1V9WGvt+F1qvoIYBX6gZDQ2Y3RkngGfHEHTLrRVY/5YfiZDXhpUDuWbdpH/1dnkbz71MZ2McaYQPAnqRwWkTOynohId8D6aA+UclVh8EfQ+0FY8TG80AZmv+xXj8cXtqrF+Bs6s+vAMfq9MovlKfuCH68xxuTBn6QyEnjZG4kxCddd/V+CGlVpExYGPe92fYfVagPf3A8vdYBlH+R7w2Sn+lX4+KaulIkI46oxs/nx1+2FErIxxuQkz6Tijd54jTcyYyuglaq2VdVlhRJdaVOrNQz5BK6dDNEVXXXYmJ6wNq9W3dCoeiyTb+5G/WrlGD52Ae/PL3j3+8YYEwh5JhVVzQDae4/3q+r+QomqtGt4FoyYAZf/z11jeecyeKcfbMk9l1evEM37f+lK90bVuOfj5Tz73W/Wfb4xptD5c/Pj00Bj4EPgYNZ0VZ0U3NAKrljcp1JQaUdg/usw40k4sg9aXQVnPQCVEnKePSOT+yct58OFKVzRPp5/X96SyHB/ajmNMaVRoO9T8afr+yrALsD3znkFilxSKZEio13z47bXwMxnYe6rsHISdBoBZ94JMVVOnD08jCcGuO7zn//+d7alHuWVwe0oX8aft9oYY06NPyWV7qr6c37TioISWVLJbl8K/PhvWDIRoiu4xNLpLy75ZPP+/I3cP3kFTWvF8uawjlSPte7zjSlxVCF1q9/jNmUXiq7vX/RzmikMFePhslfgpp+hbmf47p/wYnuXZDJPvLv+qo4JvD60A+t2uO7z12w/EKKgjTFBkTQT3jgH3uoL6UXjJuhck4qIdBWRO4E4Efm7z98o3EiO+RKRviLyq4isEZF7c3i9ooh8LiJLRWSliFzn77KlXo3mMPhDGPo5lI+DT25yPSD/PvWEO/N7N6nO+yO6ciTNdZ8/P2l3CIM2xgTElmUwvj+8fSHs2wRn/B2kaFw7zSuKKKA87rpLrM/ffmBAfiv2miO/DJwPNAMGiUizbLPdAqzymiz3Ap4WkSg/lzUA9XvA8B9gwJuQdhAm9Idxl8DmxcdnaRnvus+vWi6Kwa/P5evlW0IYsDHmpO1aCx/dAK+dCSkL4JxH4a+LoP1QCC8a101zjUJVpwPTReRtVd1wEuvuBKxR1XUAIvIecCmwynczQKy4wejLA7uBdKCzH8uaLGFh0KI/nH6x60NsxhMwphe0GAB9HoLKidStEsPHN3Vj+LgF3DxxEQ9d2Izrz6gf6siNMf5I3epGj100FsIi3bXUbn/1u4fzwuRPaisjImOARN/5/RhHpQ6Q7PM8BZcsfL0EfAZsxpWCrlLVTBHxZ1kARGQEMAIgISHnZralRkQUdBkJba6Gn5933b2s+hQ6Doced1O5XFUmDO/M7e8t5tEvVrF572Huv6ApYdZ9vjFF0+G9MOsFmPN/kHEM2g2Fnv+A2JqhjixX/iSVD4FXgdeBgvSzntM3VfamZucBS3DNlRsC34nIT34u6yaqjgHGgGv9VYD4Sq7oCq6E0vEGmPYfmPcaLJkAZ/yN6M438crg9jz2xSpen7meLfuP8PQVrYmO9OsymTGmMKQdhnlj4Kdn4MheV+vQ+36o2jDUkeXLn6SSrqr/dxLrTgHq+jyPx5VIfF0HjFbXrnmNiKwHTvdzWZOfCrXhkhehyy3w/SPw/aMw73XCe9/HwxdeTZ1KZXn8q1/Ysf8oY4a0p1JMVKgjNqZ0y0h3PwCnjYbUzdDobOjzT9eFUzHhT3OBz0XkZhGpJSJVsv78WG4+0FhE6otIFDAQV9XlayPQB0BEagBNgHV+Lmv8Vf10GPQuXPe1SzSf3Ya8egY31viNFwa2YUnyXga8OpuUPdZ9vjEhoQorP4FXusDnf4WKdWDoF3DNx8UqoYB/Nz+uz2Gyqmq+Y6qIyAXAc7gmyG+q6uMiMtJbwasiUht4G6iFq/Iararjc1s2v+2VipsfT5Uq/PIZTH0Edq+Fet1Z0exOrv46nTKR4bw1rCMt6tjAnsYUmnXTYOoo12Iz7nRXMmlygRvMrxDYGPV5sKRSABlpriXJtNFwcAepDS7khuTzWXk0jv+7pj09TosLdYTGlGybF7sfd+t+hArx7ppJ64EQVrjXNws9qYhIDPB3IEFVR4hIY6CJqn4RqCACxZLKSTiaCrNeglkvohlH+TzyPP6VejFX9mzHrWc1sgv4xgTazjXww2Ow6hMoWwV63A0drs+xq6XCEIqk8j6wEBiiqi1EpCwwW1XbBCqIQLGkcgpSt8H0/6IL3yZT4ZBGoRJB2bJliIwsA2EREB4F4ZHe40jXXj488sTHfs3nzXPCMpHeNJ9lKtQqdvXJxuRq/2aY/l9Y9A5EeB3Fdr3VtdYMoVD0UtxQVa8SkUEAqnrYu1nRlCSxNeCiZ5AuNxG+9D32bd/J3DXbOHrgKE2rl6VlzRgiSHfVZplZ/9Pc/7TD3uN07/8xn8dZ8x9zj7UgrdJxPQb0fgASugRnv40JtkO74efnYO5rrn++jsOhx11QvnqoIwsKf5LKMa90ogAi0hDIfwB1UzxVawx9HiIeqHw0nSe/+ZUHZidR+2BZ/n15S3qe6rWWzEyXZPJLPplpsGG2+zC+eR407OPqnOMD9oPKmOA6dsgNVfHzc3BkvxsLqfd9UDkx1JEFlT/VX+cAD+L64PoW6A4MU9VpQY+ugKz6KzgWJO3mno+XsXbHQfq3i+ehi5oW3j0txw66QcpmPgeHd0Pj89wHs3bbwtm+MQWVkQaLxrluVQ5shdP6wlkPQc0WoY4sRyFp/SUiVYEuuGa/c1R1Z6ACCCRLKsFzJC2Dl35Yw6vT11IpJorHLm3O+S1PbvyGk3I01VUfzHrR3WF8+kXQ616o2bLwYjAmL5mZsGoy/PAv2L0O6naBs0dBva6hjixP1qQ4D5ZUgm/l5n3c8/EyVmzaz3nNa/DYpS2oXqEQW60c2QdzXoXZL8HR/dDsUuh1H1RvWngxGONLFdZ+75oHb10G1ZtBn4fhtPMK7V6TU2FJJQ+WVApHekYmr89cz7Pf/UaZiDAevLAZV3SIp1Dbbxze4zrMnPN/roqsRX9XcqnWuPBiMCZlgbtxMeknqJQAvR+ElgMK/V6TU2FJJQ+WVArXuh0HuHfScuat380Zjarx734tSagaU7hBHNzlenGdNwbSj7iLoT3uLhYd75kiJu2Iu253aJfPX27Pvf/phyGmmus5uP0wiCgT6r0osFDcp9IQSFHVoyLSC2gFjFPVvYEKIlAsqRS+zExl4ryNjP56NRmZyl3nNWFYt0TCC7s7/QM7XCub+a+7C6VtBrnkUsJb2phcpB/zEkR+ScKbdng3HMtjuO3oihBTNdtfFahUz90FXya28PYtwEKRVJYAHXDjqXyD69ixiapeEKggAsWSSuhs3nuYBz9ZwQ+rt9M2oRJP9G9F4xoh+KClboWZz8KCt9w9MW2vdfcEVIwv/FhM4GRmwqGdkLoF9m9xraryShJH9+e+rjIVXEKIqeruaPdNEjkljrKV3Q25JVQoksoiVW0nIncDR1T1RRFZrKpFrk2nJZXQUlU+W7qZUZ+t5MDRdG7t3ZibejUkKiIEY2fv2wQzn4GFY93F0nZD3Wh5FQqxxZrJn6pLAKlb3R3nqVtd4sj627/FTTuw1d3HlF1kubwTQvbpZau4wezMcaFIKnNxvQU/AFysqutFZIWqFrlG15ZUioZdB47yyOer+GzpZprUiOWJAa1oXbdSaILZmww/PQWLx4OEu4HLuv/N9SBggivtiEsGJySMrP8+09IO/nnZ6IoQW8vnr6YbtiG2JsTWdu9fTLWQ9ZdVkoQiqTQDRuL6+3pXROrjhv0dHaggAsWSStEyddU2HvxkBdtTj3DDGfX5+zlNKBsVolYxu9fDjKdg6buuj7FOw11yKVctNPEUZ5kZcHCHT0nC92/rH9MO7/7zsuFlXGnRN2FU8EkcWf+jyhX+fpVSIW39JSKVgbqquixQAQSSJZWiZ/+RNEZ/vZqJczeSUCWG0f1b0q1hCL/Id611nfot/xAiykLnv0C321w1SWmkCmmHsl2b8Hl8OIfpB7b/uQ83CYPyNXxKEjWzJQtvWtnKxeLejdIkFCWVacAluH7ClgA7gOmq+vd8Vy7SF3geN9DW69lLN951msHe0wigKRCnqrtFJAlIBTJwQxrnu9OWVIqu2Wt3ce+kZWzYdYhBnRK474LTqRAdwoufO351yWXFJIgqD11vhi43Q9lKoYspENIO597iKbfmsulHclmZ5HBBu4qXJGqeWNooX71Y3Zth/hCKpLJYVduKyHBcKeVhEVmmqq3yWS4c+A04Bzfm/HxgkKquymX+i4E7VPUs73kS0KEgXcJYUinaDh/L4Lmpv/G/n9YRF1uGf13WknOahfjaxrZVMO0/bjTMMhVdd+SdR4a8O3IyM+HoPji813VLc2TfifdI5JYg0vIYEjq6Ui4Xs3O50B1d0RJFKRCKru8jRKQWcCXuYr2/OgFrVHUdgIi8B1wK5JhUgEHAuwVYvylmykaFc98FTbmwVS3+8dEybhy3gIta1WLUJc2pVj5EN43VaAZXvQNblrlRMH98HOa8At3+Cp1GQJnyJ7/ujLQTk0LW48N73PMje3N5fZ/XJDaPH3xlKv6RCMrXhOrNc27tdDxBVHLj2BgTZP6UVK4AHgJ+VtWbRKQB8KSq9s9nuQFAX1Ud7j2/FuisqrfmMG8MrjTTSFV3e9PWA3twn6zXVHVMLtsZAYwASEhIaL9hw4Y898cUDcfSM3lt+lpe/GENMWXCefjiZlzWpk7hdvWSk02LXMnl92/dl3H3v0Hzfu7GuONf+nvzSQre85xaNfmKiHZf9mUruf/RFf94nNO041VRJfu+CVO4ik03LV4yOi9bUumkqrflMO9VwDWqerHPtNqqullEqgPfAbep6oy8tmnVX8XP79tS+cfHy1i8cS+9m8TxeL+W1K5UNtRhQfJ8mPZvWPtD3vNFxeaRFCrmniCiK1pzWFMkFHr1l4jEAy/ixlFRYCZwu6qm5LNoClDX53k8sDmXeQeSrepLVTd7/7eLyGRcdVqeScUUP41rxPLRyG6Mm53EE1N+5ZxnpnPv+aczuHM9wgq7qxdfdTvCtZMheR5sW5FDgqjkplmVkjEn8Kf66ztgIvCON+kaYLCqnpPPchG4C/V9gE24C/VXq+rKbPNVBNbjGgEc9KaVA8JUNdV7/B3wqKpOyWubVlIp3pJ3H+L+ycv56feddEqswn/6t6Rh3Clc0zDG5CvQJRV/+s+IU9W3VDXd+3sbyHdMWVVNB27F9Rf2C/CBqq4UkZEiMtJn1n7At1kJxVMDmCkiS4F5wJf5JRRT/NWtEsO46zvx5IBWrN66n77PzeDhT93Nk8aY4sGfkspU4G3+qJ4aBFynqn2CG1rBWUml5NieeoRnv/udDxYkExUexnXdE/lLj4ZUjLEL1MYEUijuU0kAXgK64q6pzMJdUylyzawsqZQ8STsP8sx3v/HZ0s1UiI7gLz0bcl33RGKi7FqGMYFQqEnFu4FxrKpeE6gNBpMllZJr1eb9PP3tr3y/ejvVypfhtrMaMahTQmh6QDamBCnUayqqmgHEiYj1FW1CqlntCrwxrCMfjexKg7hyPPzZSs56ehofLUwhI7PkjF5qTHHnT/XXa0A73OBcxy+mq+ozwQ2t4KykUjqoKjN+38mT36xmxab9NKpenrvOPY3zmtcM/c2TxhQzoeimZbP3FwYU3zEzTYkhIvQ8LY4ejavx9YqtPPXtr4wcv4hW8RW5+7wmnNGomiUXY0IkaHfUh4KVVEqn9IxMJi3exPNTf2fT3sN0bVCVu/s2oV1C5VCHZkyRV+j3qYjIdyJSyed5ZRH5JlABGHOqIsLDuLJDXX64qycPX9yM37alcvkrsxg+dgGrt+YxVrkxJuD8vflxb9YTVd0DVA9aRMacpDIR4VzXvT4z/tGbu849jbnrdnH+8z/xt/cWs2FXPp07GmMCwp+kkuHdqwKAiNQjzz65jQmtcmUiuPWsxvx0T2/+0qMhU1Zupc/T03lg8nK27be7840JJn9af/UFxgDTvUk9gBGqWuSqwOyaisnJtv1HePGH33lvXjLhYcKwbomM7NmQyuWspbwxIen6XkSqAV0AAWYXZDTGwmRJxeRl465DPDf1NyYv2UT5qAhu7NGA68+oT/kydne+Kb2KzXgqoWBJxfjj162pPP3tr3y7ahtVy0Vxc+9GDO6cQHSkDZ1rSh9LKnmwpGIKYvHGPTz5za/MWruL2hWjuf3sxvRvF09EuHX9YkqPUHR9b0yJ1DahMhNv7MKE4Z2JqxDNPR8v59znZvDlsi1kWtcvxpyUXCuTRaRKXgtmjSVvTHHXvVE1PmlYlW9XbeOpb37llomLaF67Anef14Sep8XZ3fnGFECu1V8ish7XdDinT5SqaoN8V+5ajj0PhAOvq+robK/fDQz2nkYATXH3xezOb9mcWPWXOVUZmcqnSzbxzHe/kbLnMJ0Sq3DnuafRuUHVUIdmTFAUm2sqXrf5vwHn4Marnw8MUtVVucx/MXCHqp5V0GWzWFIxgXIsPZP352/khR/WsCP1KB0TK3Nzr0b0amIlF1OyhKJDSUSkMtAYiM6apqoz8lmsE7BGVdd563gPuBTILTEM4o/RJQu6rDEBFRURxrVdExnQvi7vz9/ImBnruO7t+TStVYGbejXkwpa1CA+z5GJMdv70/TUcmIEba/4R7/8oP9ZdB0j2eZ7iTctpGzFAX+Djk1h2hIgsEJEFO3bs8CMsY/xXNiqcYd3rM+3u3jw5oBXH0jP467uL6fP0NN6dt5Gj6RmhDtGYIsWf1l+3Ax2BDaraG2gL+PPtneO1mFzmvRj42efiv9/LquoYVe2gqh3i4uL8CMuYgouKCOOKDnX57o6evHpNO2KjI7lv0nJ6PPEj/5uxjoNH00MdojFFgj9J5YiqHgEQkTKquhpo4sdyKUBdn+fxuHFZcjKQP6q+CrqsMYUmLEzo26IWn93anXdu6ESDauV5/Ktf6P7fH3j2u9/Yc/BYqEM0JqT8uaaS4nV9/wnwnYjswb8v+PlAYxGpD2zCJY6rs88kIhWBnsA1BV3WmFAREc5sHMeZjeNYtHEPr/y4lue//53//bSOqzslMPzMBtSsGJ3/iowpYQrU+ktEegIVgSmqmu9PMhG5AHgO1yz4TVV9XERGAqjqq948w4C+qjowv2Xz2561/jKh9OvWVF6dvpbPlm4mTKB/u3j+0rMh9auVC3VoxuSq0JoUi0gFVd2f202QRfHmR0sqpihI3n2I12as5YMFKaRnZHJBy1rc1KshzWtXDHVoxvxJYSaVL1T1omw3QR7/78/Nj4XNkoopSranHuHNmUmMn7OBA0fT6dUkjpt7NaJT/Tw7qzCmUBXqzY/i7vKqq6obA7XBYLKkYoqifYfTeGd2Em/+nMTug8fsRkpTpBT6HfXeBtsHaoPBZEnFFGWHj2Ucv5Fy874jNK1VgZt7NeQCu5HShFAoeimeIyIdA7VBY0qrnG6kvM1upDQljD8llVXAacAG4CB/XFNpFfzwCsZKKqY4ycxUvl21lZd/XMvyTfuoUaEMN57ZgEGdEihno1GaQhKK6q96OU1X1Q2BCiJQLKmY4khVmblmJ6/8uJbZ63ZRKSaSoV0TGdYtkcrlokIdninhQjVGfWvgTO/pT6q6NFABBJIlFVPcZd1IOfWXbcREhduNlCboQlFSuR24EZjkTeoHjFHVFwMVRKBYUjElhe+NlOEiXN6uDiN6NKBBXPlQh2ZKmFAklWVAV1U96D0vB8y2ayrGBJ/vjZTH0jPp1SSOYd0S6dE4jjBrMWYCIBRJZTnQ0adTyWhgvqq2DFQQgWJJxZRUO1KPMnHuRsbP3cCO1KM0iCvHsG6JXN4unvJ2Ud+cglAklb8DQ4HJ3qTLgLdV9blABREollRMSXcsPZOvlm/hrZ/XszRlH7FlIriyY12GdK1HvarWx5gpuFBdqG8HnIFrTjxDVRcHKoBAsqRiSpPFG/fw1s9JfLV8Cxmq9Dm9OsO61ad7o6p2p77xWyhKKjl1VJSqqmmBCiJQLKmY0mjb/iNMmLOBCXM3suvgMRpXL8+w7on0a1uHmCirGjN5C0VSScINmLUHV1KpBGwBtgM3qurCQAVzqiypmNLsSFoGXy7bwluz1rNi034qREcwsFMC13apR90qMaEOzxRRoUgqrwKTVfUb7/m5uPHkPwCeV9XOgQrmVFlSMcbdTLlwwx7empXElBVbUVXOaVaDYd3q06VBFasaMycIRVJZkH2DWdNEZImqtslj2b7A87iBtl5X1dE5zNMLNxhXJLBTVXt605OAVCADSPdnpy2pGHOizXsPM37OBt6dt5E9h9I4vWYs13VP5NI2dYiODA91eKYICEVS+Rb4HnjPm3QVcA6utDJfVdvlslw48Js3bwpuiOBBqrrKZ55KwCzcyI8bRaS6qm73XksCOqjqTn93xpKKMTk7kpbBp0s28dbPSazemkrlmEgGdUrgmi71qF2pbKjDMyEU6KTiz1W8q4GHcWPUA8z0poUDV+axXCdgjaquAxCR94BLgVU+81wNTMoaryUroRhjAis6MpyrOiZwZYe6zF2/m7d/TuLV6Wt5bcY6+javybDuiXSoV9mqxswpyzepeCWF20SkvKoeyPbymjwWrQMk+zxPAbJffzkNiBSRaUAs7hrNuKxNA9+KiAKvqeqYnDYiIiOAEQAJCQn57Y4xpZqI0KVBVbo0qEry7kPHq8a+XL6FFnUqMKxbfS5qVcuqxsxJy3c8FRHp5nV/v8p73lpEXvFj3Tn95Mle1xYBtAcuBM4DHhKR07zXuntVa+cDt4hIj5w2oqpjVLWDqnaIi4vzIyxjDEDdKjHcd0FT5tzfh8f7teBoWiZ3fbiU7qN/4Jlvf2Xb/iOhDtEUQ/4M0vUs7gt/F4DXQ3GOX/DZpOCaImeJBzbnMM8UVT3olYhmAK297Wz2/m/H3c3fyY9tGmMKKCYqgsGd6/HtHT2YMLwzbRMq8eKPa+g++gf++u5iFm/cE+oQTTHi151Rqpqcra7VnyHq5gONRaQ+sAkYiLuG4utT4CURiQCicNVjz3qdVoapaqr3+FzgUX9iNcacHBGhe6NqdG9UjQ27DjJu9gY+mJ/MZ0s307puJa7rlsgFLWsRFeHPb1FTWvlzdiSLSDdARSRKRO4CfslvIVVNB24FvvHm/0BVV4rISBEZ6c3zCzAFWAbMwzU7XgHUAGaKyFJv+peqOuUk9s8YcxLqVS3HQxc1Y/b9fXj00uakHknjb+8voft/f+CF739n14GjoQ7RFFH+NCmuhrvX5GzcdZJvgb+q6u7gh1cw1qTYmODIzFR+WrOTt35ez7RfdxAVEUa/NnW4/oz6NKkZG+rwzCkIRZPiJqo6OFsQ3YGfAxWEMaZoCwsTep4WR8/T4lizPZU3f05i0qIU3l+QzJmNq3H9GfXpaWO8GPwrqSzKfoNjTtOKAiupGFN49hw8xsR5Gxk7K4ntqUdpGFeO67rXp3+7eMpGWZPk4qLQ7qgXka5AN+BvuBZgWSoA/VS1daCCCBRLKsYUvqwxXt6YuZ7lm/ZRKSaSqzslMKRrIjUrRoc6PJOPwqz+igLKe/P4VpruBwYEKgBjTPEWFRHGZW3rcGmb2sxP2sMbM9fxf9PXMmbGOi5qVYvrz6hPq/hKoQ7TFBJ/qr/qqeqGQornlFhJxZiiYeOuQ7w9K4kPFiRz4Gg6HRMrc8MZ9TmnWU3C7bpLkRKKDiXjgH8AzYHjZVlVPStQQQSKJRVjipbUI2m8Pz+Zt2clkbLnMHWrlGVYt/pc2SGe2OjIUIdnCF0vxe8DdwEjcePV71DVewIVRKBYUjGmaMrIVL5btZU3Zq5nftIeypeJ4MoOdbmue6INIBZioUgqC1W1vYgsU9VW3rTpWeOeFCWWVIwp+pal7OWNmev5ctkWMlU5t1lNbjizvvWSHCKhuE8layz6LSJyIa7/rvhABWCMKV1axVfi+YFtue/8poydncTEuRuZsnIrreIrcn33+tYVTDHnT0nlIuAnXOeQL+KaFD+iqp8FP7yCsZKKMcXPoWPpTFq0iTd/Xs+6HQepUaEMQ7omcnWnBCqXiwp1eCVeoVd/FSeWVIwpvjIzlem/7eCNmeuZuWYn0ZFhXN4unuu716dR9fKhDq/ECsU1lbHA7aq613teGXhaVa8PVBCBYknFmJJh9db9vDUziclLNnEsPZNeTeK44Yz6nNGoml13CbBQJJXFqto2v2lFgSUVY0qWnQeOMmHORt6Zs4GdB45yWo3yXN+9Ppe1rWOjUwZIKJLKUqCXqu7xnlcBpqtqy0AFESiWVIwpmY6mZ/DZks28MXM9q7emUqVcFFd2qMvVnRJIqGpNkk9FKJLKEOA+4CPccMBXAo+r6juBCiJQLKkYU7KpKrPX7eLtn5OY+ss2MhV6nBbH4M4J9Dm9OhHh1mqsoEJyoV5EmgFn4cZT+V5VV/m1cpG+uLFYwnEDcI3OYZ5ewHNAJLAz6/4Xf5bNzpKKMaXHln2HeX9+Mu/NS2br/iPUrBDNVR3rMrBTXWpVLBvq8IqNYtP6S0TCgd+Ac3Bj0c8HBvkmJBGpBMwC+qrqRhGprqrb/Vk2J5ZUjCl90jMy+WH1dibM3ciM33cgQJ+mNbimSz3ObFTNxnjJRyhufjxZnYA1qroOQETeAy4FfBPD1cAkVd0IoKrbC7CsMcYQER7Guc1rcm7zmmzcdYh352/kg/nJfLdqG3WrlOXqTvW4okM81cqXCXWopUIwKyDrAMk+z1O8ab5OAyqLyDQRWehdv/F3WWOMOUFC1Rju6Xs6s+47ixcHtaVOpbL8d8pquv7ne257dzFz1u2iJN2bVxQFs6SSU5kz+7sZAbQH+gBlgdkiMsfPZd1GREYAIwASEhJOOlhjTMlRJiKci1vX5uLWtVmzPZWJc5P5aGEyny/dTMO4cgzuXI/+7eKpGGM9JQdaMEsqKbiuXbLE4/oNyz7PFFU9qKo7gRlAaz+XBUBVx6hqB1XtEBcXF7DgjTElQ6Pqsfzz4mbMvf9snrqiNRXKRvLoF6vo9O+p3PXhUhZv3GOllwAK5oX6CNzF9j7AJtzF9qtVdaXPPE2Bl4DzcCNNzgMGAqvzWzYndqHeGOOPlZv3MXHuRj5ZvImDxzJoVqsCg7skcGmbOpQvE8wKnKKn2LT+AhCRC3DNhcOBN1X1cREZCaCqr3rz3A1cB2Timg4/l9uy+W3PkooxpiAOHE3n0yWbGD9nI79s2U+5qHAua1uHwZ3r0ax2hVCHVyiKVVIpbJZUjDEnQ1VZkryXCXM38vnSzRxNz6RtQiUGd67HRa1qleguYSyp5MGSijHmVO07lMbHi1KYMHcDa3ccpGLZSPq3i+fqzgklsrdkSyp5sKRijAkUVWXu+t1MmLuRKSu2kJahdGlQhWu61OPcZjVLzEBillTyYEnFGBMMOw8c5cMFKUyct4Hk3YepVt51aDmoUwJ1qxTvDi0tqeTBkooxJpgyM5Wf1uxkwpwNTP1lGwqc1aQ6Q7olFtsuYSyp5MGSijGmsGzZd5h3525k4rxkdh44SoNq5bi2az36t4+nQnTxuanSkkoeLKkYYwrbsfRMvl6xhbGzkli0cS8xUeFc3q4OQ7omclqN2FCHly9LKnmwpGKMCaXlKfsYNzuJT5du5lh6Jl0bVGVot3qc3bRGkR3rxZJKHiypGGOKgt0Hj/H+/GTGz9nApr2HqV0xmsFd6jGwY12qFrHeki2p5MGSijGmKMnIVL7/ZRvjZm9g5pqdRIWHcVHrWgztmkjrupVCHR5QvMZTMcaYUi08TI6P9bJmeyrjZm/g44UpTFq0iTZ1KzG0Wz0uaFmLMhEl5459K6kYY0whSj2SxqRFmxg7O4l1Ow5StVwUgzolMLhLQkiGQbbqrzxYUjHGFBeqys9rdvH2rCS+X72NMBHObVaDIV0T6dKgCiKFc8+LVX8ZY0wJICKc0bgaZzSuRvLuQ4yfu4H35yfz9YqtNKkRy5Bu9bisTR3KFbOu+K2kYowxRcSRtAw+W7qZsbOSWLl5P7HREVzRvi7Xdq1H/WrlgrJNq/7KgyUVY0xJoKos2riHsbM28NXyLaRnKj1Pi2NYt0R6nhYX0O5gLKnkwZKKMaak2b7/CO/OS2bC3A1sTz1KQpUYhnStxxXt61Ix5tS7gylWSUVE+gLP40ZvfF1VR2d7vRfwKbDemzRJVR/1XksCUoEMIN2fnbakYowpqdIyMpmyYivjZicxP2kP0ZFh9GvruoNpWuvkR6ksNhfqRSQceBk4B0gB5ovIZ6q6KtusP6nqRbmspreq7gxWjMYYU1xEhodxcevaXNy6Nis37+Od2RuYvHgT785LplP9Koy7vlORGKEymM0KOgFrVHUdgIi8B1wKZE8qxhhjCqB57YqM7t+Ke88/nQ8WJLNux8EikVAguEmlDpDs8zwF6JzDfF1FZCmwGbhLVVd60xX4VkQUeE1Vx+S0EREZAYwASEhICFTsxhhT5FWKiWJEj4ahDuMEwUwqOTVPyH4BZxFQT1UPiMgFwCdAY++17qq6WUSqA9+JyGpVnfGnFbpkMwbcNZWARW+MMabAgtkXcwpQ1+d5PK40cpyq7lfVA97jr4BIEanmPd/s/d8OTMZVpxljjCnCgplU5gONRaS+iEQBA4HPfGcQkZri9UUgIp28eHaJSDkRifWmlwPOBVYEMVZjjDEBELTqL1VNF5FbgW9wTYrfVNWVIjLSe/1VYABwk4ikA4eBgaqqIlIDmOzlmwhgoqpOCVasxhhjAsNufjTGmFIs0PepFM3xLY0xxhRLllSMMcYEjCUVY4wxAVOirqmIyA5gw0kuXg2wLmEcOxYnsuNxIjsefygJx6KeqsYFamUlKqmcChFZEMiLVcWZHYsT2fE4kR2PP9ix+DOr/jLGGBMwllSMMcYEjCWVP+TYYWUpZcfiRHY8TmTH4w92LLKxayrGGGMCxkoqxhhjAsaSijHGmIAp9UlFRPqKyK8iskZE7g11PKEkInVF5EcR+UVEVorI7aGOKdREJFxEFovIF6GOJdREpJKIfCQiq71zpGuoYwolEbnD+5ysEJF3RSQ61DEVBaU6qYhIOPAycD7QDBgkIs1CG1VIpQN3qmpToAtwSyk/HgC3A7+EOogi4nlgiqqeDrSmFB8XEakD/BXooKotcD2xDwxtVEVDqU4quIG/1qjqOlU9BrwHXBrimEJGVbeo6iLvcSruS6NOaKMKHRGJBy4EXg91LKEmIhWAHsAbAKp6TFX3hjSo0IsAyopIBBBDtkEIS6vSnlTqAMk+z1MoxV+ivkQkEWgLzA1xKKH0HPAPIDPEcRQFDYAdwFtedeDr3gB6pZKqbgKeAjYCW4B9qvptaKMqGkp7UpEcppX6NtYiUh74GPibqu4PdTyhICIXAdtVdWGoYykiIoB2wP+palvgIFBqr0GKSGVcrUZ9oDZQTkSuCW1URUNpTyopQF2f5/GU8iKsiETiEsoEVZ0U6nhCqDtwiYgk4apFzxKR8aENKaRSgBRVzSq5foRLMqXV2cB6Vd2hqmnAJKBbiGMqEkp7UpkPNBaR+iIShbvQ9lmIYwoZceM3vwH8oqrPhDqeUFLV+1Q1XlUTcefFD6paan+JqupWIFlEmniT+gCrQhhSqG0EuohIjPe56UMpbrjgK2hj1BcHqpouIrcC3+Bab7ypqitDHFYodQeuBZaLyBJv2v2q+lXoQjJFyG3ABO8H2DrguhDHEzKqOldEPgIW4VpNLsa6bAGsmxZjjDEBVNqrv4wxxgSQJRVjjDEBY0nFGGNMwFhSMcYYEzCWVIwxxgSMJRVjQkhEelkPyKYksaRijDEmYCypGOMHEblGROaJyBIRec0bZ+WAiDwtIotE5HsRifPmbSMic0RkmYhM9vqJQkQaichUEVnqLdPQW315n3FKJnh3aCMio0Vklbeep0K068YUiCUVY/IhIk2Bq4DuqtoGyAAGA+WARaraDpgOPOwtMg64R1VbAct9pk8AXlbV1rh+orZ409sCf8ON6dMA6C4iVYB+QHNvPf8K5j4aEyiWVIzJXx+gPTDf676mD+7LPxN435tnPHCGiFQEKqnqdG/6WKCHiMQCdVR1MoCqHlHVQ94881Q1RVUzgSVAIrAfOAK8LiKXA1nzGlOkWVIxJn8CjFXVNt5fE1UdlcN8efV5lNMwC1mO+jzOACJUNR03iNzHwGXAlIKFbExoWFIxJn/fAwNEpDqAiFQRkXq4z88Ab56rgZmqug/YIyJnetOvBaZ749KkiMhl3jrKiEhMbhv0xrSp6HXm+TegTcD3ypggKNW9FBvjD1VdJSIPAt+KSBiQBtyCG6iquYgsBPbhrrsADAVe9ZKGb2++1wKvicij3jquyGOzscCnIhKNK+XcEeDdMiYorJdiY06SiBxQ1fKhjsOYosSqv4wxxgSMlVSMMcYEjJVUjDHGBIwlFWOMMQFjScUYY0zAWFIxxhgTMJZUjDHGBMz/A6veYc6VIwf8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1)\n",
    "x = np.arange(len(history.history['loss']))\n",
    "ax.plot(x, history.history['loss'], label=\"train loss\")\n",
    "ax.plot(x, history.history['val_loss'], label=\"val loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"categorial cross entropy loss\")\n",
    "ax.legend()\n",
    "plt.title(\"Metrics for training SNLI dataset, 70K samples (glove 100d)\")\n",
    "#history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 40s 85ms/step - loss: 0.7687 - accuracy: 0.6780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7687164545059204, 0.6780181527137756]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_filepath)\n",
    "model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate\n",
    "Trying the code from [here](https://github.com/songyang0716/NLP/blob/master/natural_language_inference/sentence_encoding_RNN/model.py)\n",
    "and [concatenate lstm stackoverflow](https://stackoverflow.com/questions/53956998/how-can-i-concatenate-two-lstm-with-keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 100)    2567500     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 100)    2567500     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 200)          160800      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 200)          160800      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 400)          0           bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           6416        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 3)            51          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,463,067\n",
      "Trainable params: 328,067\n",
      "Non-trainable params: 5,135,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size= len(lang_tokenizer.word_index)\n",
    "dim = embedding_matrix.shape[1]\n",
    "#dim = 50 #keep it same as the dim of the embedding matrix so that we can compare\n",
    "embedding_hyp_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True)\n",
    "embedding_evi_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True)\n",
    "\n",
    "inp1 = keras.Input(shape=(None,))\n",
    "inp2 = keras.Input(shape=(None,))\n",
    "x_hyp = embedding_hyp_layer(inp1)\n",
    "x_evi = embedding_evi_layer(inp2)\n",
    "\n",
    "#this throws an error in the d type instances in AWS, works on p type instances\n",
    "#hyp_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(dim))(x_hyp)\n",
    "hyp_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim)))(x_hyp)\n",
    "hyp_evi = tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(dim)))(x_evi)\n",
    "\n",
    "#tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(64))),\n",
    "w = keras.layers.concatenate([hyp_lstm, hyp_evi], axis = 1)\n",
    "\n",
    "x3 = tf.keras.layers.Dense(16, activation='relu')(w)\n",
    "x4 = tf.keras.layers.Dropout(0.1)(x3)\n",
    "output = tf.keras.layers.Dense(3, activation='softmax')(x4)\n",
    "    \n",
    "model1 = keras.Model(inputs=[inp1, inp2], outputs=output)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(), \n",
    "          metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 84)\n",
      "(300000, 62)\n"
     ]
    }
   ],
   "source": [
    "print(hyp_tokens.shape)\n",
    "print(evi_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_tensor_train_h, input_tensor_val_h, input_tensor_train_e, input_tensor_val_e, target_tensor_train, target_tensor_val = train_test_split(hyp_tokens, evi_tokens, train_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210000, 84)\n",
      "(90000, 84)\n",
      "(210000, 62)\n",
      "(90000, 62)\n",
      "(210000, 3)\n",
      "(90000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor_train_h.shape)\n",
    "print(input_tensor_val_h.shape)\n",
    "print(input_tensor_train_e.shape)\n",
    "print(input_tensor_val_e.shape)\n",
    "print(target_tensor_train.shape)\n",
    "print(target_tensor_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = 32000\n",
    "# BATCH_SIZE = 64\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices(({'input_6': input_tensor_train_h, 'input_7': input_tensor_train_e}, target_tensor_train))\n",
    "# train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices(({'input_6': input_tensor_val_h, 'input_7': input_tensor_val_e}, target_tensor_val))\n",
    "# val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<TensorSliceDataset shapes: ((84,), (62,)), types: (tf.int32, tf.int32)>\n",
      "<ZipDataset shapes: (((84,), (62,)), (3,)), types: ((tf.int32, tf.int32), tf.float64)>\n",
      "<BatchDataset shapes: (((64, 84), (64, 62)), (64, 3)), types: ((tf.int32, tf.int32), tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "print(type(input_tensor_train_h))\n",
    "dataset_12 = tf.data.Dataset.from_tensor_slices((input_tensor_train_h, input_tensor_train_e))\n",
    "print(dataset_12)\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices(target_tensor_train)\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((dataset_12, dataset_label))\n",
    "print(train_dataset)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(train_dataset)\n",
    "\n",
    "dataset_12_val = tf.data.Dataset.from_tensor_slices((input_tensor_val_h, input_tensor_val_e))\n",
    "dataset_label_val = tf.data.Dataset.from_tensor_slices(target_tensor_val)\n",
    "\n",
    "val_dataset = tf.data.Dataset.zip((dataset_12_val, dataset_label_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tf.Tensor(\n",
      "[[  3   6  15 ...   0   0   0]\n",
      " [  3   2 130 ...   0   0   0]\n",
      " [  3   2 263 ...   0   0   0]\n",
      " ...\n",
      " [  3 671   7 ...   0   0   0]\n",
      " [  3  53  12 ...   0   0   0]\n",
      " [  3  16  85 ...   0   0   0]], shape=(64, 62), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]], shape=(64, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for t in train_dataset.take(1):\n",
    "    print(len(t))\n",
    "    print(t[0][1])\n",
    "    print(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'tmp/checkpoint_bowman_2'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint  checkpoint_bowman_2.data-00000-of-00001  checkpoint_bowman_2.index\n"
     ]
    }
   ],
   "source": [
    "ls tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf tmp/checkpoint*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (((64, 84), (64, 46)), (64, 3)), types: ((tf.int32, tf.int32), tf.float64)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 100)    1645200     input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200)          160800      embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 200)          160800      embedding[2][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 400)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           6416        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            51          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,973,267\n",
      "Trainable params: 328,067\n",
      "Non-trainable params: 1,645,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# print(physical_devices)\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (((64, 84), (64, 62)), (64, 3)), types: ((tf.int32, tf.int32), tf.float64)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1380/3281 [===========>..................] - ETA: 6:21 - loss: 0.9487 - accuracy: 0.5333"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-13f28808436c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_early\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     validation_steps=30)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#since we are creating variables in the first call, we need to set this parameter, to run functions eagerly\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "history1 = model1.fit(train_dataset, epochs=20,\n",
    "                    callbacks=[stop_early, model_checkpoint_callback],\n",
    "                    validation_data=val_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
