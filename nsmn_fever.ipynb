{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment\n",
    "In the NSMN paper, alignment is one of the key steps in the modelling. We will look at the details of textual entailment here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_zip_file = \"glove.6B.zip\"\n",
    "glove_vectors_file = \"glove.6B.50d.txt\"\n",
    "\n",
    "snli_zip_file = \"snli_1.0.zip\"\n",
    "snli_dev_file = \"snli_1.0_dev.txt\"\n",
    "snli_full_dataset_file = \"snli_1.0_train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p entailment/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the glove embeddings and the SNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__MACOSX  glove.6B.zip\tsnli_1.0  snli_1.0.bak.zip  snli_1.0.zip\n"
     ]
    }
   ],
   "source": [
    "!ls entailment/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data\n",
    "\n",
    "We will use golve embeddings to initialize the embeddings in our nerual network and also set it to non-trainable.\n",
    "\n",
    "Load the glove embeddings into an embedding matrix, but first we need the tokens from our text.\n",
    "\n",
    "Lets peek into a few lines in the training dataset. The training dataset .csv file is apprently quite large, so we will settle for the 'dev' .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines from file snli_1.0/snli_1.0_train.txt\n",
      "header line:\n",
      "gold_label\tsentence1_binary_parse\tsentence2_binary_parse\tsentence1_parse\tsentence2_parse\tsentence1\tsentence2\tcaptionID\tpairID\tlabel1\tlabel2\tlabel3\tlabel4\tlabel5\n",
      "\n",
      "neutral\t( ( ( A person ) ( on ( a horse ) ) ) ( ( jumps ( over ( a ( broken ( down airplane ) ) ) ) ) . ) )\t( ( A person ) ( ( is ( ( training ( his horse ) ) ( for ( a competition ) ) ) ) . ) )\t(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN on) (NP (DT a) (NN horse)))) (VP (VBZ jumps) (PP (IN over) (NP (DT a) (JJ broken) (JJ down) (NN airplane)))) (. .)))\t(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) (VP (VBG training) (NP (PRP$ his) (NN horse)) (PP (IN for) (NP (DT a) (NN competition))))) (. .)))\tA person on a horse jumps over a broken down airplane.\tA person is training his horse for a competition.\t3416050480.jpg#4\t3416050480.jpg#4r1n\tneutral\t\t\t\t\n",
      "\n",
      "['neutral' 'A person on a horse jumps over a broken down airplane.'\n",
      " 'A person is training his horse for a competition.']\n",
      "contradiction\t( ( ( A person ) ( on ( a horse ) ) ) ( ( jumps ( over ( a ( broken ( down airplane ) ) ) ) ) . ) )\t( ( A person ) ( ( ( ( is ( at ( a diner ) ) ) , ) ( ordering ( an omelette ) ) ) . ) )\t(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN on) (NP (DT a) (NN horse)))) (VP (VBZ jumps) (PP (IN over) (NP (DT a) (JJ broken) (JJ down) (NN airplane)))) (. .)))\t(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) (PP (IN at) (NP (DT a) (NN diner))) (, ,) (S (VP (VBG ordering) (NP (DT an) (NN omelette))))) (. .)))\tA person on a horse jumps over a broken down airplane.\tA person is at a diner, ordering an omelette.\t3416050480.jpg#4\t3416050480.jpg#4r1c\tcontradiction\t\t\t\t\n",
      "\n",
      "['contradiction' 'A person on a horse jumps over a broken down airplane.'\n",
      " 'A person is at a diner, ordering an omelette.']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "import numpy as np\n",
    "zip_file_name = 'entailment/data/snli_1.0.zip'\n",
    "output_file_name = 'snli_1.0/snli_1.0_train.txt'\n",
    "\n",
    "counter = 0\n",
    "columns = ['gold_label','sentence1','sentence2']\n",
    "indices = [-1, -1, -1]\n",
    "with zipfile.ZipFile(zip_file_name) as z:\n",
    "    for info in z.infolist():\n",
    "        if output_file_name in info.filename:\n",
    "            # read the file\n",
    "            print(\"Reading lines from file {}\".format(output_file_name))\n",
    "            with io.TextIOWrapper(z.open(output_file_name), encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    terms = line.split('\\t')\n",
    "                    if np.min(indices) == -1: # this is the first line\n",
    "                        print(\"header line:\")\n",
    "                        print(line)\n",
    "                        indices = [np.where(np.array(terms) == val)[0] for val in columns]\n",
    "                    else:\n",
    "                        print(line)\n",
    "                        idx = [i[0] for i in indices]\n",
    "                        print(np.array(terms)[idx])\n",
    "                    counter += 1\n",
    "                    if counter > 2:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The structure of the training records\n",
    "As you can see from above, the dataset contains a label, the hypothesis sentence in 'sentence1' and the evidence sentence in 'sentence2', among several other values such as the parsed sentences as binary parsed as well as parsed with POS tagging.\n",
    "\n",
    "The important fields for us are the gold_label, the sentence1 and sentence2.\n",
    "\n",
    "We will need to write a parser to extract these into our training dataset.\n",
    "\n",
    "\n",
    "- sentence1: The premise caption that was supplied to the author of the pair.\n",
    "\n",
    "- sentence2: The hypothesis caption that was written by the author of the pair.\n",
    "\n",
    "- sentence{1,2}_parse: The parse produced by the Stanford Parser (3.5.2, case insensitive PCFG, trained on the standard training set augmented with the parsed Brown Corpus) in Penn Treebank format.\n",
    "\n",
    "- sentence{1,2}_binary_parse: The same parse as in sentence{1,2}_parse, but formatted for use in tree-structured neural networks with no unary nodes and no labels.\n",
    "\n",
    "- annotator_labels (label1-5 in the tab separated file): These are all of the individual labels from annotators in phases 1 and 2. The first label comes from the phase 1 author, and is the only label for examples that did not undergo phase 2 annotation. In a few cases, the one of the phase 2 labels may be blank, indicating that an annotator saw the example but could not annotate it.\n",
    "\n",
    "- gold_label: This is the label chosen by the majority of annotators. Where no majority exists, this is '-', and the pair should not be included when evaluating hard classification accuracy.\n",
    "\n",
    "- captionID: A unique identifier for each sentence1 from the original Flickr30k example.\n",
    "\n",
    "- pairID: A unique identifier for each sentence1--sentence2 pair.\n",
    "\n",
    "*NOTE: captionID and pairID contain information that can be useful in making classification decisions and should not be included in model input (nor, of course, should either annotator_labels or gold_label).*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "zip_file_name = 'entailment/data/snli_1.0.zip'\n",
    "output_file_name = 'snli_1.0/snli_1.0_train.txt'\n",
    "\n",
    "def load_data(num_samples=10):\n",
    "    counter = 0\n",
    "    columns = ['gold_label','sentence1','sentence2']\n",
    "    indices = [-1, -1, -1]\n",
    "    data = []\n",
    "    with zipfile.ZipFile(zip_file_name) as z:\n",
    "        for info in z.infolist():\n",
    "            if output_file_name in info.filename:\n",
    "                # read the file\n",
    "                print(\"Reading lines from file {}\".format(output_file_name))\n",
    "                with io.TextIOWrapper(z.open(output_file_name), encoding=\"utf-8\") as f:\n",
    "                    for line in tqdm(f):\n",
    "                        terms = line.split('\\t')\n",
    "                        if np.min(indices) == -1: # this is the first line\n",
    "                            indices = [np.where(np.array(terms) == val)[0] for val in columns]\n",
    "                            counter += 1\n",
    "                        else:\n",
    "                            idx = [i[0] for i in indices]\n",
    "                            #do not include the '-' label\n",
    "                            if np.array(terms)[idx][0] != '-':\n",
    "                                data.append(np.array(terms)[idx])\n",
    "                                counter += 1\n",
    "                        if (num_samples > -1) & (counter > num_samples):\n",
    "                            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1213it [00:00, 12127.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines from file snli_1.0/snli_1.0_train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100111it [00:04, 20698.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 100000 records\n"
     ]
    }
   ],
   "source": [
    "data = load_data(100000)\n",
    "print(\"Read {} records\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare train and dev dataset\n",
    "We will need to create the train and dev dataset splits. We need to ensure that each of the splits have sufficient number of samples for all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, ratio=0.7):\n",
    "    labels = [d[0] for d in data]\n",
    "    u_labels = np.unique(labels)\n",
    "    samples = {}\n",
    "    for d in data:\n",
    "        if d[0] in samples:\n",
    "            samples[d[0]].append((d[1], d[2]))\n",
    "        else:\n",
    "            samples[d[0]] = [(d[1], d[2])]\n",
    "\n",
    "    train = []\n",
    "    dev = []\n",
    "\n",
    "    for lbl in u_labels:\n",
    "        num_samples = len(samples[lbl])\n",
    "        idx = np.arange(num_samples)\n",
    "        np.random.shuffle(idx)\n",
    "        train.extend([(lbl, s1, s2) for s1, s2 in np.array(samples[lbl])[idx][:int(num_samples*ratio)]])\n",
    "        dev.extend([(lbl, s1, s2) for s1, s2 in np.array(samples[lbl])[idx][int(num_samples*ratio):]])\n",
    "    return train, dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contradiction',\n",
       "  'A white dog running in the backyard.',\n",
       "  'The dog is sleeping.'),\n",
       " ('contradiction',\n",
       "  'A man walking through town carrying his goods.',\n",
       "  'A man sleeping on his couch.'),\n",
       " ('contradiction',\n",
       "  'A boy in a green sweatshirt and blue jeans performs a trick on a skateboard while spectators in the background are looking on.',\n",
       "  'The piano player finishes his piece of music.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preparation\n",
    "\n",
    "We will need to convert the texts to sequences.\n",
    "\n",
    "We will need to write a sentence preprocessor, one that processes each of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "def preprocess(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "        w = w.strip()\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a tokenizer that will first fit to the entire corpus and then tokenize the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer needs to see the entire corpus, so we will need to merge the hypothesis and evidence for the tokenization task.\n",
    "Using the tokenizer, we will also tokenize the sentences, the hypothesis and the evidences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = [\" \".join((preprocess(t[1]),preprocess(t[2]))) for t in train]\n",
    "lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "lang_tokenizer.fit_on_texts(all_texts)\n",
    "hyp_tokens = lang_tokenizer.texts_to_sequences([preprocess(t[1]) for t in train])\n",
    "hyp_tokens = tf.keras.preprocessing.sequence.pad_sequences(hyp_tokens, padding='post')\n",
    "evi_tokens = lang_tokenizer.texts_to_sequences([preprocess(t[2]) for t in train])\n",
    "evi_tokens = tf.keras.preprocessing.sequence.pad_sequences(evi_tokens, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3   6  31   8 149   5   4   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "(46,)\n",
      "(84,)\n",
      "<class 'numpy.ndarray'>\n",
      "Max length of hypothesis = 84 and evidence = 46\n"
     ]
    }
   ],
   "source": [
    "print(evi_tokens[0])\n",
    "print(evi_tokens[0].shape)\n",
    "print(hyp_tokens[0].shape)\n",
    "print(type(evi_tokens))\n",
    "len(evi_tokens)\n",
    "max_len_hyp = hyp_tokens[0].shape[0]\n",
    "max_len_evi = evi_tokens[0].shape[0]\n",
    "print(\"Max length of hypothesis = {} and evidence = {}\".format(max_len_hyp, max_len_evi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [t[0] for t in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels_enc = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A peek a the reshaped labels:\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "The datatypes of the training dataset, features=<class 'numpy.ndarray'>, labels=<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.zeros(shape=(len(labels_enc),3))\n",
    "for idx, val in enumerate(labels_enc):\n",
    "    train_labels[idx][val]=1\n",
    "print(\"A peek a the reshaped labels:\")\n",
    "print(train_labels[:5])\n",
    "print(\"The datatypes of the training dataset, features={}, labels={}\".format(type(labels_enc), type(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((hyp_tokens, evi_tokens), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(features, train_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   3    2   15  553   54   62    2  727    7    2 2295 8179  223    5\n",
      "     4    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     3    2   15  867  727   54    7   21 8858    7    6  216   13    2\n",
      "   599    5    4    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "(48999, 130)\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor_train[:1])\n",
    "print(input_tensor_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inputs are now encoded, so we will need to send in these encoded inputs to our network for training. \n",
    "\n",
    "What about golve embeddings?\n",
    "\n",
    "The sequence tokens we have are word vectorized by vocabulary.\n",
    "\n",
    "Some reference is [here](https://heartbeat.fritz.ai/text-classification-using-long-short-term-memory-glove-embeddings-6894abb730e1)\n",
    "\n",
    "The lang tokenizer we used earlier already has the word index. We can use those to look up glove embeddings.\n",
    "\n",
    "Let's read the glove embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'a': 2,\n",
       " '<start>': 3,\n",
       " '<end>': 4,\n",
       " '.': 5,\n",
       " 'the': 6,\n",
       " 'in': 7,\n",
       " 'is': 8,\n",
       " 'man': 9,\n",
       " 'on': 10,\n",
       " 'and': 11,\n",
       " 'of': 12,\n",
       " 'are': 13,\n",
       " 'with': 14,\n",
       " ',': 15,\n",
       " 'to': 16,\n",
       " 'woman': 17,\n",
       " 'two': 18,\n",
       " 'people': 19,\n",
       " 'his': 20,\n",
       " 'at': 21,\n",
       " 'an': 22,\n",
       " 'young': 23,\n",
       " 'black': 24,\n",
       " 'shirt': 25,\n",
       " 'playing': 26,\n",
       " 'girl': 27,\n",
       " 'while': 28,\n",
       " 'men': 29,\n",
       " 'boy': 30,\n",
       " 'wearing': 31,\n",
       " 'white': 32,\n",
       " 'dog': 33,\n",
       " 'her': 34,\n",
       " 'sitting': 35,\n",
       " 'red': 36,\n",
       " 'outside': 37,\n",
       " 'standing': 38,\n",
       " 'blue': 39,\n",
       " 'one': 40,\n",
       " 'down': 41,\n",
       " 'for': 42,\n",
       " 'group': 43,\n",
       " 'children': 44,\n",
       " 'by': 45,\n",
       " 'holding': 46,\n",
       " 'street': 47,\n",
       " 'walking': 48,\n",
       " 'three': 49,\n",
       " 'brown': 50,\n",
       " 'women': 51,\n",
       " 'person': 52,\n",
       " 'other': 53,\n",
       " 'as': 54,\n",
       " 'green': 55,\n",
       " 'there': 56,\n",
       " 'dressed': 57,\n",
       " 'their': 58,\n",
       " 'front': 59,\n",
       " 'next': 60,\n",
       " 'water': 61,\n",
       " 'into': 62,\n",
       " 'that': 63,\n",
       " 'looking': 64,\n",
       " 'sidewalk': 65,\n",
       " 'near': 66,\n",
       " 'camera': 67,\n",
       " 'little': 68,\n",
       " 'background': 69,\n",
       " 'it': 70,\n",
       " 'doing': 71,\n",
       " 'from': 72,\n",
       " 'jumping': 73,\n",
       " 'has': 74,\n",
       " 'building': 75,\n",
       " 'kids': 76,\n",
       " 'crowd': 77,\n",
       " 'boys': 78,\n",
       " 'floor': 79,\n",
       " 'sit': 80,\n",
       " 'together': 81,\n",
       " 'walks': 82,\n",
       " 'up': 83,\n",
       " 'talking': 84,\n",
       " 'hair': 85,\n",
       " 'couple': 86,\n",
       " 'hands': 87,\n",
       " 'him': 88,\n",
       " 'ball': 89,\n",
       " 'something': 90,\n",
       " 'ocean': 91,\n",
       " 'air': 92,\n",
       " 'side': 93,\n",
       " 'baby': 94,\n",
       " 'looks': 95,\n",
       " 'line': 96,\n",
       " 's': 97,\n",
       " 'girls': 98,\n",
       " 'grass': 99,\n",
       " 'park': 100,\n",
       " 'he': 101,\n",
       " 'stands': 102,\n",
       " 'rides': 103,\n",
       " 'them': 104,\n",
       " 'child': 105,\n",
       " 'having': 106,\n",
       " 'field': 107,\n",
       " 'running': 108,\n",
       " 'some': 109,\n",
       " 'suit': 110,\n",
       " 'inside': 111,\n",
       " 'around': 112,\n",
       " 'beach': 113,\n",
       " 'over': 114,\n",
       " 'out': 115,\n",
       " 'team': 116,\n",
       " 'plays': 117,\n",
       " 'car': 118,\n",
       " 'behind': 119,\n",
       " 'yellow': 120,\n",
       " 'sleeping': 121,\n",
       " 'waiting': 122,\n",
       " 'be': 123,\n",
       " 'riding': 124,\n",
       " 'through': 125,\n",
       " 'guy': 126,\n",
       " 'pink': 127,\n",
       " 'bicycle': 128,\n",
       " 'large': 129,\n",
       " 'ground': 130,\n",
       " 'sits': 131,\n",
       " 'being': 132,\n",
       " 'orange': 133,\n",
       " 'bar': 134,\n",
       " 'pants': 135,\n",
       " 'another': 136,\n",
       " 'lake': 137,\n",
       " 'dogs': 138,\n",
       " 'past': 139,\n",
       " 'basketball': 140,\n",
       " 'watch': 141,\n",
       " 'head': 142,\n",
       " 'pool': 143,\n",
       " 'laundry': 144,\n",
       " 'house': 145,\n",
       " 'work': 146,\n",
       " 'middle': 147,\n",
       " 'play': 148,\n",
       " 'ride': 149,\n",
       " 'toy': 150,\n",
       " 'bike': 151,\n",
       " 'food': 152,\n",
       " 'taking': 153,\n",
       " 'who': 154,\n",
       " 'reading': 155,\n",
       " 'face': 156,\n",
       " 'gray': 157,\n",
       " 'watching': 158,\n",
       " 'small': 159,\n",
       " 'corn': 160,\n",
       " 'game': 161,\n",
       " 'each': 162,\n",
       " 'store': 163,\n",
       " 'window': 164,\n",
       " 'sand': 165,\n",
       " 'colored': 166,\n",
       " 'dancing': 167,\n",
       " 'have': 168,\n",
       " 'long': 169,\n",
       " 'indian': 170,\n",
       " 'piece': 171,\n",
       " 'four': 172,\n",
       " 'top': 173,\n",
       " 'picture': 174,\n",
       " 'steps': 175,\n",
       " 'soccer': 176,\n",
       " 'holds': 177,\n",
       " 'or': 178,\n",
       " 'smiling': 179,\n",
       " 'bench': 180,\n",
       " 'dark': 181,\n",
       " 'selling': 182,\n",
       " 'working': 183,\n",
       " 'five': 184,\n",
       " 'stand': 185,\n",
       " 'about': 186,\n",
       " 'guitar': 187,\n",
       " 'city': 188,\n",
       " 'baseball': 189,\n",
       " 'female': 190,\n",
       " 'older': 191,\n",
       " 'hat': 192,\n",
       " 'teams': 193,\n",
       " 'father': 194,\n",
       " 'table': 195,\n",
       " 'kid': 196,\n",
       " 'off': 197,\n",
       " 'players': 198,\n",
       " 'cart': 199,\n",
       " 'hand': 200,\n",
       " 'bright': 201,\n",
       " 'under': 202,\n",
       " 'swimming': 203,\n",
       " 'laying': 204,\n",
       " 'look': 205,\n",
       " 'friend': 206,\n",
       " 'singing': 207,\n",
       " 'fish': 208,\n",
       " 'she': 209,\n",
       " 'dress': 210,\n",
       " 'clothes': 211,\n",
       " 'perform': 212,\n",
       " 'toddler': 213,\n",
       " 'jerseys': 214,\n",
       " 'goalie': 215,\n",
       " 'towards': 216,\n",
       " 'this': 217,\n",
       " 'jumps': 218,\n",
       " 'striped': 219,\n",
       " 'paper': 220,\n",
       " 'male': 221,\n",
       " 'watches': 222,\n",
       " 'sky': 223,\n",
       " 'foot': 224,\n",
       " 'bicycles': 225,\n",
       " 'area': 226,\n",
       " 'wall': 227,\n",
       " 'sled': 228,\n",
       " 'slide': 229,\n",
       " 'which': 230,\n",
       " 'both': 231,\n",
       " 'uniform': 232,\n",
       " 'workers': 233,\n",
       " 'time': 234,\n",
       " 'catch': 235,\n",
       " 'walk': 236,\n",
       " 'purple': 237,\n",
       " 'away': 238,\n",
       " 'football': 239,\n",
       " 'clothing': 240,\n",
       " 'colorful': 241,\n",
       " 'eating': 242,\n",
       " 'all': 243,\n",
       " 'phone': 244,\n",
       " 'race': 245,\n",
       " 'edge': 246,\n",
       " 'track': 247,\n",
       " 'train': 248,\n",
       " 'microphone': 249,\n",
       " 'pole': 250,\n",
       " 'arm': 251,\n",
       " 'chair': 252,\n",
       " 'back': 253,\n",
       " 'market': 254,\n",
       " 'drink': 255,\n",
       " 'high': 256,\n",
       " 'score': 257,\n",
       " 'restaurant': 258,\n",
       " 'boat': 259,\n",
       " 'chasing': 260,\n",
       " 'site': 261,\n",
       " 'horse': 262,\n",
       " 'they': 263,\n",
       " 'bus': 264,\n",
       " 'asian': 265,\n",
       " 'going': 266,\n",
       " 'vest': 267,\n",
       " 'countryside': 268,\n",
       " 'clouds': 269,\n",
       " 'grassy': 270,\n",
       " 'runs': 271,\n",
       " 'rock': 272,\n",
       " 'couch': 273,\n",
       " 'construction': 274,\n",
       " 'cooking': 275,\n",
       " 'hot': 276,\n",
       " 'animal': 277,\n",
       " 'winter': 278,\n",
       " 'stone': 279,\n",
       " 'outdoors': 280,\n",
       " 'performing': 281,\n",
       " 'haired': 282,\n",
       " 'what': 283,\n",
       " 'aisle': 284,\n",
       " 'day': 285,\n",
       " 'passes': 286,\n",
       " 'onlookers': 287,\n",
       " 'apron': 288,\n",
       " 'equipment': 289,\n",
       " 'subway': 290,\n",
       " 'shirts': 291,\n",
       " 'using': 292,\n",
       " 'number': 293,\n",
       " 'motorcycle': 294,\n",
       " 'reaches': 295,\n",
       " 'propeller': 296,\n",
       " 'several': 297,\n",
       " 'suits': 298,\n",
       " 'lady': 299,\n",
       " 'climbing': 300,\n",
       " 'instrument': 301,\n",
       " 'take': 302,\n",
       " 'blond': 303,\n",
       " 'party': 304,\n",
       " 'flowers': 305,\n",
       " 'object': 306,\n",
       " 'wooden': 307,\n",
       " 'showing': 308,\n",
       " 'eye': 309,\n",
       " 'corner': 310,\n",
       " 'snow': 311,\n",
       " 'coat': 312,\n",
       " 'desk': 313,\n",
       " 'sweater': 314,\n",
       " 'others': 315,\n",
       " 'beautiful': 316,\n",
       " 'spectators': 317,\n",
       " 'jersey': 318,\n",
       " 't': 319,\n",
       " 'theater': 320,\n",
       " 'against': 321,\n",
       " 'appears': 322,\n",
       " 'giving': 323,\n",
       " 'driving': 324,\n",
       " 'drinking': 325,\n",
       " 'run': 326,\n",
       " 'road': 327,\n",
       " 'produce': 328,\n",
       " 'quarterback': 329,\n",
       " 'court': 330,\n",
       " 'crowded': 331,\n",
       " 'performer': 332,\n",
       " 'shopping': 333,\n",
       " 'television': 334,\n",
       " 'home': 335,\n",
       " 'leading': 336,\n",
       " 'uses': 337,\n",
       " 'presentation': 338,\n",
       " 'someone': 339,\n",
       " 'police': 340,\n",
       " 'parked': 341,\n",
       " 'button': 342,\n",
       " 'jeans': 343,\n",
       " 'flight': 344,\n",
       " 'way': 345,\n",
       " 'station': 346,\n",
       " 'john': 347,\n",
       " 'just': 348,\n",
       " 'photo': 349,\n",
       " 'bag': 350,\n",
       " 'tennis': 351,\n",
       " 'lot': 352,\n",
       " 'ring': 353,\n",
       " 'does': 354,\n",
       " 'boots': 355,\n",
       " 'end': 356,\n",
       " 'golf': 357,\n",
       " 'lab': 358,\n",
       " 'reaching': 359,\n",
       " 'violin': 360,\n",
       " 'touching': 361,\n",
       " 'ready': 362,\n",
       " 'suv': 363,\n",
       " 'cyclist': 364,\n",
       " 'neon': 365,\n",
       " 'aged': 366,\n",
       " 'writing': 367,\n",
       " 'view': 368,\n",
       " 'sun': 369,\n",
       " 'only': 370,\n",
       " 'goal': 371,\n",
       " 'amusement': 372,\n",
       " 'seated': 373,\n",
       " 'trees': 374,\n",
       " 'plane': 375,\n",
       " 'during': 376,\n",
       " 'eat': 377,\n",
       " 'sport': 378,\n",
       " 'event': 379,\n",
       " 'display': 380,\n",
       " 'cap': 381,\n",
       " 'squatting': 382,\n",
       " 'tackled': 383,\n",
       " 'outfits': 384,\n",
       " 'letters': 385,\n",
       " 'toward': 386,\n",
       " 'alone': 387,\n",
       " 'accordion': 388,\n",
       " 'cartwheel': 389,\n",
       " 'son': 390,\n",
       " 'listening': 391,\n",
       " 'headphones': 392,\n",
       " 'hanging': 393,\n",
       " 'hoping': 394,\n",
       " 'win': 395,\n",
       " 'good': 396,\n",
       " 'officers': 397,\n",
       " 'gear': 398,\n",
       " 'life': 399,\n",
       " 'farmers': 400,\n",
       " 'touch': 401,\n",
       " 'runner': 402,\n",
       " 'ice': 403,\n",
       " 'drinks': 404,\n",
       " 'deere': 405,\n",
       " 'computer': 406,\n",
       " 'kicked': 407,\n",
       " 'brick': 408,\n",
       " 'bathroom': 409,\n",
       " 'fishing': 410,\n",
       " 'school': 411,\n",
       " 'gathered': 412,\n",
       " 'carpet': 413,\n",
       " 'costume': 414,\n",
       " 'sign': 415,\n",
       " 'dirty': 416,\n",
       " 'gentleman': 417,\n",
       " 'pictures': 418,\n",
       " 'shirtless': 419,\n",
       " 'cellphone': 420,\n",
       " 'resting': 421,\n",
       " 'light': 422,\n",
       " 'east': 423,\n",
       " 'cob': 424,\n",
       " 'rugby': 425,\n",
       " 'like': 426,\n",
       " 'structure': 427,\n",
       " 'mother': 428,\n",
       " 'mid': 429,\n",
       " 'placing': 430,\n",
       " 'along': 431,\n",
       " 'was': 432,\n",
       " 'classroom': 433,\n",
       " 'science': 434,\n",
       " 'skateboarding': 435,\n",
       " 'brass': 436,\n",
       " 'pots': 437,\n",
       " 'cleaning': 438,\n",
       " 'overlooking': 439,\n",
       " 'guys': 440,\n",
       " 'after': 441,\n",
       " 'wedding': 442,\n",
       " 'direction': 443,\n",
       " 'barefoot': 444,\n",
       " 'slinky': 445,\n",
       " 'wave': 446,\n",
       " 'old': 447,\n",
       " 'polo': 448,\n",
       " 'music': 449,\n",
       " 'balloons': 450,\n",
       " 'podium': 451,\n",
       " 'tall': 452,\n",
       " 'fruit': 453,\n",
       " 'model': 454,\n",
       " 'lap': 455,\n",
       " 'backyard': 456,\n",
       " 'deck': 457,\n",
       " 'racing': 458,\n",
       " 'bride': 459,\n",
       " 'power': 460,\n",
       " 'safety': 461,\n",
       " 'electronics': 462,\n",
       " 'yoga': 463,\n",
       " 'tunnel': 464,\n",
       " 'defensive': 465,\n",
       " 'photos': 466,\n",
       " 'throwing': 467,\n",
       " 'dividing': 468,\n",
       " 'tabs': 469,\n",
       " 'beside': 470,\n",
       " 'nobody': 471,\n",
       " 'stairs': 472,\n",
       " 'cat': 473,\n",
       " 'projector': 474,\n",
       " 'tricks': 475,\n",
       " 'drives': 476,\n",
       " 'atv': 477,\n",
       " 'will': 478,\n",
       " 'not': 479,\n",
       " 'go': 480,\n",
       " 'dune': 481,\n",
       " 'protective': 482,\n",
       " 'stadium': 483,\n",
       " 'wet': 484,\n",
       " 'smoking': 485,\n",
       " 'weaving': 486,\n",
       " 'cloth': 487,\n",
       " 'player': 488,\n",
       " 'flies': 489,\n",
       " 'nun': 490,\n",
       " 'beer': 491,\n",
       " 'gingerbread': 492,\n",
       " 'crew': 493,\n",
       " 'bird': 494,\n",
       " 'brunette': 495,\n",
       " 'multicolored': 496,\n",
       " 'outfit': 497,\n",
       " 'eyes': 498,\n",
       " 'half': 499,\n",
       " 'costumes': 500,\n",
       " 'tents': 501,\n",
       " 'close': 502,\n",
       " 'shade': 503,\n",
       " 'enjoys': 504,\n",
       " 'anvil': 505,\n",
       " 'waterfall': 506,\n",
       " 'brushing': 507,\n",
       " 'clean': 508,\n",
       " 'gets': 509,\n",
       " 'camel': 510,\n",
       " 'playground': 511,\n",
       " 'newspaper': 512,\n",
       " 'covering': 513,\n",
       " 'attempts': 514,\n",
       " 'barely': 515,\n",
       " 'trick': 516,\n",
       " 'wheel': 517,\n",
       " 'supermarket': 518,\n",
       " 'worker': 519,\n",
       " 'slides': 520,\n",
       " 'marathon': 521,\n",
       " 'performs': 522,\n",
       " 'slip': 523,\n",
       " 'officer': 524,\n",
       " 'body': 525,\n",
       " 'railroad': 526,\n",
       " 'tracks': 527,\n",
       " 'hallway': 528,\n",
       " 'rope': 529,\n",
       " 'handing': 530,\n",
       " 'kites': 531,\n",
       " 'beard': 532,\n",
       " 'set': 533,\n",
       " 'desert': 534,\n",
       " 'auditorium': 535,\n",
       " 'poster': 536,\n",
       " 'competing': 537,\n",
       " 'laughs': 538,\n",
       " 'taxi': 539,\n",
       " 'washing': 540,\n",
       " 'boats': 541,\n",
       " 'opposing': 542,\n",
       " 'helmets': 543,\n",
       " 'friends': 544,\n",
       " 'room': 545,\n",
       " 'bull': 546,\n",
       " 'tank': 547,\n",
       " 'boyfriend': 548,\n",
       " 'prepare': 549,\n",
       " 'outdoor': 550,\n",
       " 'matching': 551,\n",
       " 'cloudy': 552,\n",
       " 'getting': 553,\n",
       " 'post': 554,\n",
       " 'different': 555,\n",
       " 'world': 556,\n",
       " 'short': 557,\n",
       " 'many': 558,\n",
       " 'drawing': 559,\n",
       " 'horses': 560,\n",
       " 'inflatable': 561,\n",
       " 'late': 562,\n",
       " 'wagon': 563,\n",
       " 'artwork': 564,\n",
       " 'stumbles': 565,\n",
       " 'band': 566,\n",
       " 'masked': 567,\n",
       " 'actors': 568,\n",
       " 'traditional': 569,\n",
       " 'japanese': 570,\n",
       " 'sprayed': 571,\n",
       " 'hose': 572,\n",
       " 'tag': 573,\n",
       " 'try': 574,\n",
       " 'match': 575,\n",
       " 'bottle': 576,\n",
       " 'straddles': 577,\n",
       " 'doorway': 578,\n",
       " 'falls': 579,\n",
       " 'wife': 580,\n",
       " 'trimming': 581,\n",
       " 'sells': 582,\n",
       " 'grilled': 583,\n",
       " 'crash': 584,\n",
       " 'whilst': 585,\n",
       " 'rider': 586,\n",
       " 'summer': 587,\n",
       " 'computers': 588,\n",
       " 'before': 589,\n",
       " 'chairs': 590,\n",
       " 'facing': 591,\n",
       " 'image': 592,\n",
       " 'mop': 593,\n",
       " 'pad': 594,\n",
       " 'utep': 595,\n",
       " 'member': 596,\n",
       " 'wisconsin': 597,\n",
       " 'overalls': 598,\n",
       " 'bubbles': 599,\n",
       " 'jackets': 600,\n",
       " 'grocery': 601,\n",
       " 'tv': 602,\n",
       " 'hill': 603,\n",
       " 'obstacle': 604,\n",
       " 'nine': 605,\n",
       " 'robes': 606,\n",
       " 'hoods': 607,\n",
       " 'plush': 608,\n",
       " 'asleep': 609,\n",
       " 'local': 610,\n",
       " 'buggy': 611,\n",
       " 'paved': 612,\n",
       " 'bikini': 613,\n",
       " 'cardboard': 614,\n",
       " 'box': 615,\n",
       " 'well': 616,\n",
       " 'built': 617,\n",
       " 'tips': 618,\n",
       " 'give': 619,\n",
       " 'cigarette': 620,\n",
       " 'right': 621,\n",
       " 'cig': 622,\n",
       " 'puddle': 623,\n",
       " 'paddle': 624,\n",
       " 'bed': 625,\n",
       " 'stunts': 626,\n",
       " 'clothespins': 627,\n",
       " 'hang': 628,\n",
       " 'rolling': 629,\n",
       " 'leaving': 630,\n",
       " 'cream': 631,\n",
       " 'but': 632,\n",
       " 'temple': 633,\n",
       " 'pray': 634,\n",
       " 'entertaining': 635,\n",
       " 'symphony': 636,\n",
       " 'graffiti': 637,\n",
       " 'curry': 638,\n",
       " 'caught': 639,\n",
       " 'bamboo': 640,\n",
       " 'dinner': 641,\n",
       " 'lunch': 642,\n",
       " 'lying': 643,\n",
       " 'floral': 644,\n",
       " 'pearl': 645,\n",
       " 'necklace': 646,\n",
       " 'wineglass': 647,\n",
       " 'closed': 648,\n",
       " 'medium': 649,\n",
       " 'sized': 650,\n",
       " 'shows': 651,\n",
       " 'beige': 652,\n",
       " 'foreground': 653,\n",
       " 'dead': 654,\n",
       " 'santa': 655,\n",
       " 'claus': 656,\n",
       " 'stringed': 657,\n",
       " 'says': 658,\n",
       " 'honk': 659,\n",
       " 'indict': 660,\n",
       " 'bush': 661,\n",
       " 'elmo': 662,\n",
       " 'sesame': 663,\n",
       " 'males': 664,\n",
       " 'surly': 665,\n",
       " 'younger': 666,\n",
       " 'points': 667,\n",
       " 'lost': 668,\n",
       " 'scenic': 669,\n",
       " 'stroll': 670,\n",
       " 'sleeps': 671,\n",
       " 'works': 672,\n",
       " 'do': 673,\n",
       " 'drums': 674,\n",
       " 'rains': 675,\n",
       " 'swim': 676,\n",
       " 'interest': 677,\n",
       " 'net': 678,\n",
       " 'parents': 679,\n",
       " 'waxing': 680,\n",
       " 'towel': 681,\n",
       " 'neck': 682,\n",
       " 'painted': 683,\n",
       " 'mirror': 684,\n",
       " 'carefully': 685,\n",
       " 'balanced': 686,\n",
       " 'tattoo': 687,\n",
       " 'brow': 688,\n",
       " 'daughter': 689,\n",
       " 'garden': 690,\n",
       " 'waits': 691,\n",
       " 'cracked': 692,\n",
       " 'giant': 693,\n",
       " 'sphere': 694,\n",
       " 'adult': 695,\n",
       " 'mountain': 696,\n",
       " 'seat': 697,\n",
       " 'vault': 698,\n",
       " 'inches': 699,\n",
       " 'caucasian': 700,\n",
       " 'slacks': 701,\n",
       " 'amused': 702,\n",
       " 'upon': 703,\n",
       " 'unamused': 704,\n",
       " 'straw': 705,\n",
       " 'senior': 706,\n",
       " 'serves': 707,\n",
       " 'sandwiches': 708,\n",
       " 'see': 709,\n",
       " 'motorbikes': 710,\n",
       " 'furniture': 711,\n",
       " 'handed': 712,\n",
       " 'stuffed': 713,\n",
       " 'pulled': 714,\n",
       " 'string': 715,\n",
       " 'studying': 716,\n",
       " 'shelves': 717,\n",
       " 'crane': 718,\n",
       " 'stature': 719,\n",
       " 'progress': 720,\n",
       " 'teenage': 721,\n",
       " 'decline': 722,\n",
       " 'hockey': 723,\n",
       " 'bowler': 724,\n",
       " 'papers': 725,\n",
       " 'doctors': 726,\n",
       " 'surgery': 727,\n",
       " 'woods': 728,\n",
       " 'pushing': 729,\n",
       " 'cargo': 730,\n",
       " 'headband': 731,\n",
       " 'hold': 732,\n",
       " 'burning': 733,\n",
       " 'trying': 734,\n",
       " 'grilling': 735,\n",
       " 'barbecue': 736,\n",
       " 'bowls': 737,\n",
       " 'carpeted': 738,\n",
       " 'soldier': 739,\n",
       " 'gun': 740,\n",
       " 'sideways': 741,\n",
       " 'pamper': 742,\n",
       " 'pieces': 743,\n",
       " 'walls': 744,\n",
       " 'carrying': 745,\n",
       " 'christmas': 746,\n",
       " 'show': 747,\n",
       " 'hug': 748,\n",
       " 'hillside': 749,\n",
       " 'distance': 750,\n",
       " 'guns': 751,\n",
       " 'plaid': 752,\n",
       " 'pattern': 753,\n",
       " 'surrounded': 754,\n",
       " 'kisses': 755,\n",
       " 'bearing': 756,\n",
       " 'saris': 757,\n",
       " 'circle': 758,\n",
       " 'raised': 759,\n",
       " 'photographed': 760,\n",
       " 'suspenseful': 761,\n",
       " 'nears': 762,\n",
       " 'basket': 763,\n",
       " 'urban': 764,\n",
       " 'banner': 765,\n",
       " 'lotos': 766,\n",
       " 'break': 767,\n",
       " 'lined': 768,\n",
       " 'flags': 769,\n",
       " 'full': 770,\n",
       " 'protection': 771,\n",
       " 'uniforms': 772,\n",
       " 'shaving': 773,\n",
       " 'kicking': 774,\n",
       " 'bullpen': 775,\n",
       " 'indoors': 776,\n",
       " 'engage': 777,\n",
       " 'its': 778,\n",
       " 'sunglasses': 779,\n",
       " 'patterned': 780,\n",
       " 'plate': 781,\n",
       " 'balconies': 782,\n",
       " 'midair': 783,\n",
       " 'note': 784,\n",
       " 'practicing': 785,\n",
       " 'martial': 786,\n",
       " 'arts': 787,\n",
       " 'gym': 788,\n",
       " 'cows': 789,\n",
       " 'shorts': 790,\n",
       " 'hispanic': 791,\n",
       " 'vegetables': 792,\n",
       " 'public': 793,\n",
       " 'reads': 794,\n",
       " 'jumped': 795,\n",
       " 'huge': 796,\n",
       " 'brightly': 797,\n",
       " 'sets': 798,\n",
       " 'enjoying': 799,\n",
       " 'block': 800,\n",
       " 'festival': 801,\n",
       " 'rollerskates': 802,\n",
       " 'chinese': 803,\n",
       " 'meet': 804,\n",
       " 'favorite': 805,\n",
       " 'competition': 806,\n",
       " 'stage': 807,\n",
       " 'new': 808,\n",
       " 'wine': 809,\n",
       " 'get': 810,\n",
       " 'present': 811,\n",
       " 'families': 812,\n",
       " 'turn': 813,\n",
       " 'very': 814,\n",
       " 'sort': 815,\n",
       " 'canadian': 816,\n",
       " 'arena': 817,\n",
       " 'youngster': 818,\n",
       " 'makeshift': 819,\n",
       " 'companion': 820,\n",
       " 'nearby': 821,\n",
       " 'business': 822,\n",
       " 'vendor': 823,\n",
       " 'night': 824,\n",
       " 'tries': 825,\n",
       " 'toys': 826,\n",
       " 'colorfully': 827,\n",
       " 'center': 828,\n",
       " 'barricade': 829,\n",
       " 'gentlemen': 830,\n",
       " 'items': 831,\n",
       " 'held': 832,\n",
       " 'workshop': 833,\n",
       " 'throw': 834,\n",
       " 'fun': 835,\n",
       " 'crafts': 836,\n",
       " 'stretches': 837,\n",
       " 'blows': 838,\n",
       " 'gaze': 839,\n",
       " 'except': 840,\n",
       " 'entrance': 841,\n",
       " 'entertains': 842,\n",
       " 'falling': 843,\n",
       " 'upside': 844,\n",
       " 'examining': 845,\n",
       " 'pedal': 846,\n",
       " 'kayaking': 847,\n",
       " 'waters': 848,\n",
       " 'learning': 849,\n",
       " 'use': 850,\n",
       " 'adolescent': 851,\n",
       " 'jump': 852,\n",
       " 'patrol': 853,\n",
       " 'normal': 854,\n",
       " 'reflective': 855,\n",
       " 'flipping': 856,\n",
       " 'washed': 857,\n",
       " 'gurnee': 858,\n",
       " 'hospital': 859,\n",
       " 'taller': 860,\n",
       " 'smaller': 861,\n",
       " 'checkered': 862,\n",
       " 'makes': 863,\n",
       " 'laugh': 864,\n",
       " 'airplane': 865,\n",
       " 'no': 866,\n",
       " 'cigarettes': 867,\n",
       " 'making': 868,\n",
       " 'carve': 869,\n",
       " 'freshly': 870,\n",
       " 'barbecued': 871,\n",
       " 'hog': 872,\n",
       " 'speaker': 873,\n",
       " 'bowtie': 874,\n",
       " 'rest': 875,\n",
       " 'surrounding': 876,\n",
       " 'extended': 877,\n",
       " 'if': 878,\n",
       " 'trumpet': 879,\n",
       " 'interesting': 880,\n",
       " 'covered': 881,\n",
       " 'unto': 882,\n",
       " 'filthy': 883,\n",
       " 'moon': 884,\n",
       " 'decisions': 885,\n",
       " 'sorts': 886,\n",
       " 'dominoes': 887,\n",
       " 'eats': 888,\n",
       " 'profile': 889,\n",
       " 'luxury': 890,\n",
       " 'feet': 891,\n",
       " 'yard': 892,\n",
       " 'fingers': 893,\n",
       " 'smudges': 894,\n",
       " 'lawn': 895,\n",
       " 'words': 896,\n",
       " 'art': 897,\n",
       " 'coats': 898,\n",
       " 'scarves': 899,\n",
       " 'seemed': 900,\n",
       " 'concrete': 901,\n",
       " 'scarf': 902,\n",
       " 'silver': 903,\n",
       " 'word': 904,\n",
       " 'camden': 905,\n",
       " 'india': 906,\n",
       " 'protected': 907,\n",
       " 'umbrella': 908,\n",
       " 'shades': 909,\n",
       " 'were': 910,\n",
       " 'excited': 911,\n",
       " 'pass': 912,\n",
       " 'parking': 913,\n",
       " 'wax': 914,\n",
       " 'row': 915,\n",
       " 'shops': 916,\n",
       " 'lifted': 917,\n",
       " 'attempt': 918,\n",
       " 'defending': 919,\n",
       " 'turns': 920,\n",
       " 'wrapped': 921,\n",
       " 'lower': 922,\n",
       " 'part': 923,\n",
       " 'garage': 924,\n",
       " 'geometric': 925,\n",
       " 'designs': 926,\n",
       " 'granddaughter': 927,\n",
       " 'trouble': 928,\n",
       " 'poor': 929,\n",
       " 'heat': 930,\n",
       " 'family': 931,\n",
       " 'approaching': 932,\n",
       " 'opponent': 933,\n",
       " 'discussing': 934,\n",
       " 'movie': 935,\n",
       " 'cash': 936,\n",
       " 'glance': 937,\n",
       " 'strange': 938,\n",
       " 'beaked': 939,\n",
       " 'astroturf': 940,\n",
       " 'talks': 941,\n",
       " 'touches': 942,\n",
       " 'vintage': 943,\n",
       " 'aircraft': 944,\n",
       " 'swings': 945,\n",
       " 'patient': 946,\n",
       " 'riot': 947,\n",
       " 'shield': 948,\n",
       " 'classwork': 949,\n",
       " 'shallows': 950,\n",
       " 'maintenance': 951,\n",
       " 'skipping': 952,\n",
       " 'donald': 953,\n",
       " 'duck': 954,\n",
       " 'hotel': 955,\n",
       " 'backpack': 956,\n",
       " 'wandering': 957,\n",
       " 'roll': 958,\n",
       " 'came': 959,\n",
       " 'trashcan': 960,\n",
       " 'also': 961,\n",
       " 'things': 962,\n",
       " 'latex': 963,\n",
       " 'glove': 964,\n",
       " 'tables': 965,\n",
       " 'type': 966,\n",
       " 'order': 967,\n",
       " 'army': 968,\n",
       " 'few': 969,\n",
       " 'catching': 970,\n",
       " 'lane': 971,\n",
       " 'igloo': 972,\n",
       " 'overnight': 973,\n",
       " 'stay': 974,\n",
       " 'clasping': 975,\n",
       " 'complete': 976,\n",
       " 'lit': 977,\n",
       " 'spotlights': 978,\n",
       " 'professional': 979,\n",
       " 'registration': 980,\n",
       " 'sink': 981,\n",
       " 'fountain': 982,\n",
       " 'base': 983,\n",
       " 'jacket': 984,\n",
       " 'advertisement': 985,\n",
       " 'backpacks': 986,\n",
       " 'removing': 987,\n",
       " 'item': 988,\n",
       " 'tree': 989,\n",
       " 'snag': 990,\n",
       " 'n': 991,\n",
       " 'badges': 992,\n",
       " 'thing': 993,\n",
       " 'third': 994,\n",
       " 'multiple': 995,\n",
       " 'colors': 996,\n",
       " 'ritual': 997,\n",
       " 'takeoff': 998,\n",
       " 'protesting': 999,\n",
       " 'raids': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m__MACOSX\u001b[0m/  \u001b[01;31mglove.6B.zip\u001b[0m  \u001b[01;34msnli_1.0\u001b[0m/  \u001b[01;31msnli_1.0.bak.zip\u001b[0m  \u001b[01;31msnli_1.0.zip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ls entailment/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.50d.txt\n",
      "Reading lines from file glove.6B.50d.txt\n",
      "glove.6B.100d.txt\n",
      "glove.6B.200d.txt\n",
      "glove.6B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "glove_zip_file = \"entailment/data/glove.6B.zip\"\n",
    "glove_vectors_file = \"glove.6B.50d.txt\"\n",
    "embeddings_index = {}\n",
    "with zipfile.ZipFile(glove_zip_file) as z:\n",
    "        for info in z.infolist():\n",
    "            print(info.filename)\n",
    "            if glove_vectors_file in info.filename:\n",
    "                # read the file\n",
    "                print(\"Reading lines from file {}\".format(glove_vectors_file))\n",
    "                with io.TextIOWrapper(z.open(glove_vectors_file), encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        terms = line.split()\n",
    "                        word = terms[0]\n",
    "                        coefs = np.asarray(terms[1:], dtype='float32')\n",
    "                        \n",
    "                        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze what we have in the tokenized sequences. A look at the first example, we have a label, the hypothesis and the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contradiction',\n",
       "  'A white dog running in the backyard.',\n",
       "  'The dog is sleeping.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens for the above hypothesis 'Masked actors perform traditional Japanese theater.' is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 84)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  3,   2,  28,  31,  70,   7,   6, 829,   5,   4,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(hyp_tokens[:1].shape)\n",
    "hyp_tokens[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens are the indices of the words in the vocabublary of the tokenizer as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word a in the tokenized sequence is 2\n",
      "Index of word group in the tokenized sequence is 40\n",
      "Index of word of in the tokenized sequence is 13\n",
      "Index of word people in the tokenized sequence is 17\n",
      "Index of word clap in the tokenized sequence is 4529\n",
      "Index of word and in the tokenized sequence is 11\n",
      "Index of word take in the tokenized sequence is 410\n",
      "Index of word pictures in the tokenized sequence is 405\n",
      "Index of word of in the tokenized sequence is 13\n",
      "Index of word an in the tokenized sequence is 21\n",
      "Index of word unseen in the tokenized sequence is 2185\n",
      "Index of word subject in the tokenized sequence is 4913\n",
      "Index of word . in the tokenized sequence is 5\n"
     ]
    }
   ],
   "source": [
    "sent = 'A group of people clap and take pictures of an unseen subject .'\n",
    "for word in sent.lower().split(' '):\n",
    "    print(\"Index of word {} in the tokenized sequence is {}\".format(word, lang_tokenizer.word_index.get(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are now using the glove embeddings, instead of using the indices of the words, we will be using the 50 dimensional word embedding for the word in the vocabulary. E.g. the embedding for 'masked' would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0912e-01, -7.7150e-01,  5.8735e-01, -2.7371e-01,  1.2132e+00,\n",
       "        1.0705e-01, -1.2723e-01,  2.5333e-02, -8.0314e-01, -2.0963e-02,\n",
       "        2.6252e-01, -1.1936e+00, -3.6585e-01,  9.5928e-01, -8.6888e-02,\n",
       "       -7.5409e-01, -3.2490e-01,  2.1727e-01, -1.1456e+00, -3.8625e-01,\n",
       "       -2.4218e-01,  9.1697e-01,  1.9012e-01,  3.7527e-01, -7.4516e-01,\n",
       "       -3.5927e-01, -5.2885e-01,  2.7933e-01,  5.4898e-02, -4.1675e-01,\n",
       "        1.0453e+00, -6.1269e-04, -1.4485e-01, -2.5002e-01,  2.7415e-01,\n",
       "        9.8532e-01, -1.2851e-01, -1.5922e+00, -5.0248e-01,  2.1163e-02,\n",
       "       -1.4607e-01,  5.6129e-02, -2.9917e-01,  8.6975e-02,  1.1894e+00,\n",
       "       -1.7840e+00, -5.7097e-02, -3.2755e-01,  7.1164e-01, -1.4130e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(lang_tokenizer.word_index)\n",
    "embeddings_index.get('masked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the embedding matrix which will serve as the look up for the embedding for the words. \n",
    "\n",
    "The keras embedding layer will look up the embedding when running our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of sequence is 50\n"
     ]
    }
   ],
   "source": [
    "embeddings_index[\"the\"].shape\n",
    "max_length = embeddings_index[\"the\"].shape[0]\n",
    "print(\"max length of sequence is {}\".format(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "embedding_matrix = np.zeros((len(lang_tokenizer.word_index) + 1, max_length))\n",
    "for word, i in lang_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of 'masked' was 567, the embedding for it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11941   ,  0.41365001, -0.069033  , -0.7823    ,  0.71780002,\n",
       "        1.39260006, -0.42550001, -0.97036999,  1.0668    ,  0.29361001,\n",
       "        0.42355999, -0.26536   ,  1.2586    ,  0.58969998,  0.66358   ,\n",
       "       -0.11947   , -0.54562998,  0.63848001, -0.33372   ,  0.16693   ,\n",
       "       -0.24625   ,  0.70568001,  1.18280005,  1.50090003, -0.23576   ,\n",
       "       -0.65605003, -0.25080001, -0.62052   ,  0.092142  ,  0.018576  ,\n",
       "        2.62179995,  0.23755001,  0.20074999, -0.92991   ,  0.14143001,\n",
       "        0.26080999,  0.22187001, -0.55680001, -0.34384   , -1.00489998,\n",
       "       -0.15775   , -0.31162   ,  0.18086   ,  0.97092003,  0.31748   ,\n",
       "       -0.29818001,  0.24237999, -0.78579003, -0.48475   ,  0.92211998])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[567]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14864, 50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network options\n",
    "\n",
    "At first, we will not use the embedding layer, instead we will send in the tokens as is into our network.\n",
    "\n",
    "Let us prepare the network.\n",
    "\n",
    "From the references  [here](https://www.tensorflow.org/text/tutorials/text_classification_rnn) we build the network.\n",
    "\n",
    "Note, we will not send any embeddin matrix here, in fact we will let the network learn the embeddings.\n",
    "\n",
    "The embedding layer, when called, converts the sequences of word indices to sequences of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = Embedding(len(word_index),\n",
    "#                             EMBEDDING_DIM,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=False)\n",
    "# sequence_input = keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "# embedded_sequences = embedding_layer(sequence_input)\n",
    "# print(embedded_sequences)\n",
    "# x1 = Conv1D(filters, kernel_size, padding='valid', activation='tanh')(embedded_sequences)\n",
    "# x2 = keras.layers.GlobalMaxPooling1D()(x1)\n",
    "# model = keras.Model(sequence_input, x2)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14863"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lang_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 50)          743200    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100)               40400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 785,267\n",
      "Trainable params: 42,067\n",
      "Non-trainable params: 743,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size= len(lang_tokenizer.word_index)\n",
    "dim = embedding_matrix.shape[1]\n",
    "#dim = 50 #keep it same as the dim of the embedding matrix so that we can compare\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True)\n",
    "inp = keras.Input(shape=(None,))\n",
    "x1 = embedding_layer(inp)\n",
    "x2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(dim))(x1)\n",
    "x3 = tf.keras.layers.Dense(16, activation='relu')(x2)\n",
    "x4 = tf.keras.layers.Dropout(0.1)(x3)\n",
    "output = tf.keras.layers.Dense(3, activation='softmax')(x4)\n",
    "    \n",
    "model = keras.Model(inp, output)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(), \n",
    "          metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate embedding layer with glove network \n",
    "`embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            max_length,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(lang_tokenizer.word_index)\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Embedding(\n",
    "#         input_dim=vocab_size,\n",
    "#         output_dim=64,\n",
    "#         # Use masking to handle the variable sequence lengths\n",
    "#         mask_zero=True),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),\n",
    "#     tf.keras.layers.Dense(3)\n",
    "# ])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contradiction',\n",
       "  'A white dog running in the backyard.',\n",
       "  'The dog is sleeping.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = 'Masked actors perform traditional Japanese theater.'\n",
    "# print(s)\n",
    "# s = preprocess(s)\n",
    "# print(s)\n",
    "# s_toks = lang_tokenizer.texts_to_sequences([s])\n",
    "# s_toks = tf.keras.preprocessing.sequence.pad_sequences(s_toks, padding='post')\n",
    "# s_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l, h, e = train[:1][0]\n",
    "# h = preprocess(h)\n",
    "# h_toks = lang_tokenizer.texts_to_sequences([h])\n",
    "# h_toks = tf.keras.preprocessing.sequence.pad_sequences(h_toks, maxlen=max_len_hyp, padding='post')\n",
    "# e = preprocess(e)\n",
    "# e_toks = lang_tokenizer.texts_to_sequences([e])\n",
    "# e_toks = tf.keras.preprocessing.sequence.pad_sequences(e_toks, maxlen=max_len_evi, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(h_toks.shape)\n",
    "# h_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_toks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.concatenate((h_toks, e_toks), axis = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_text = ('The movie was cool. The animation and the graphics '\n",
    "             #  'were out of this world. I would recommend this movie.')\n",
    "# predictions = model.predict(np.concatenate((h_toks, e_toks), axis = 1))\n",
    "# print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tmp/checkpoint\n",
    "!mkdir -p tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 130)\n",
      "tf.Tensor(\n",
      "[[ 3  2  9 ...  0  0  0]\n",
      " [ 3  2 26 ...  0  0  0]\n",
      " [ 3 16 82 ...  0  0  0]\n",
      " ...\n",
      " [ 3  2 26 ...  0  0  0]\n",
      " [ 3  2 15 ...  0  0  0]\n",
      " [ 3  2 15 ...  0  0  0]], shape=(64, 130), dtype=int32)\n",
      "(64, 3)\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "    print(example.shape)\n",
    "    print(example)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the val loss starts increasing for certain number of iterations, the training will stop and we would need to go back to the bext model for which the loss was the least. Therefore, we would need to checkpoint the model weights to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "765/765 [==============================] - 70s 85ms/step - loss: 1.0475 - accuracy: 0.4424 - val_loss: 0.9365 - val_accuracy: 0.5536\n",
      "Epoch 2/20\n",
      "765/765 [==============================] - 66s 85ms/step - loss: 0.9348 - accuracy: 0.5512 - val_loss: 0.8854 - val_accuracy: 0.5880\n",
      "Epoch 3/20\n",
      "765/765 [==============================] - 57s 75ms/step - loss: 0.8935 - accuracy: 0.5799 - val_loss: 0.8522 - val_accuracy: 0.6089\n",
      "Epoch 4/20\n",
      "765/765 [==============================] - 61s 80ms/step - loss: 0.8617 - accuracy: 0.6056 - val_loss: 0.8294 - val_accuracy: 0.6339\n",
      "Epoch 5/20\n",
      "765/765 [==============================] - 61s 80ms/step - loss: 0.8361 - accuracy: 0.6255 - val_loss: 0.8130 - val_accuracy: 0.6411\n",
      "Epoch 6/20\n",
      "765/765 [==============================] - 63s 82ms/step - loss: 0.8143 - accuracy: 0.6380 - val_loss: 0.8175 - val_accuracy: 0.6375\n",
      "Epoch 7/20\n",
      "765/765 [==============================] - 65s 84ms/step - loss: 0.7983 - accuracy: 0.6453 - val_loss: 0.8053 - val_accuracy: 0.6385\n",
      "Epoch 8/20\n",
      "765/765 [==============================] - 66s 86ms/step - loss: 0.7784 - accuracy: 0.6590 - val_loss: 0.7993 - val_accuracy: 0.6422\n",
      "Epoch 9/20\n",
      "765/765 [==============================] - 67s 87ms/step - loss: 0.7645 - accuracy: 0.6671 - val_loss: 0.7918 - val_accuracy: 0.6370\n",
      "Epoch 10/20\n",
      "765/765 [==============================] - 68s 89ms/step - loss: 0.7408 - accuracy: 0.6820 - val_loss: 0.7899 - val_accuracy: 0.6542\n",
      "Epoch 11/20\n",
      "765/765 [==============================] - 68s 89ms/step - loss: 0.7293 - accuracy: 0.6840 - val_loss: 0.7865 - val_accuracy: 0.6474\n",
      "Epoch 12/20\n",
      "765/765 [==============================] - 69s 89ms/step - loss: 0.7096 - accuracy: 0.6969 - val_loss: 0.7977 - val_accuracy: 0.6526\n",
      "Epoch 13/20\n",
      "765/765 [==============================] - 72s 94ms/step - loss: 0.6909 - accuracy: 0.7061 - val_loss: 0.8004 - val_accuracy: 0.6479\n",
      "Epoch 14/20\n",
      "765/765 [==============================] - 71s 92ms/step - loss: 0.6719 - accuracy: 0.7147 - val_loss: 0.7987 - val_accuracy: 0.6516\n",
      "Epoch 15/20\n",
      "765/765 [==============================] - 72s 93ms/step - loss: 0.6570 - accuracy: 0.7234 - val_loss: 0.8061 - val_accuracy: 0.6536\n",
      "Epoch 16/20\n",
      "765/765 [==============================] - 71s 93ms/step - loss: 0.6471 - accuracy: 0.7278 - val_loss: 0.8043 - val_accuracy: 0.6562\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=20,\n",
    "                    callbacks=[stop_early, model_checkpoint_callback],\n",
    "                    validation_data=val_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Metrics for training SNLI dataset, 70K samples (glove)')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1)\n",
    "x = np.arange(len(history.history['loss']))\n",
    "ax.plot(x, history.history['loss'], label=\"train loss\")\n",
    "ax.plot(x, history.history['val_loss'], label=\"val loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"categorial cross entropy loss\")\n",
    "ax.legend()\n",
    "plt.title(\"Metrics for training SNLI dataset, 70K samples (glove)\")\n",
    "#history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f582ad60a60>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 11s 32ms/step - loss: 0.8351 - accuracy: 0.6413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8350659608840942, 0.6412919163703918]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "loss: 0.8276 - accuracy: 0.6454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language translation\n",
    "\n",
    "\n",
    "Now let's take a look at how the data preparation is done in case of sequence to sequence models in tensorflow.\n",
    "\n",
    "We will look at the example of machine translation.\n",
    "\n",
    "We will use the code from [networks_seq2seq_nmt](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt) from tensorflow\n",
    "\n",
    "First some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In language translations, the input format is as follows, where the translations are separated by a tab.\n",
    "\n",
    "The lines are read in unicode, we will need to convert them to ascii after the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\"May I borrow this book?\\t¿Puedo tomar prestado este libro?\",\n",
    "         \"May I borrow this bookish?\\t¿Puedo tomar prestado este libromacho\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "def preprocess(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [[preprocess(w) for w in l.split('\\t')] for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_lang , inp_lang = zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<start> ¿ puedo tomar prestado este libro ? <end>',\n",
       " '<start> ¿ puedo tomar prestado este libromacho <end>')"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "        # lang = list of sentences in a language\n",
    "\n",
    "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
    "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "        lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
    "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
    "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
    "\n",
    "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
    "        ## and pads the sequences to match the longest sequences in the given input\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2,  3,  4,  5,  6,  9,  7,  8],\n",
       "        [ 2,  3,  4,  5,  6, 10,  7,  8]], dtype=int32),\n",
       " <keras_preprocessing.text.Tokenizer at 0x7fcd821d7760>)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(tar_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nmt():\n",
    "    path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "    path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "    return path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.keras/datasets/spa-eng/spa.txt'"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_nmt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the NMT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/root/.keras/datasets/spa-eng/spa.txt'\n",
    "lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "n_samples = 1000\n",
    "word_pairs = [[preprocess(w) for w in l.split('\\t')] for l in lines[:n_samples]]\n",
    "tar_lang , inp_lang = zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tensor, tar_toknz = tokenize(tar_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_tensor, inp_toknz = tokenize(inp_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shapes, target = (1000, 7), input = (1000, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensor shapes, target = {}, input = {}\".format(tar_tensor.shape, inp_tensor.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(inp_tensor, tar_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ex, tar_ex = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int32, numpy=array([  2,   5,   9, 123,   4,   3,   0], dtype=int32)>"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_ex[0]\n",
    "tar_ex[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size = 823, Target vocab size = 374\n",
      "Max input length = 9, max output length = 7\n",
      "Steps per epoch = 15\n"
     ]
    }
   ],
   "source": [
    "vocab_inp_size = len(inp_toknz.word_index)+1\n",
    "vocab_tar_size = len(tar_toknz.word_index)+1\n",
    "max_length_input = inp_ex.shape[1]\n",
    "max_length_output = tar_ex.shape[1]\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "steps_per_epoch = n_samples//BATCH_SIZE\n",
    "print(\"Input vocab size = {}, Target vocab size = {}\".format(vocab_inp_size, vocab_tar_size))\n",
    "print(\"Max input length = {}, max output length = {}\".format(max_length_input, max_length_output))\n",
    "print(\"Steps per epoch = {}\".format(steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See a text classification example [here](https://www.tensorflow.org/text/tutorials/text_classification_rnn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 64)          52672     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 127,041\n",
      "Trainable params: 127,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_inp_size,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 0],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 0],\n",
       "       [94],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 9]], dtype=int32)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_predict = \"How are you?\"\n",
    "tensor = inp_toknz.texts_to_sequences(text_to_predict) \n",
    "## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
    "## and pads the sequences to match the longest sequences in the given input\n",
    "tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00504636]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(tensor)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embedding layer. The embedding layer needs the vocab size and the chosen embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(vocab_inp_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.recurrent_v2.LSTMCell at 0x7fcd82144340>"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_size = 64\n",
    "rnn = tf.keras.layers.LSTMCell(rnn_size)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 17, 300)\n",
      "(17, 30, 300)\n",
      "(3060, 50)\n",
      "17\n",
      "(180, 50)\n"
     ]
    }
   ],
   "source": [
    "# N: The number of elements in each of our batches, \n",
    "#   which we use to train subsets of data for efficiency's sake.\n",
    "N = 30\n",
    "l_h = 9\n",
    "l_e = 8\n",
    "D = 300\n",
    "l_seq = l_h + l_e\n",
    "vector_size=50\n",
    "hyp = tf.Variable(tf.ones(shape=[N, l_h, D]), dtype=tf.float32)\n",
    "evi = tf.Variable(tf.ones(shape=[N, l_e, D]), dtype=tf.float32)\n",
    "x = tf.concat([hyp, evi], 1)\n",
    "print(x.shape)\n",
    "# Permuting batch_size and n_steps\n",
    "x = tf.transpose(x, [1, 0, 2]) # (Le+Lh), N, d\n",
    "print(x.shape)\n",
    "x = tf.reshape(x, [-1, vector_size])\n",
    "print(x.shape)\n",
    "# # Reshaping to (n_steps*batch_size, n_input)\n",
    "# x = tf.reshape(x, [-1, vector_size]) # (Le+Lh)*N, d\n",
    "# # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "x = tf.split(x, l_seq,)\n",
    "print(len(x))\n",
    "print(x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
